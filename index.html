<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="16x16" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="mask-icon" href="https://dogecoin.com/assets/images/doge.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>



<link rel="canonical" href="http://example.com/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>hipudding's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">hipudding's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hipudding"
      src="https://dogecoin.com/assets/images/doge.svg">
  <p class="site-author-name" itemprop="name">hipudding</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hipudding" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hipudding" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:huafengchun@gmail.com" title="E-Mail → mailto:huafengchun@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/23/Ray%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8-%E7%BC%96%E8%AF%91%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BB%BB%E5%8A%A1%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/23/Ray%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8-%E7%BC%96%E8%AF%91%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BB%BB%E5%8A%A1%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/" class="post-title-link" itemprop="url">Ray技术入门-编译部署和任务生命周期</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-01-23 11:39:15 / 修改时间：15:17:18" itemprop="dateCreated datePublished" datetime="2025-01-23T11:39:15+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="背景知识">背景知识</h2>
<p>Ray是一个使用Bazel构建的，基于gRPC上层打造的开源分布式计算框架，旨在简化分布式应用的开发和运行。它支持无缝地将
Python 代码扩展到多核、多节点环境，适合构建高性能的分布式系统。Ray
提供灵活的任务调度和状态管理，支持多种编程模型，包括任务并行和 actor
模式，并通过自动化的资源管理和容错机制简化复杂分布式工作的部署。它还拥有丰富的生态系统，包含机器学习库（如
Ray Tune、Ray Serve 和
RLlib），适用于模型训练、超参数调优、在线服务等场景，是云原生应用和大规模计算的理想选择。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/ray-project/ray">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://www.ray.io/">主页</a></strong></p>
<h3 id="bazel">Bazel</h3>
<p>Bazel是一种高效、可扩展的构建工具，最初由Google开发，专为管理大型代码库和复杂项目而设计。它支持多语言和多平台构建，包括C++,
Java, Python,
Go等，并能够跨操作系统（如Linux、macOS和Windows）执行构建任务。Bazel通过声明式的构建规则（BUILD文件）和依赖管理，实现了高性能的增量构建，避免了不必要的重复编译。其特点包括分布式构建、沙盒化执行和强大的缓存机制，可以加快构建速度并提高构建的稳定性。此外，Bazel还提供高度可配置的扩展机制，方便开发者为特定需求编写自定义规则，适合从小型项目到超大规模工程的使用场景。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/bazelbuild/bazel">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://bazel.build">主页</a></strong></p>
<h3 id="grpc">gRPC</h3>
<p>gRPC 是由 Google 开发的高性能开源 RPC 框架，基于 HTTP/2
协议，支持多语言和跨平台通信。它使用 Protocol Buffers
定义接口和序列化数据，简化了服务间的集成开发。gRPC
提供高效的请求-响应模型、流式传输、负载均衡和内置 TLS
安全特性，非常适合云原生应用、微服务架构和实时通信场景，广泛应用于分布式系统和高性能应用开发中。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/grpc/grpc">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://grpc.io/">主页</a></strong></p>
<h3 id="cython">cython</h3>
<p>Cython 是一种优化的 Python 扩展语言，结合了 Python 的易用性和 C
的高性能，旨在提升 Python 代码的运行速度。通过将 Python 代码转译为 C 或
C++ 并进行编译，Cython 可以显著减少运行时的性能开销，同时支持调用 C/C++
库，从而实现与底层代码的高效交互。Cython 保留了大部分 Python
的语法，同时允许使用 C
类型声明进行性能优化，非常适合计算密集型任务或对性能要求较高的场景，如科学计算、机器学习和数据处理。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/cython/cython">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://cython.org/">主页</a></strong></p>
<h2 id="角色">角色</h2>
<p>Ray集群的整体架构如下图所示</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/06_Ray架构图-20250121160411114.png"
alt="Ray架构图" />
<figcaption aria-hidden="true">Ray架构图</figcaption>
</figure>
<p>一个Ray集群包括多个Node节点，其中每个Node节点包含Actor，Worker，共享内存，本地调度器。其中Head
Node还有GCS服务，包含各类元数据存储，WebUI，全局调度等功能。</p>
<h3 id="node">Node</h3>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/v2-cf09ec3ccc183b816fa303164f4144b3_1440w.jpg"
alt="Ray Nodes" />
<figcaption aria-hidden="true">Ray Nodes</figcaption>
</figure>
<p>Node中包含一个Raylet进程，负责本地调度以及共享内存。Raylet会根据任务情况启动一个或者多个worker或者Actor。Head
node是一个特殊的Node，除了普通Node的功能之外，还有一些外的进程，包括gcs_server服务，dashboard等。并且每个Node节点还会启动monitor进程，log_monitor进程，agent进程等。</p>
<p>Raylet和gcs_server是非Python进程，其他辅助进程，包括Driver，worker以及Actor均是python进程（针对Python语言而言）。</p>
<h3 id="gcs_server">gcs_server</h3>
<p>Ray 的 GCS Server 是 Ray
框架的核心组件，负责元数据存储、任务调度、资源管理和集群状态维护。它通过存储模块（Redis
或 内存）管理节点和任务的生命周期，使用 Pub-Sub
系统进行状态广播，并通过高效的调度协调与其他组件（如 Scheduler, Worker,
Actor 或
Object）协作，确保系统的高性能、高可用性和灵活扩展性。gcs_server是一个非Python进程，二进制文件路径在
<code>ray/python/ray/core/src/ray/gcs/gcs_server</code>。</p>
<h3 id="raylet">raylet</h3>
<p>Raylet 是 Ray
集群中每个节点的核心运行时组件，负责任务执行、资源管理和数据依赖协调。它接收
GCS Server 分配的任务，管理本地资源，启动 Worker 进程执行任务，并通过
Plasma Store 处理数据存储与传输。同时，Raylet 定期向 GCS Server
汇报节点状态，协作实现任务调度、资源分配和故障恢复，是 Ray
分布式运行的关键执行单元。raylet是一个非Python进程，二进制文件路径在
<code>ray/python/ray/core/src/ray/raylet/raylet</code>。</p>
<h3 id="worker">worker</h3>
<p>Worker 是 Ray 中的核心计算单元，由 Raylet 启动，负责具体任务的执行和
Actor 的运行。它与 Plasma Store 协作进行数据存取，并通过与 Raylet
的通信完成任务调度和资源管理。Worker 支持多语言运行环境（如
Python、Java），能够高效并行处理任务，是 Ray
框架实现分布式计算的基础组件。</p>
<h3 id="actor">actor</h3>
<p>Actor 是 Ray
框架中的一种状态管理单元，允许用户在分布式系统中创建带有持久状态的计算对象。每个
Actor 由一个独立的 Worker
进程运行，支持并行调用方法并维护自身状态。Actor
可以通过远程调用接口与其他组件交互，实现任务分解和动态扩展，是 Ray
中用于构建有状态应用和分布式服务的重要抽象。</p>
<h3 id="driver">driver</h3>
<p>Driver就是用户程序（例如，用@ray.remote修饰的用户python代码），Driver负责Task的定义和提交，需要运行在Ray的Head或者Node节点上。</p>
<h2 id="编译">编译</h2>
<p>安装Ray有多种方法，包括wheel包，pip，conda，容器镜像等。这些内容可以参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/ray-overview/installation.html">社区手册</a>。这里介绍从<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/ray-contribute/development.html#building-ray">源码安装</a>。</p>
<h3 id="使用conda环境">使用Conda环境</h3>
<p>官方推荐conda或者venv两种虚拟环境安装ray，建议选择conda。无虚拟环境将无法编译ray，并且在实测中发现了venv的未知错误。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -c conda-forge python=3.9 -n myenv</span><br><span class="line">conda activate myenv</span><br></pre></td></tr></table></figure>
<h3 id="安装依赖">安装依赖</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y build-essential curl clang pkg-config psmisc unzip</span><br></pre></td></tr></table></figure>
<p><strong>安装bazel</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ci/env/install-bazel.sh</span><br></pre></td></tr></table></figure>
<p><strong>安装npm</strong></p>
<p>用于dashboard</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">(curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh)</span></span><br><span class="line">nvm install 14</span><br><span class="line">nvm use 14</span><br></pre></td></tr></table></figure>
<h3 id="构建">构建</h3>
<p><strong>构建dashboard</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ray/python/ray/dashboard/client</span><br><span class="line">npm ci</span><br><span class="line">npm run build</span><br></pre></td></tr></table></figure>
<p><strong>构建ray</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd ../../..</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#如果构建机器的内存小于32G，需要限制内存使用，避免oom</span></span></span><br><span class="line">export BAZEL_ARGS=&quot;--local_ram_resources=8&quot;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#debug编译，保留符号表供调试</span></span></span><br><span class="line">export RAY_DEBUG_BUILD=debug</span><br><span class="line">pip install -e . --verbose</span><br></pre></td></tr></table></figure>
<h3 id="可选的编译环境变量">可选的编译环境变量</h3>
<ul>
<li><code>RAY_INSTALL_JAVA</code>: If set and equal to <code>1</code>,
extra build steps will be executed to build java portions of the
codebase</li>
<li><code>RAY_INSTALL_CPP</code>: If set and equal to <code>1</code>,
<code>ray-cpp</code> will be installed</li>
<li><code>RAY_DISABLE_EXTRA_CPP</code>: If set and equal to
<code>1</code>, a regular (non - <code>cpp</code>) build will not
provide some <code>cpp</code> interfaces</li>
<li><code>SKIP_BAZEL_BUILD</code>: If set and equal to <code>1</code>,
no Bazel build steps will be executed</li>
<li><code>SKIP_THIRDPARTY_INSTALL</code>: If set will skip installation
of third-party python packages</li>
<li><code>RAY_DEBUG_BUILD</code>: Can be set to <code>debug</code>,
<code>asan</code>, or <code>tsan</code>. Any other value will be
ignored</li>
<li><code>BAZEL_ARGS</code>: If set, pass a space-separated set of
arguments to Bazel. This can be useful for restricting resource usage
during builds, for example. See https://bazel.build/docs/user-manual for
more information about valid arguments.</li>
<li><code>IS_AUTOMATED_BUILD</code>: Used in CI to tweak the build for
the CI machines</li>
<li><code>SRC_DIR</code>: Can be set to the root of the source checkout,
defaults to <code>None</code> which is <code>cwd()</code></li>
<li><code>BAZEL_SH</code>: used on Windows to find a
<code>bash.exe</code>, see below</li>
<li><code>BAZEL_PATH</code>: used on Windows to find
<code>bazel.exe</code>, see below</li>
<li><code>MINGW_DIR</code>: used on Windows to find
<code>bazel.exe</code> if not found in <code>BAZEL_PATH</code></li>
</ul>
<h2 id="启动一个ray集群">启动一个Ray集群</h2>
<p>接下来使用一个简单的例子来使用Ray，这是一个使用概率计算圆周率π的程序（蒙特卡洛法）。蒙特卡洛方法在计算圆周率时设一个正方形内部相切一个圆，这时圆和正方形的面积之比是π/4。在这个正方形内部，随机产生n个点（这些点服从均匀分布），计算它们与中心点的距离是否大于圆的半径，以此判断是否落在圆的内部。统计圆内的点数，与n的比值乘以4，就是π的值。理论上，n越大，计算的π值越精确。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/3d5d171ad6df43c2babfe981a2ee91e8.jpeg"
alt="蒙特卡洛法" />
<figcaption aria-hidden="true">蒙特卡洛法</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/4f050b95b2c1439baa33a66de817e659.jpeg"
alt="蒙特卡洛法" />
<figcaption aria-hidden="true">蒙特卡洛法</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ray</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Ray</span></span><br><span class="line">ray.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个任务：生成点并统计圆内点数</span></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_points_in_circle</span>(<span class="params">num_samples: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">        x, y = random.uniform(<span class="number">0</span>, <span class="number">1</span>), random.uniform(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> x**<span class="number">2</span> + y**<span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_pi</span>(<span class="params">num_samples: <span class="built_in">int</span>, num_workers: <span class="built_in">int</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="comment"># 每个 worker 分配的样本数</span></span><br><span class="line">    samples_per_worker = num_samples // num_workers</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建并运行任务</span></span><br><span class="line">    futures = [</span><br><span class="line">        count_points_in_circle.remote(samples_per_worker) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_workers)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 收集结果</span></span><br><span class="line">    total_in_circle = <span class="built_in">sum</span>(ray.get(futures))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算圆周率</span></span><br><span class="line">    pi_estimate = <span class="number">4</span> * total_in_circle / num_samples</span><br><span class="line">    <span class="keyword">return</span> pi_estimate</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 总样本数和并行任务数</span></span><br><span class="line">    total_samples = <span class="number">100_000_000</span></span><br><span class="line">    num_workers = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 π</span></span><br><span class="line">    pi = calculate_pi(total_samples, num_workers)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Estimated π: <span class="subst">&#123;pi&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭 Ray</span></span><br><span class="line">    ray.shutdown()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="临时启动">临时启动</h3>
<p>如果不启动Ray集群，直接执行该Python程序，那会在当前节点上默认拉起一个ray集群供计算。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python pi.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 09:35:24,180 INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Estimated π: 3.12</span></span></span><br></pre></td></tr></table></figure>
<h3 id="启动集群">启动集群</h3>
<p><strong>启动Head节点</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">ray start --head</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Local node IP: 192.168.64.8</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Ray runtime started.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Next steps</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To add another node to this Ray cluster, run</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray start --address=&#x27;192.168.64.8:6379&#x27;</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To connect to this Ray cluster:</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    import ray</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray.init()</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To submit a Ray job using the Ray Jobs CLI:</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    RAY_ADDRESS=&#x27;http://127.0.0.1:8265&#x27; ray job submit --working-dir . -- python my_script.py</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  for more information on submitting Ray jobs to the Ray cluster.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To terminate the Ray runtime, run</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray stop</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To view the status of the cluster, use</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray status</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To monitor and debug Ray, view the dashboard at</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    127.0.0.1:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  If connection to the dashboard fails, check your firewall settings and network configuration.</span></span></span><br></pre></td></tr></table></figure>
<p>可以在<code>http://127.0.0.1:8265</code>查看Ray控制台。</p>
<p><strong>启动Node节点</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ray start --address=&#x27;192.168.64.8:6379&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Local node IP: 192.168.64.9</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#[2025-01-21 10:49:34,903 W 1882 1882] global_state_accessor.cc:463: Retrying to get node with node ID ba8aafea9f23f6f29ff6cd174e31aaac37cddb0e832c4e3170ddcf63</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Ray runtime started.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#To terminate the Ray runtime, run</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  ray stop</span></span></span><br></pre></td></tr></table></figure>
<p><strong>集群状态</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">======== Autoscaler status: 2025-01-21 10:54:25.170247 ========</span><br><span class="line">Node status</span><br><span class="line">---------------------------------------------------------------</span><br><span class="line">Active:</span><br><span class="line"> 1 node_948847515a43d4fba13c1bdb6a5e5611c2580ecb60f60183ef033771</span><br><span class="line"> 1 node_ba8aafea9f23f6f29ff6cd174e31aaac37cddb0e832c4e3170ddcf63</span><br><span class="line">Pending:</span><br><span class="line"> (no pending nodes)</span><br><span class="line">Recent failures:</span><br><span class="line"> (no failures)</span><br><span class="line"></span><br><span class="line">Resources</span><br><span class="line">---------------------------------------------------------------</span><br><span class="line">Usage:</span><br><span class="line"> 0.0/16.0 CPU</span><br><span class="line"> 0B/8.40GiB memory</span><br><span class="line"> 0B/3.93GiB object_store_memory</span><br><span class="line"></span><br><span class="line">Demands:</span><br><span class="line"> (no resource demands)</span><br></pre></td></tr></table></figure>
<h3 id="启用监控">启用监控</h3>
<p><strong>Prometheus</strong></p>
<p>ray提供了一个命令来下载和部署普罗米修斯，ray提供了数据采集接口，可以让普罗米修斯通过这个接口来收集集群数据，注意，简易命令拉起的普罗米修斯不能用于生产环境，完整部署可参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/cluster/metrics.html#optional-manual-running-prometheus-locally">官方手册</a>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ray metrics launch-prometheus</span><br></pre></td></tr></table></figure>
<p>可以在这个地址上查看普罗米修斯状态：http://localhost:9090，可查看其采集的信息<code>ray_dashboard_api_requests_count_requests_total</code>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121190550385.png"
alt="普罗米修斯dashboard" />
<figcaption aria-hidden="true">普罗米修斯dashboard</figcaption>
</figure>
<p><strong>grafana</strong></p>
<p>普罗米修斯采集的数据，通过grafana的方式进行可视化显示，并且ray
dashboard中的metric页面的信息也是来自于grafana。可以通过启动新的grafana服务来完成配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/share/grafana</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#启动grafana需要创建data目录，需要sudo执行</span></span></span><br><span class="line">sudo ./bin/grafana-server --config /tmp/ray/session_latest/metrics/grafana/grafana.ini web</span><br></pre></td></tr></table></figure>
<p>将grafana dashboard加入到已有的grafana server可以参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/cluster/metrics.html#simplest-setting-up-grafana-with-ray-provided-configurations">官方手册</a>。</p>
<p>可以在这个地址上查看grafana的dashboard：http://localhost:3000</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191218231.png"
alt="grafana dashboard" />
<figcaption aria-hidden="true">grafana dashboard</figcaption>
</figure>
<p><strong>Ray dashboard</strong></p>
<p>完成上述两个步骤后，Ray
dashboard中的metric即可正常显示，如果不是本机部署，你可能需要配置允许所有ip访问：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RAY_GRAFANA_HOST=http://192.168.64.8:3000 ray start --head --dashboard-host=0.0.0.0</span><br></pre></td></tr></table></figure>
<p><code>RAY_GRAFANA_HOST</code>的作用是让ray的dashboard能够访问到grafana服务；</p>
<p><code>--dashboard-host=0.0.0.0</code>允许所有ip访问。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191317452.png"
alt="Ray dashboard" />
<figcaption aria-hidden="true">Ray dashboard</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191504505.png"
alt="Ray dashboard" />
<figcaption aria-hidden="true">Ray dashboard</figcaption>
</figure>
<h3 id="提交一个任务">提交一个任务</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python pi.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:17:11,760 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 192.168.64.8:6379...</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:17:11,775 INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at 192.168.64.8:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Estimated π: 3.36</span></span></span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191829305.png"
alt="Ray 任务列表" />
<figcaption aria-hidden="true">Ray 任务列表</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191902742.png"
alt="Ray 任务详情" />
<figcaption aria-hidden="true">Ray 任务详情</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191917649.png"
alt="Ray 任务详情" />
<figcaption aria-hidden="true">Ray 任务详情</figcaption>
</figure>
<h3 id="部署一个服务">部署一个服务</h3>
<p>Ray除了提供基础的分布式计算能力之外，还提供了一系列的AI
libs，其中可以在其上部署服务，Ray自动提供proxy和负载均衡能力。这里使用一个翻译的服务举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> starlette.requests <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ray</span><br><span class="line"><span class="keyword">from</span> ray <span class="keyword">import</span> serve</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@serve.deployment(<span class="params">num_replicas=<span class="number">2</span>, ray_actor_options=&#123;<span class="string">&quot;num_cpus&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;num_gpus&quot;</span>: <span class="number">0</span>&#125;</span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Translator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Load model</span></span><br><span class="line">        self.model = pipeline(<span class="string">&quot;translation_en_to_fr&quot;</span>, model=<span class="string">&quot;t5-small&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">translate</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># Run inference</span></span><br><span class="line">        model_output = self.model(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Post-process output to return only the translation text</span></span><br><span class="line">        translation = model_output[<span class="number">0</span>][<span class="string">&quot;translation_text&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> translation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, http_request: Request</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        english_text: <span class="built_in">str</span> = <span class="keyword">await</span> http_request.json()</span><br><span class="line">        <span class="keyword">return</span> self.translate(english_text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">translator_app = Translator.bind()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    ray.init()</span><br><span class="line">    serve.start(http_options=&#123;<span class="string">&quot;host&quot;</span>: <span class="string">&quot;0.0.0.0&quot;</span>&#125;)  <span class="comment"># 设置监听地址为 0.0.0.0</span></span><br><span class="line">    serve.run(translator_app)</span><br></pre></td></tr></table></figure>
<p>具体修改方法，可以参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/serve/getting_started.html">官方手册</a></p>
<p>直接运行这个python程序即可完成服务的部署：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">python translate.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:23:45,794 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 192.168.64.8:6379...</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:23:45,810 INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at 192.168.64.8:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:46,673 serve 8302 -- Started Serve in namespace &quot;serve&quot;.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:46,675 serve 8302 -- Connecting to existing Serve app in namespace &quot;serve&quot;. New http options will not be applied.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#WARNING 2025-01-21 11:23:46,675 serve 8302 -- The new client HTTP config differs from the existing one in the following fields: [&#x27;host&#x27;, &#x27;location&#x27;]. The new HTTP config is ignored.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeController pid=6931) INFO 2025-01-21 11:23:46,687 controller 6931 -- Deploying new version of Deployment(name=&#x27;Translator&#x27;, app=&#x27;default&#x27;) (initial target replicas: 2).</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ProxyActor pid=8045) INFO 2025-01-21 11:23:46,644 proxy 192.168.64.8 -- Proxy starting on node 8e8707766c1fc9b7d838c24446c99440be5881c04ea44b6e4e83a7aa (HTTP port: 8000).</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ProxyActor pid=8045) INFO 2025-01-21 11:23:46,660 proxy 192.168.64.8 -- Got updated endpoints: &#123;&#125;.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ProxyActor pid=8045) INFO 2025-01-21 11:23:46,690 proxy 192.168.64.8 -- Got updated endpoints: &#123;Deployment(name=&#x27;Translator&#x27;, app=&#x27;default&#x27;): EndpointInfo(route=&#x27;/&#x27;, app_is_cross_language=False)&#125;.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeController pid=6931) INFO 2025-01-21 11:23:46,792 controller 6931 -- Adding 2 replicas to Deployment(name=&#x27;Translator&#x27;, app=&#x27;default&#x27;).</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeReplica:default:Translator pid=8044) Device set to use cpu</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:50,711 serve 8302 -- Application &#x27;default&#x27; is ready at http://0.0.0.0:8000/.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:50,711 serve 8302 -- Deployed app &#x27;default&#x27; successfully.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeReplica:default:Translator pid=2371, ip=192.168.64.9) Device set to use cpu</span></span></span><br></pre></td></tr></table></figure>
<p>在dashboard上可以看到服务的详情：</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121192836367.png"
alt="Ray Serve详情" />
<figcaption aria-hidden="true">Ray Serve详情</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121192851056.png"
alt="Ray Serve metrics" />
<figcaption aria-hidden="true">Ray Serve metrics</figcaption>
</figure>
<p>通过curl命令可以验证服务运行情况：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://127.0.0.1:8000/ -H &quot;Content-Type: application/json&quot; -d &#x27;&quot;Hello world!&quot;&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Bonjour monde!</span></span></span><br></pre></td></tr></table></figure>
<h2 id="调试">调试</h2>
<p>Ray是一个多进程，Python和C++混合调用的程序（以Python语言为例），调试上需要掌握一定的技巧。调试Python，Driver，以及自动拉起的gcs_server，raylet以及worker，actor的方法都不同。下面以VsCode为例。</p>
<h3 id="调试python">调试python</h3>
<p>Python调试与普通程序调试相同，直接点debug
python文件，或者配置launch.json即可。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Python: pi.py&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;debugpy&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/pi.py&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;console&quot;</span><span class="punctuation">:</span> <span class="string">&quot;integratedTerminal&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;justMyCode&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="调试driver">调试Driver</h3>
<p>Driver就是用户python程序，调试Driver的Python部分参考上一节，如果调试Driver的C++部分，需要调试python进程，前提是Ray是debug编译的，否则没有符号表无法调试。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Ray C++&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/home/hua/miniconda3/envs/myenv/bin/python3.9&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;$&#123;workspaceFolder&#125;/pi.py&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stopAtEntry&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;externalConsole&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;MIMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;setupCommands&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;为 gdb 启用整齐打印&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-enable-pretty-printing&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;将反汇编风格设置为 Intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-gdb-set disassembly-flavor intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="调试worker或者其他进程">调试Worker或者其他进程</h3>
<p>gcs_server，raylet，worker以及actor都是自动拉起的进程，调试的时候需要attach到这些进程上进行调试。</p>
<blockquote>
<p>注意，需要接触gdb attach的限制，永久接触方法如下：</p>
<p>sudo vi /etc/sysctl.d/10-ptrace.conf</p>
<p>kernel.yama.ptrace_scope = 0</p>
<p>sudo sysctl --system</p>
</blockquote>
<p>调试上述pi.py，在一个worker的情况下大概需要8G内存，否则会导致Ray
kill掉worker或者gdb异常退出。在调试worker过程中，为了方便，可以限制仅启动一个worker，在本地拉起的情况下，配置<code>ray.init(num_cpus=1)</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attach to worker&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;attach&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;processId&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;command:pickProcess&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/home/hua/miniconda3/envs/myenv/bin/python3.9&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;sourceFileMap&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;/proc/self/cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/home/hua/code/ray&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;setupCommands&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;为 gdb 启用整齐打印&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-enable-pretty-printing&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;将反汇编风格设置为 Intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-gdb-set disassembly-flavor intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>worker和actor是python进程，gcs_server和reylet是非python进程，二进制在
ray/python/ray/core/src/ray/下。</p>
<h2 id="grpc流程">gRPC流程</h2>
<h3 id="grpc是什么">gRPC是什么</h3>
<p>简单来说，RPC框架就是像调用本地函数一样调用远程函数。gRPC使用protobuf来定义服务和传输的对象，在客户端中，有一个存根（Stub），与服务有相同的函数签名，通过调用这个存根，即可完成一次RPC调用。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/landing-2.svg"
alt="gRPC原理" />
<figcaption aria-hidden="true">gRPC原理</figcaption>
</figure>
<p>Ray是基于gRPC构建的分布式计算系统，有关gRPC的代码存放在
ray/src/ray/rpc目录下，下面，我们通过worker进程的gRPC服务来分析。</p>
<h3 id="grpc-client">gRPC client</h3>
<p>涉及到gRPC client的几个文件：<code>grpc_client.h</code>,
<code>client_call.h</code></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/20250122145957615.png"
alt="gRPC client类图" />
<figcaption aria-hidden="true">gRPC client类图</figcaption>
</figure>
<p><code>ClientCall</code>是对RPC调用的一个封装，主要包括需要调用的stub函数指针，以及相关的状态和结果获取，当gRPC调用返回时，需要回调<code>ClientCall</code>中注册的回调函数。<code>ClientCallManager</code>是对gRPC调用发起的管理，包括结果队列，监听线程等，
<code>GrpcClient</code>保存的是gRPC的连接句柄，可以通过该对象发起一个gRPC请求。</p>
<p>对于CoreWorker来说，在此之上还有一层封装（<code>worker/core_worker_client.h</code>,
<code>worker/core_worker_client_pool.h/cc</code>），<code>CoreWorkerClient</code>，该类封装了CoreWorkerService可用的所用调用，直接调用提供的函数接口即可完成RPC调用。与之匹配的还有一个<code>CoreWorkerClientPool</code>，用于<code>CoreWorkerClient</code>的缓存。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250122142401278.png"
alt="CoreWorkerClient 类图" />
<figcaption aria-hidden="true">CoreWorkerClient 类图</figcaption>
</figure>
<p><code>CoreWorkerClientPool</code>维护一个map&lt;WorkerId,
CoreWorkerClient&gt;，当已经存在对应的<code>CoreWorkerClient</code>时直接取出使用。如果不存在，则调用<code>CoreWorkerClientFactoryFn</code>工厂方法创建一个gRPC的client连接。该工厂方法在<code>CoreWorker</code>的构造函数中定义，通过一个<code>rpc::Address</code>创建对应的<code>CoreWorkerClient</code>对象。</p>
<p>每个<code>CoreWorkerClient</code>对象构造过程中，会创建gRPC连接，并且通过<code>ClientCallManager</code>来发起RPC请求，并通过监听CompletionQueue来响应RPC的处理结果。</p>
<h3 id="grpc-server">gRPC server</h3>
<p>涉及到gRPC server的几个文件：<code>grpc_server.h/cc</code>,
<code>client_server.h/cc</code></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/20250122145823773.png"
alt="gPRC Server类图" />
<figcaption aria-hidden="true">gPRC Server类图</figcaption>
</figure>
<p><code>GrpcServer</code>是gRPC的服务端，其中定义了初始化，关闭，注册服务，运行等操作。它会根据其中注册的Service来向gRPC服务中注册服务和对应的处理方法。<code>GrpcService</code>是一个虚拟类，其本身没有实现，需要不同的组件来继承实现，例如，CoreWorker就会用<code>CoreWorkerGrpcService</code>来实现一个Worker对应的Service。Service中需要提供一组<code>ServiceCallFactory</code>，这些Factory记录了服务，gRPC的stub，回调函数，本地异步IO组件等信息，供<code>GrpcServer</code>来注册对应的服务。<code>ServiceCall</code>即服务端服务的本身，包括一系列回调函数处理对应的事件，这个call对象会以Tag的方式放入gRPC请求中，处理时取出call对象对相应的处理。</p>
<p>对于CoreWorker来说，需要基于<code>GrpcService</code>实现<code>GrpcCoreWorkerGrpcService</code>（<code>work/core_worker_server.h</code>）。实际上的工作就是将CoreWorkerService中的服务全部注册到ServiceCallFactory中。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250122152145818.png"
alt="CoreWorkerService 类图" />
<figcaption aria-hidden="true">CoreWorkerService 类图</figcaption>
</figure>
<p><code>CoreWorkerServiceHandler</code>是一组Handle方法的集合，包含CoreWorkerService中的所有服务的处理方法，<code>CoreWorkerGrpcService</code>中会通过一组宏来构造protobuf中的注册，响应等必要信息的对象集合（ServiceCallFactory）。</p>
<p>注册完成后，<code>GrpcServer</code>在运行之前，会将所有的事件和响应注册到队列中，这样，队列中进入事件时，就可以调用对应的处理函数进行处理。</p>
<h3 id="本地异步调用">本地异步调用</h3>
<p>Ray使用了大量的异步处理，例如gRPC框架中的请求和响应，以及本地的异步处理框架。Ray的Worker等进程中，除了gRPC的异步框架之外，还有一个<code>boost::asio::io_context</code>框架，所有gRPC的响应并不是在pull_threads中处理，而是把事件转交给本地的异步处理框架，然后在该异步处理框架中处理。并且该框架中还内置了一个EventTracker，来记录所有时间的处理信息。</p>
<p>结果的处理交给本地异步处理来运行，猜测是为了加快gRPC队列中的数据消费。</p>
<h2 id="driver提交流程">Driver提交流程</h2>
<p>以无状态任务为例，描述任务的提交流程。</p>
<h3 id="python部分">Python部分</h3>
<p><strong><span class="citation"
data-cites="ray.remote">@ray.remote</span></strong></p>
<p>被<code>@ray.remote</code>装饰的函数会被ray分布式处理。该装饰器会将函数（或者对象，后续的描述均为函数的装饰）封装成<code>RemoteFunction</code>对象。该对象保存了被装饰函数的function对象，并且提供<code>remote</code>方法。</p>
<p>当<code>remote</code>方法被调用时，会将python函数包装成<code>PythonFunctionDescriptor</code>,记录了module/function/class
name，以及分配的uuid。随后使用pickle_dump将函数序列化，交给worker处理。worker会将序列化后的函数存储到gcs服务的function
table中，并记录该函数的uuid，以便于通过函数描述找到函数体。</p>
<p>以上工作完成后，remote方法会调用worker的submit_task方法提交任务，该任务即可通过gRPC发送到集群中处理。submit_task返回一个object_ref。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">invocation (/home/hua/code/ray/python/ray/remote_function.py:485)</span><br><span class="line">_remote (/home/hua/code/ray/python/ray/remote_function.py:504)</span><br><span class="line">_invocation_remote_span (/home/hua/code/ray/python/ray/util/tracing/tracing_helper.py:310)</span><br><span class="line">auto_init_wrapper (/home/hua/code/ray/python/ray/_private/auto_init_hook.py:21)</span><br><span class="line">_remote_proxy (/home/hua/code/ray/python/ray/remote_function.py:156)</span><br><span class="line">&lt;listcomp&gt; (/home/hua/code/ray/pi.py:23)</span><br><span class="line">calculate_pi (/home/hua/code/ray/pi.py:22)</span><br><span class="line">&lt;module&gt; (/home/hua/code/ray/pi.py:39)</span><br></pre></td></tr></table></figure>
<h3 id="driver部分">Driver部分</h3>
<p><strong>任务提交到本地</strong></p>
<p>Driver的python代码调用submit_task后，会通过cython调用到C++
extention中。对应的函数是<code>CoreWorker::SubmitTask</code>，这里会将相关的任务信息打包成<code>TaskSpec</code>，然后提交到本地异步IO中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::core::CoreWorker::SubmitTask() (/home/hua/code/ray/src/ray/core_worker/core_worker.cc:2467)</span><br><span class="line"></span><br><span class="line">cython ...</span><br><span class="line">Python ...</span><br></pre></td></tr></table></figure>
<p><strong>解决依赖</strong></p>
<p>从异步IO调度到该任务后(<code>NormalTaskSubmitter::SubmitTask</code>)，会先等待依赖的资源处理结束，这里使用了回调的方式异步等待依赖的任务结束。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::SubmitTask() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:23)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()(const struct &#123;...&#125; * const __closure) (/home/hua/code/ray/src/ray/core_worker/core_worker.cc:2469)</span><br><span class="line"></span><br><span class="line">_raylet.so!EventTracker::RecordExecution() (/home/hua/code/ray/src/ray/common/event_stats.cc:113)</span><br><span class="line"></span><br><span class="line">_raylet.so!std::_Function_handler&lt;void(), instrumented_io_context::post() (/home/hua/code/ray/src/ray/common/asio/instrumented_io_context.cc:97)</span><br><span class="line"></span><br><span class="line">从异步IO调度</span><br></pre></td></tr></table></figure>
<p><strong>请求资源</strong></p>
<p>依赖的任务执行结束后，准备执行当前任务，但是对当前SchedulingKey来说目前没有空闲的Worker，需要先向reylet请求Worker，<code>NormalTaskSubmitter::RequestNewWorkerIfNeeded</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::RequestNewWorkerIfNeeded() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:347)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()(struct &#123;...&#125; * const __closure, ray::Status status) (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:80)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::LocalDependencyResolver::ResolveDependencies() (/home/hua/code/ray/src/ray/core_worker/transport/dependency_resolver.cc:84)</span><br><span class="line"></span><br><span class="line">异步回调</span><br></pre></td></tr></table></figure>
<p><strong>任务提交到集群</strong></p>
<p>Worker资源异步请求会返回空闲Worker的Address，然后可以通过RPC将任务直接提交(<code>PushTask</code>)给这个Worker。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::rpc::CoreWorkerClient::PushNormalTask() (/home/hua/code/ray/src/ray/rpc/worker/core_worker_client.h:399)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::PushNormalTask() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:561)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::OnWorkerIdle() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:181)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:436)</span><br><span class="line"></span><br><span class="line">RPC回调</span><br></pre></td></tr></table></figure>
<h2 id="worker执行流程">Worker执行流程</h2>
<p><strong>调试技巧</strong></p>
<ol type="1">
<li>首先启动Driver，在<code>INVOKE_RPC_CALL</code>执行之前打断点，阻塞任务提交到集群。</li>
<li>attach到Worker进程上，并且在<code>CoreWorker::HandlePushTask</code>打断点，这里是处理RPC请求的入口。</li>
<li>让Driver继续执行，Worker就会命中断点，可以继续调试Worker。</li>
</ol>
<p><strong>gRPC将任务提交到本地</strong></p>
<p><code>GrpcServer::PollEventsFromCompletionQueue</code>会等待gRPC请求，当收到请求后，就会调用从Tag中取出ServerCall对象，该对象中保存着该请求的所有处理的必要信息。然后将该任务提交给异步IO。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::rpc::ServerCallImpl&lt;ray::rpc::CoreWorkerServiceHandler, ray::rpc::GetCoreWorkerStatsRequest, ray::rpc::GetCoreWorkerStatsReply, (ray::rpc::AuthType)0&gt;::HandleRequest() (/home/hua/code/ray/bazel-out/aarch64-dbg/bin/_virtual_includes/grpc_common_lib/ray/rpc/server_call.h:237)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::rpc::GrpcServer::PollEventsFromCompletionQueue() (/home/hua/code/ray/src/ray/rpc/grpc_server.cc:199)</span><br><span class="line"></span><br><span class="line">gRPC pulling thread</span><br></pre></td></tr></table></figure>
<p><strong>调用gRPC注册的Handler方法</strong></p>
<p>异步IO会回调注册的方法<code>CoreWorker::HandlePushTask</code>，配置定义<code>send_reply_callback</code>回调函数，最后将任务通过异步IO提交给<code>task_execution_service</code>（就是另外一个异步IO队列）。</p>
<p><strong>执行函数</strong></p>
<p>当执行调度到PushTask任务时，就会回调上一步配置的<code>send_reply_callback</code>回调，远程函数的执行就在这个回调中运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!operator()(const struct &#123;...&#125; * const __closure, const ray::TaskSpecification &amp; task_spec, ray::rpc::SendReplyCallback send_reply_callback) (/home/hua/code/ray/src/ray/core_worker/transport/task_receiver.cc:100)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::InboundRequest::Accept(ray::core::InboundRequest * const this) (/home/hua/code/ray/src/ray/core_worker/transport/actor_scheduling_util.cc:36)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::NormalSchedulingQueue::ScheduleRequests(ray::core::NormalSchedulingQueue * const this) (/home/hua/code/ray/src/ray/core_worker/transport/normal_scheduling_queue.cc:87)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::TaskReceiver::RunNormalTasksFromQueue(ray::core::TaskReceiver * const this) (/home/hua/code/ray/src/ray/core_worker/transport/task_receiver.cc:294)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()(const struct &#123;...&#125; * const __closure) (/home/hua/code/ray/src/ray/core_worker/core_worker.cc:3777)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>回调中的<code>task_handler_</code>
就是注册进去的<code>CoreWorker::ExecuteTask</code>，将这个对象封装成了一个lamda函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> execute_task = std::<span class="built_in">bind</span>(&amp;CoreWorker::ExecuteTask,</span><br><span class="line">                                  <span class="keyword">this</span>,</span><br><span class="line">                                  std::placeholders::_1,</span><br><span class="line">                                  std::placeholders::_2,</span><br><span class="line">                                  std::placeholders::_3,</span><br><span class="line">                                  std::placeholders::_4,</span><br><span class="line">                                  std::placeholders::_5,</span><br><span class="line">                                  std::placeholders::_6,</span><br><span class="line">                                  std::placeholders::_7,</span><br><span class="line">                                  std::placeholders::_8);</span><br></pre></td></tr></table></figure>
<p>最终调用到了<code>options_.task_execution_callback</code>，这个callback会根据语言的不同而不同，以Python为例，这个callback调用的是注册进来的一个Python方法，将Python的远程函数交还给Python解释器来执行。这部分代码在ray/python/ray/_raylet.pyx中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cdef void execute_task: return function(actor, *arguments, **kwarguments)</span><br><span class="line">cdef execute_task_with_cancellation_handler: execute_task</span><br><span class="line">cdef CRayStatus task_execution_handler: execute_task_with_cancellation_handler</span><br><span class="line">CoreWorker::__cinit__: options.task_execution_callback = task_execution_handler</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/13/OpenCV%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/11/13/OpenCV%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/" class="post-title-link" itemprop="url">OpenCV昇腾原生支持</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-13 15:40:05" itemprop="dateCreated datePublished" datetime="2024-11-13T15:40:05+08:00">2024-11-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:47:20" itemprop="dateModified" datetime="2025-01-23T15:47:20+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>OpenCV</strong> (Open Source Computer Vision Library)
是一个开源计算机视觉和机器学习软件库，由Intel在1999年发布。OpenCV提供了丰富的图像和视频处理功能，广泛应用于各种计算机视觉任务，如面部识别、物体检测、运动跟踪、图像增强等。它支持多种编程语言（如C++、Python、Java等）和操作系统（如Windows、Linux、macOS等），并且可以与其他深度学习框架（如TensorFlow、PyTorch）无缝集成。</p>
<p><strong>OpenCV-contrib</strong>
是一个附加模块的集合，为OpenCV核心库提供扩展功能。由于OpenCV核心库为了保持稳定性，通常只包含相对成熟和通用的模块，而OpenCV-contrib则提供了实验性和前沿的功能。这些模块包括一些新的特征检测算法、图像处理技术、深度学习工具、3D重建等。OpenCV-contrib项目中的模块也可能包含特定领域的工具包，例如面部识别、人脸标志检测、目标跟踪等。</p>
<p>在计算机视觉和深度学习任务中，硬件加速器（如GPU、TPU、NPU等）被广泛用于加速计算。OpenCV的性能也可以通过这些硬件加速器来提高，这通常通过使用专用的后端硬件加速库来实现。</p>
<h2 id="需求分析">1. 需求分析</h2>
<p>华为Ascend系列AI处理器，凭借其强大的计算能力和高能效，在各种AI应用场景中得到了广泛应用。为了充分发挥Ascend硬件的优势，基于Ascend的OpenCV后端加速项目旨在利用Ascend的计算能力，加速OpenCV库中的部分核心算法。</p>
<h3 id="目标">1.1 目标</h3>
<p>开发一个基于华为Ascend
AI处理器的OpenCV硬件加速后端，优化并加速OpenCV中的特定算法。该后端将通过集成Ascend
Computing Library (ACL)
来实现对OpenCV算法的硬件加速，从而提升计算性能，减少延迟，并提高能源效率。</p>
<h3 id="需求概述">1.2 需求概述</h3>
<p><strong>支持的算法类型</strong></p>
<ul>
<li>识别出OpenCV中最常用的传统图像算法，并优先为这些算法实现Ascend后端加速支持。</li>
<li>常见的候选算法包括算数运算，图像变换，色域转换等等。</li>
<li>根据Ascend硬件特性，可能需要修改或重新设计部分算法，以适应硬件架构，进一步优化性能。</li>
</ul>
<p><strong>实现Ascend Runtime</strong></p>
<ul>
<li>实现Ascend设备控制、Device-Host内存复制、流与事件管理等方面的功能。</li>
</ul>
<p><strong>异步算子支持</strong></p>
<ul>
<li>接口调用使用异步任务提交，实现异步计算结果获取，提高设备利用率，系统的响应速度与吞吐量。</li>
<li>支持ACL算子调用能力，以及AscendC自定义算子。</li>
<li>支持OpenCV矩阵结构向ACL矩阵结构转换能力。</li>
</ul>
<p><strong>兼容性与集成</strong></p>
<ul>
<li>确保该后端能够无缝集成到现有的OpenCV框架中，用户无需进行大量修改即可使用加速功能。</li>
<li>保证与OpenCV其他后端的兼容性，用户可以根据具体硬件环境选择最优的加速方案。</li>
</ul>
<p><strong>性能评估与优化</strong></p>
<ul>
<li>针对不同算法，设计性能评估测试用例，并基于测试结果持续优化算法与后端实现。</li>
<li>与基线（CPU）性能进行对比，明确性能优势与改进方向。</li>
</ul>
<p><strong>文档与用户指南</strong></p>
<ul>
<li>编写详细的开发文档与用户指南，帮助开发者理解如何使用该后端进行算法加速。</li>
<li>提供API描述、使用示例、常见问题解答、性能优化建议等。</li>
</ul>
<h2 id="架构设计">2. 架构设计</h2>
<h3 id="opencv项目架构">2.1 OpenCV项目架构</h3>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/29/6188c31faa10d14953661334c757720b-20240829102417-e9056e.png"
alt="OpenCV项目架构" />
<figcaption aria-hidden="true">OpenCV项目架构</figcaption>
</figure>
<p>OpenCV内部模块较多，如上图所示，这是一个简化的OpenCV整体架构。按模块类型可以分为以下几类（含内部模块举例）：</p>
<p><strong>核心模块 (Core Modules)</strong></p>
<ul>
<li><strong>核心模块 (Core)</strong>
基础数据结构和算法（如矩阵运算、线性代数）。</li>
<li><strong>图像处理 (Imgproc)</strong>
图像滤波、形态学变换、边缘检测等。</li>
<li><strong>视频处理 (Video)</strong>
视频捕捉、帧处理、运动检测等。</li>
<li><strong>相机校正 (Calib3d)</strong>
相机标定、3D重建、立体匹配等。</li>
<li><strong>特征检测 (Features2d)</strong>
特征点检测与描述子计算（如SIFT、ORB）。</li>
</ul>
<p><strong>算法库 (Algorithm Libraries)</strong></p>
<ul>
<li><p><strong>机器学习 (ML)</strong></p>
<p>支持分类、回归、聚类、神经网络等机器学习算法。</p></li>
<li><p><strong>对象检测 (Objdetect)</strong></p>
<p>人脸检测、目标跟踪等高级检测算法。</p></li>
<li><p><strong>图像分割 (Imgsegm)</strong></p>
<p>超像素分割、图像聚类等。</p></li>
</ul>
<p><strong>硬件加速 (Hardware Acceleration)</strong></p>
<ul>
<li><p><strong>CUDA 支持</strong></p>
<p>基于NVIDIA GPU的CUDA加速模块。</p></li>
<li><p><strong>OpenCL 支持</strong></p>
<p>基于OpenCL的跨平台硬件加速支持。</p></li>
<li><p><strong>Vulkan 支持</strong></p>
<p>基于Vulkan的图像处理加速。</p></li>
</ul>
<p><strong>第三方集成 (Third-Party Integrations)</strong></p>
<ul>
<li><p><strong>Python 接口</strong></p>
<p>提供对Python的API绑定。</p></li>
<li><p><strong>Java 接口</strong></p>
<p>提供对Java的API绑定。</p></li>
<li><p><strong>Android/IOS 支持</strong></p>
<p>移动设备上的OpenCV应用开发支持。</p></li>
</ul>
<p><strong>应用层 (Application Layer)</strong></p>
<ul>
<li><strong>图像和视频处理应用</strong></li>
<li><strong>增强现实 (AR)</strong></li>
<li><strong>机器人视觉</strong></li>
<li><strong>自动驾驶</strong></li>
</ul>
<h3 id="opencv-cann硬件加速模块架构">2.2 OpenCV
CANN硬件加速模块架构</h3>
<p>OpenCV的昇腾原生支持将在硬件加速(Hardware Acceleration)中添加对Ascend
NPU的支持。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/29/ebcc2df8052dec2278815f533e23fd3c-ebcc2df8052dec2278815f533e23fd3c-20240829111755-4523ce-f49b69.png"
alt="OpenCV昇腾支持示意图" />
<figcaption aria-hidden="true">OpenCV昇腾支持示意图</figcaption>
</figure>
<p>针对该项目的需求，需要实现以下模块：</p>
<p><strong>Ascend Runtime</strong></p>
<ul>
<li><p><strong>设备控制</strong>：负责管理与Ascend
NPU硬件的通信和控制。</p></li>
<li><p><strong>设备-主机内存复制</strong>：处理数据在Ascend
NPU设备和主机之间的内存传输。</p></li>
<li><p><strong>流管理</strong>：管理计算任务流的调度和执行。</p></li>
<li><p><strong>事件管理</strong>：处理计算过程中的事件和同步问题。</p></li>
</ul>
<p><strong>AscendC内核</strong></p>
<ul>
<li><p><strong>AscendC构建框架</strong>：提供内核构建的工具和框架。</p></li>
<li><p><strong>内核实现</strong>：实现具体的计算内核，提供加速计算功能。</p></li>
<li><p><strong>内核调用管理</strong>：管理内核的调用过程和参数。</p></li>
<li><p><strong>内核结果获取</strong>：从内核执行中获取计算结果。</p></li>
</ul>
<p><strong>ACL算子</strong></p>
<ul>
<li><p><strong>OpenCV到ACL结构转换</strong>：将OpenCV数据结构转换为ACL支持的格式。</p></li>
<li><p><strong>算子编译和调用</strong>：编译并调用ACL算子来执行计算任务。</p></li>
<li><p><strong>异步结果获取</strong>：支持异步获取ACL算子的计算结果。</p></li>
</ul>
<p><strong>CANN模块</strong></p>
<ul>
<li><p><strong>cann_module</strong>：作为核心模块，定义各类数据结构，Allocator，Ascend
Runtime接口封装等。</p></li>
<li><p><strong>element_operator</strong>：处理基本的元素级操作，例如加法、乘法等。</p></li>
<li><p><strong>core</strong>：提供核心图像变换功能，例如，merge，flip等。</p></li>
<li><p><strong>cvtcolor</strong>：专门处理颜色空间转换操作，如RGB到灰度转换等。</p></li>
</ul>
<p><strong>接口和绑定</strong></p>
<ul>
<li><p><strong>C++接口</strong>：为C++应用程序提供接口，允许直接调用CANN模块的功能。</p></li>
<li><p><strong>Python绑定</strong>：为Python应用程序提供接口，方便用户在Python环境中使用CANN模块。</p></li>
</ul>
<p><strong>其他</strong></p>
<ul>
<li><strong>错误处理</strong>：负责管理和记录各类级别日志，方便问题排查定位。</li>
<li><strong>功能测试</strong>：确保模块各项功能的正确性和稳定性。</li>
<li><strong>性能测试</strong>：对模块进行性能测试，验证加速效果和计算效率。</li>
<li><strong>样例</strong>：提供示例代码，帮助用户了解如何使用CANN模块。</li>
<li><strong>教程</strong>：提供详细的用户指南和教程，帮助用户快速上手和使用CANN模块。</li>
</ul>
<p>CANN各个模块的依赖和调用关系如下图所示：</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/29/9fb69ef2f5087e843463fc6e5bff5206-20240829112417-e21481.png"
alt="OpenCV算法调用时序图" />
<figcaption aria-hidden="true">OpenCV算法调用时序图</figcaption>
</figure>
<h2 id="cann_module">3. Cann_Module</h2>
<p>CANN模块中定义了OpenCV中的关键结构体，AscendMat，AscendStream和AscendEvent。其中AscendMat结构与Mat结构类似，需要有与InputArray（各类Mat的通用结构）相互转换的能力。与其他后端的Mat，Stream和Event类型类似，需要实现以下接口。AscendMat中存储着矩阵的shape和数据，并且有Device-Host内存拷贝能力，以及类型转换等能力。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/29/cb0fb968ce95e0e8ab3be5e8900a7e6e-20240829113201-89a37a.png"
alt="Cann Module类图" />
<figcaption aria-hidden="true">Cann Module类图</figcaption>
</figure>
<h3 id="类和组件">3.1 类和组件</h3>
<p><strong>AscendMat 类</strong></p>
<p>AscendMat 是一个封装了 Ascend 设备内存的矩阵类，类似于 OpenCV 的 Mat
类，但专为 Ascend 硬件设计。它支持各种矩阵操作，并通过内部的 Allocator
进行内存管理。</p>
<p><strong>主要属性</strong>:</p>
<ul>
<li><p>Allocator* allocator：用于内存分配的分配器。</p></li>
<li><p>int
flags：包括魔术签名、连续性标志、深度和通道数等信息的位字段。</p></li>
<li><p>int rows, cols：矩阵的行数和列数。</p></li>
<li><p>size_t step：每行的字节数。</p></li>
<li><p>std::shared_ptr<uchar> data：指向矩阵数据的智能指针。</p></li>
<li><p>uchar* datastart, const uchar* dataend：辅助字段用于 ROI
定位和调整。</p></li>
</ul>
<p><strong>主要方法</strong>:</p>
<ul>
<li><p>构造函数和拷贝构造函数，用于初始化和复制矩阵。</p></li>
<li><p>setTo：设置矩阵中的所有元素。</p></li>
<li><p>create：分配新的矩阵数据。</p></li>
<li><p>upload 和 download：将数据上传到设备或从设备下载。</p></li>
<li><p>convertTo：将矩阵转换为其他数据类型。</p></li>
<li><p>isContinuous, elemSize, size
等方法用于获取矩阵的属性和信息。</p></li>
<li><p>defaultAllocator()：获取默认分配器。</p></li>
<li><p>setDefaultAllocator(Allocator*
allocator)：设置默认分配器。</p></li>
</ul>
<p><strong>Allocator 类</strong></p>
<p>Allocator 是 AscendMat 的内部类，用于处理内存分配。</p>
<p><strong>主要方法</strong>:</p>
<ul>
<li><p>allocate(size_t size)：分配指定大小的内存。</p></li>
<li><p>allocate(AscendMat* mat, int rows, int cols, size_t
elemSize)：为矩阵分配内存并初始化相关字段。</p></li>
</ul>
<p><strong>DefaultAllocator 类</strong></p>
<p>DefaultAllocator 继承自
Allocator，实现了具体的内存分配和释放方法。</p>
<p><strong>主要方法</strong>:</p>
<ul>
<li><p>allocate(size_t size)：使用 aclrtMalloc 分配内存。</p></li>
<li><p>allocate(AscendMat* mat, int rows, int cols, size_t elemSize)：为
AscendMat 分配内存并设置步幅。</p></li>
</ul>
<p><strong>AscendStream 类</strong></p>
<p>AscendStream 管理 Ascend
设备上的任务流，支持任务的异步执行和同步。</p>
<p><strong>主要方法</strong>:</p>
<ul>
<li><p>waitForCompletion()：阻塞当前线程直到流中的所有操作完成。</p></li>
<li><p>waitAscendEvent(const AscendEvent&amp;
event)：阻塞当前线程直到事件触发。</p></li>
<li><p>Null()：返回默认的空流对象。</p></li>
<li><p>addTensorHolder(const std::shared_ptr<uchar>&amp;
holder)：向流中添加张量持有者。</p></li>
</ul>
<p><strong>AscendEvent 类</strong></p>
<p>AscendEvent 用于流之间的同步。</p>
<p><strong>主要方法</strong>:</p>
<ul>
<li>record(AscendStream&amp; stream)：记录事件。</li>
<li>waitForComplete()：等待事件完成。</li>
</ul>
<h2 id="acl封装">4. ACL封装</h2>
<p>AscendTensor结构与AscendMat相对应，用于将AscendMat转换成Ascend亲和的格式。其中矩阵数据使用智能指针，用户任务异步执行。</p>
<p>OperatorRunner是算子执行的类，用于设置算子执行所需的算子名称，属性，输入和输出矩阵，该类的成员函数均返回自身指针，方便设置多个属性。</p>
<p>除此之外，ACL封装也为了隔离ACL相关的库，整个工程中仅在此文件中会依赖ACL相关符号，避免OpenCV和ACL库的过度耦合，除了下述类图之外，其他的设备管理均做了封装，例如初始化，去初始化，内存管理和拷贝等。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/30/42149854740023745a869eb4e6a93683-42149854740023745a869eb4e6a93683-20240830103751-eea9d0-40ba0e.png"
alt="ACL 模块类图" />
<figcaption aria-hidden="true">ACL 模块类图</figcaption>
</figure>
<p>ACL算子允许异步提交，来提高硬件利用率，提高数据处理吞吐。所以当某个算子任务提交后，无法直接判断其执行进度，所以需要对矩阵数据进行保存，避免计算完成前数据被释放。</p>
<p>算子提交后，其智能指针会保存到AscendStream的tensorHolder中，即使超出某个AscendTensor的生命周期，该部分数据仍会保存，直到Stream
Sync后，这些tensor才会真正释放。</p>
<p>为了避免内存浪费，可以在AscendStream中插入AscendEvent，通过判断Event是否完成，来判断有那些tensor已经计算完成，可以尽快释放。</p>
<p>为了进一步提高内存的分配效率，后续可以添加内存池，由应用来进行Device上的内存管理，避免频繁调用Device的内存申请释放。</p>
<h2 id="支持的算法">5. 支持的算法</h2>
<h3 id="算数计算">5.1 算数计算</h3>
<p>本节详细描述针对 Ascend 硬件加速器的 OpenCV
算术操作实现。包括各种算术操作的计算公式及其在代码中的实现细节。</p>
<p><strong>加法(Add)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像对应像素值的和。支持输入图像的类型为
AscendMat 和 Scalar。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = alpha\times\text{src1}(i, j) +
beta\times\text{src2}(i, j)
\]</span></p></li>
<li><p><strong>说明</strong>：alpha 和 beta
参数允许对输入图像进行加权和调整。</p></li>
</ul>
<p><strong>减法 (subtract)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像对应像素值的差。支持输入图像的类型为
AscendMat 和 Scalar。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = \text{src1}(i, j) - \text{src2}(i, j)
\]</span></p></li>
</ul>
<p><strong>乘法 (multiply)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像对应像素值的乘积。支持输入图像的类型为
AscendMat 和 Scalar，还支持缩放因子的应用。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = scale\times(\text{src1}(i, j) \times \text{src2}(i,
j))
\]</span></p></li>
<li><p><strong>说明</strong>：scale
参数允许对结果进行缩放，以调整输出图像的亮度或对比度。</p></li>
</ul>
<p><strong>除法 (divide)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像对应像素值的商。支持输入图像的类型为
AscendMat 和 Scalar，还支持缩放因子的应用。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = scale\times\frac{\text{src1}(i, j)}{\text{src2}(i,
j)}
\]</span></p></li>
<li><p><strong>说明</strong>：scale
参数用于调整除法结果的缩放。</p></li>
</ul>
<p><strong>按位与 (bitwise_and)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像对应像素值的按位与操作。支持输入图像的类型为
AscendMat 和 Scalar。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = \text{src1}(i, j) \&amp; \text{src2}(i, j)
\]</span></p></li>
<li><p><strong>说明</strong>：用于图像的遮罩和掩盖操作。</p></li>
</ul>
<p><strong>按位或 (bitwise_or)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像对应像素值的按位或操作。支持输入图像的类型为
AscendMat 和 Scalar。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = \text{src1}(i, j) \| \text{src2}(i, j)
\]</span></p></li>
<li><p><strong>说明</strong>：用于图像合成和区域提取。</p></li>
</ul>
<p><strong>按位异或 (bitwise_xor)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像对应像素值的按位异或操作。支持输入图像的类型为
AscendMat 和 Scalar。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = \text{src1}(i, j) \oplus \text{src2}(i, j)
\]</span></p></li>
<li><p><strong>说明</strong>：用于图像的特殊编码和数据加密。</p></li>
</ul>
<p><strong>按位取反 (bitwise_not)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算图像像素值的按位取反操作。支持输入图像的类型为
AscendMat。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = \sim \text{src}(i, j)
\]</span></p></li>
<li><p><strong>说明</strong>：用于反转图像中的每个像素值。</p></li>
</ul>
<p><strong>加权和 (addWeighted)</strong></p>
<ul>
<li><p><strong>描述</strong>：计算两个输入图像的加权和。支持输入图像的类型为
AscendMat 和 Scalar，并且可以指定加权系数和加法常数。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{dst}(i, j) = \text{alpha} \times \text{src1}(i, j) + \text{beta}
\times \text{src2}(i, j) + \text{gamma}
\]</span></p></li>
<li><p><strong>说明</strong>：alpha 和 beta 用于加权输入图像，gamma
用于加法常数。</p></li>
</ul>
<p><strong>Threshold</strong></p>
<ul>
<li><p><strong>描述</strong>：对输入图像中的像素值进行阈值操作，根据指定的阈值和类型，将像素值调整为新的值。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
Binary:
\text{dst}(i, j) =
\begin{cases}
\text{maxVal} &amp; \text{if } \text{src}(i, j) &gt; \text{thresh} \\
0 &amp; \text{otherwise}
\end{cases}
\\
\\
Binary Inverted:
\text{dst}(i, j) =
\begin{cases}
0 &amp; \text{if } \text{src}(i, j) &gt; \text{thresh} \\
\text{maxVal} &amp; \text{otherwise}
\end{cases}
\\
\\
Truncate:
\text{dst}(i, j) =
\begin{cases}
\text{thresh} &amp; \text{if } \text{src}(i, j) &gt; \text{thresh} \\
\text{src}(i, j) &amp; \text{otherwise}
\end{cases}
\\
\\
To Zero:
\text{dst}(i, j) =
\begin{cases}
\text{src}(i, j) &amp; \text{if } \text{src}(i, j) &gt; \text{thresh} \\
0 &amp; \text{otherwise}
\end{cases}
\\
\\
To Zero Inverted:
\text{dst}(i, j) =
\begin{cases}
0 &amp; \text{if } \text{src}(i, j) &gt; \text{thresh} \\
\text{src}(i, j) &amp; \text{otherwise}
\end{cases}
\]</span></p></li>
<li><p><strong>说明</strong>：Threshold
操作广泛用于图像分割和预处理阶段。不同的阈值类型允许对图像中的不同区域进行区分和处理。</p></li>
</ul>
<p>上述算法接口较为类似，为了避免重复代码，需要将此类函数调用使用模板的方式进行抽象。</p>
<p>接口分为外部接口和内部接口，外部接口是对内部接口的封装，避免代码重复和额外的数据类型转换。</p>
<h3 id="图像核心算法">5.2 图像核心算法</h3>
<p>这段代码是 OpenCV 项目中的一部分，专门为 Ascend
硬件加速器提供了图像处理操作的实现。这些操作包括数据格式转换、图像合并与分割、转置、翻转、旋转、裁剪和缩放。以下是对每个操作的简要介绍：</p>
<p><strong>数据转换 (transData)</strong></p>
<ul>
<li><p><strong>描述</strong>：将输入数据从一种格式转换为另一种格式，例如从
NCHW 转换为 NHWC。</p></li>
<li><p><strong>说明</strong>：此函数用于将输入图像或矩阵的存储格式在不同的维度顺序之间进行转换，以适应不同的深度学习模型或计算需求。</p></li>
</ul>
<p><strong>图像合并 (merge)</strong></p>
<ul>
<li><p><strong>描述</strong>：将多个输入矩阵按通道维度合并为一个矩阵。输入矩阵的数量和类型必须相同。</p></li>
<li><p><strong>计算公式</strong>：</p></li>
</ul>
<p><span class="math display">\[
\text{dst}(x, y) = \begin{bmatrix}
\text{B}(x, y) \
\text{G}(x, y) \
\text{R}(x, y)
\end{bmatrix}
\]</span></p>
<ul>
<li><strong>说明</strong>：此函数在图像处理和深度学习中，用于将多通道图像合并为单一矩阵，以适应后续的处理或模型输入要求。</li>
</ul>
<p><strong>图像分割 (split)</strong></p>
<ul>
<li><strong>描述</strong>：将一个多通道矩阵按通道维度分割为多个单通道矩阵。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
\text{B}(x, y) = \text{dst}(x, y)[0] \\
\text{G}(x, y) = \text{dst}(x, y)[1] \\
\text{R}(x, y) = \text{dst}(x, y)[2]
\]</span></p>
<ul>
<li><strong>说明</strong>：此函数用于将多通道图像分割成独立的单通道图像，通常用于图像分析和预处理。</li>
</ul>
<p><strong>转置 (transpose)</strong></p>
<ul>
<li><strong>描述</strong>：对输入矩阵执行转置操作，交换指定的维度。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
\text{dst}(i, j) = \text{src}(j, i)
\]</span></p>
<ul>
<li><strong>说明</strong>：转置操作通常用于调整矩阵的维度顺序，以适应特定的算法或网络层要求。</li>
</ul>
<p><strong>翻转 (flip)</strong></p>
<ul>
<li><strong>描述</strong>：根据指定的轴对图像进行翻转操作。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
水平翻转：\text{dst}(x, y) = \text{src}(x, H - 1 - y) \\
垂直翻转：\text{dst}(x, y) = \text{src}(H - 1 - x, y) \\
\]</span></p>
<ul>
<li><strong>说明</strong>：图像翻转常用于数据增强，帮助模型学习不同的视角和方向。</li>
</ul>
<p><strong>旋转 (rotate)</strong></p>
<ul>
<li><strong>描述</strong>：根据指定的模式对图像进行旋转操作，支持 90
度顺时针、180 度和 90 度逆时针旋转。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
90度顺时针：\text{dst}(x, y) = \text{src}(H - y - 1, x) \\
180度：\text{dst}(x, y) = \text{src}(H - x - 1, W - y - 1) \\
90度逆时针：\text{dst}(x, y) = \text{src}(y, W - x - 1) \\
\]</span></p>
<ul>
<li><strong>说明</strong>：旋转操作在图像处理和数据增强中常用，用于产生不同角度的视图。</li>
</ul>
<p><strong>裁剪 (crop)</strong></p>
<ul>
<li><strong>描述</strong>：从输入图像中裁剪指定矩形区域。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
\text{dst}(x, y) = \text{src}(x + x_{offset}, y + y_{offset}) \\
其中 x_{offset} 和 y_{offset} 为裁剪区域的偏移量。
\]</span></p>
<ul>
<li><strong>说明</strong>：裁剪操作用于提取图像中的特定区域，以进行更细致的分析或处理。</li>
</ul>
<p><strong>调整大小 (resize)</strong></p>
<ul>
<li><strong>描述</strong>：将输入图像缩放到指定的大小，支持不同的插值方法，如双三次插值和区域插值。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
双线性插值(Bilinear):dst(x, y) = (y_2 - y) \cdot \left[ (x_2 - x) \cdot
src(x_1, y_1) + (x - x_1) \cdot src(x_2, y_1) \right] +\\ (y - y_1)
\cdot \left[ (x_2 - x) \cdot src(x_1, y_2) + (x - x_1) \cdot src(x_2,
y_2) \right]
\\
双三次差值(Cubic):\text{dst}(x, y) = \sum_{m=-1}^{2} \sum_{n=-1}^{2}
w(m) w(n) \text{src}(x+m, y+n) \\
区域插值(Area):\text{dst}(x, y) = \frac{1}{\text{area}}
\sum_{(x{\prime}, y{\prime}) \in \text{region}} \text{src}(x{\prime},
y{\prime})
\]</span></p>
<ul>
<li><strong>说明</strong>：调整大小操作在图像预处理阶段非常重要，用于将图像缩放到模型要求的输入尺寸。</li>
</ul>
<p><strong>裁剪+调整大小 (crop_resize)</strong></p>
<ul>
<li><p><strong>描述</strong>：从一张大图中扣出一张或多张子图，并缩放到指定尺寸。</p></li>
<li><p><strong>计算公式</strong>为crop和resize算子依次执行，此处不做赘述。</p></li>
<li><p><strong>说明</strong>：</p>
<ul>
<li><p>若crop的宽高与resize之后的宽高一致，则不进行缩放；resize宽高必须与输出宽高一致。</p></li>
<li><p>输入图片分辨率在[10×6, 4096×4096]范围内，支持<a
target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha001/apiref/appdevgapi/aclcppdevg_03_0637.html#ZH-CN_TOPIC_0000002046543820__section9180153242717">图片格式、宽高对齐、内存约束</a>处说明的输入图片格式。</p></li>
</ul></li>
</ul>
<p><strong>原地边框填充 (copyMakeBorder)</strong></p>
<ul>
<li><p><strong>描述</strong>：该函数在使用指定的外推边界模式时，计算并返回与指定外推像素相对应的供体像素的坐标。</p></li>
<li><p><strong>计算公式</strong>： <span class="math display">\[
\text{常数填充}: src&#39;(x, y) = \text{value}, \quad \text{如果} \ (x,
y) \text{是边界元素}\\
\text{复制边缘}: src&#39;(x, y) = src(x&#39;, y&#39;), \quad \text{其中}
\ (x&#39;, y&#39;) \text{为最近的图像内的像素坐标}
\]</span></p></li>
<li><p><strong>说明</strong>：</p>
<ul>
<li><code>scalar_value</code>仅在填充类型为<code>HI_BORDER_CONSTANT</code>的时候有效，指定填充的像素值。<br />
</li>
<li>输入图片分辨率在[10×6, 4096×4096]范围内，支持<a
target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha001/apiref/appdevgapi/aclcppdevg_03_0637.html#ZH-CN_TOPIC_0000002046543820__section9180153242717">图片格式、宽高对齐、内存约束</a>处说明的输入图片格式。</li>
</ul></li>
</ul>
<p><strong>裁剪+调整大小+边框填充 (cropResizeMakeBorder)</strong></p>
<ul>
<li><p><strong>描述</strong>：按指定区域从一张输入图片中抠出一个或多个子图，对子图缩放后，再将每个子图按指定类型填充，作为一张或多张目标图片输出，主要用于等比例缩放场景。</p></li>
<li><p><strong>计算公式</strong>为crop、resize和copyMakeBorder算子依次执行，此处不做赘述。</p></li>
<li><p><strong>说明</strong>：</p>
<ul>
<li>若crop的宽高与resize之后的宽高一致，则不进行缩放。</li>
<li><code>scalar_value</code>仅在填充类型为<code>HI_BORDER_CONSTANT</code>的时候有效，指定填充的像素值。</li>
<li>输入图片分辨率在[10×6, 4096×4096]范围内，支持<a
target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha001/apiref/appdevgapi/aclcppdevg_03_0637.html#ZH-CN_TOPIC_0000002046543820__section9180153242717">图片格式、宽高对齐、内存约束</a>处说明的输入图片格式。</li>
</ul></li>
</ul>
<p>接口分为外部接口和内部接口，外部接口是对内部接口的封装，避免代码重复和额外的数据类型转换。</p>
<h3 id="色域转换">5.3 色域转换</h3>
<p>本节是用于在华为Ascend硬件上加速OpenCV图像处理操作的。以下是对主要函数的解释：</p>
<p><strong>cvtBGRtoBGR</strong></p>
<ul>
<li><strong>描述</strong>: 将 BGR
图像转换为指定的通道数（DCN）并根据选项交换蓝色通道的位置。</li>
<li><strong>说明</strong> 将 BGR
图像的三个通道分离，并根据是否需要交换蓝色通道进行处理。然后将处理后的通道合并为指定的通道数（3
或 4）。</li>
</ul>
<p><strong>cvtBGRtoGray</strong></p>
<ul>
<li><strong>描述</strong>：将 BGR 图像转换为灰度图像。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
\text{Gray} = 0.299 \times \text{Red} + 0.587 \times \text{Green} +
0.114 \times \text{Blue}
\]</span></p>
<ul>
<li><strong>说明</strong>：将 BGR
图像转换为灰度图像，通过应用加权系数来计算每个像素的灰度值。</li>
</ul>
<p><strong>cvtGraytoBGR</strong></p>
<ul>
<li><strong>描述</strong>： 将灰度图像转换为 BGR 图像。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
\text{BGR} = \begin{bmatrix}
\text{Gray} \
\text{Gray} \
\text{Gray}
\end{bmatrix}
\]</span></p>
<ul>
<li><strong>说明</strong>：将灰度图像的单通道复制为三个通道（或四个通道），然后合并为
BGR（或 BGRA）图像。</li>
</ul>
<p><strong>cvtBGRtoXYZ</strong></p>
<ul>
<li><strong>描述</strong>：将 BGR 图像转换为 XYZ 颜色空间。</li>
<li><strong>计算公式</strong>：</li>
</ul>
$$
<span class="math display">\[\begin{bmatrix}
X \\
Y \\
Z
\end{bmatrix}\]</span>
<span class="math display">\[\begin{bmatrix}
0.412453 &amp; 0.357580 &amp; 0.180423 \\
0.212671 &amp; 0.715160 &amp; 0.072169 \\
0.019334 &amp; 0.119193 &amp; 0.950227
\end{bmatrix}
\begin{bmatrix}
\text{Blue} \\
\text{Green} \\
\text{Red}
\end{bmatrix}\]</span>
<p>$$</p>
<p><strong>说明</strong>：将 BGR 图像通过矩阵乘法转换为 XYZ
颜色空间。</p>
<p><strong>cvtXYZtoBGR</strong></p>
<ul>
<li><strong>描述</strong>：将 XYZ 颜色空间图像转换为 BGR 图像。</li>
<li><strong>计算公式</strong>：</li>
</ul>
$$
<span class="math display">\[\begin{bmatrix}
\text{Blue} \\
\text{Green} \\
\text{Red}
\end{bmatrix}\]</span>
<span class="math display">\[\begin{bmatrix}
3.240479 &amp; -1.53715 &amp; -0.498535 \\
-0.969256 &amp; 1.875991 &amp; 0.041556 \\
0.055648 &amp; -0.204043 &amp; 1.057311
\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
Z
\end{bmatrix}\]</span>
<p>$$</p>
<ul>
<li><strong>说明</strong>：将 XYZ 图像通过矩阵乘法转换为 BGR
图像，并根据需要添加 alpha 通道。</li>
</ul>
<p><strong>cvtBGRtoYCrCb</strong></p>
<ul>
<li><strong>描述</strong>：将 BGR 图像转换为 YCrCb 颜色空间。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
Y = 0.299 \times \text{Red} + 0.587 \times \text{Green} + 0.114 \times
\text{Blue} \\
\text{Cr} = 0.713 \times (\text{Red} - Y) + 128 \\
\text{Cb} = 0.564 \times (\text{Blue} - Y) + 128
\]</span></p>
<ul>
<li><strong>说明</strong>：将 BGR 图像转换为 YCrCb
颜色空间，并根据需要调整通道顺序。</li>
</ul>
<p><strong>cvtYCrCbtoBGR</strong></p>
<ul>
<li><strong>描述</strong>：将 YCrCb 颜色空间图像转换为 BGR 图像。</li>
<li><strong>计算公式</strong>：</li>
</ul>
<p><span class="math display">\[
\text{Red} = Y + 1.402 \times (\text{Cr} - 128) \\
\text{Blue} = Y + 1.772 \times (\text{Cb} - 128) \\
\text{Green} = Y - 0.344136 \times (\text{Cr} - 128) - 0.714136 \times
(\text{Cb} - 128)
\]</span></p>
<ul>
<li><strong>说明</strong>：将 YCrCb 图像转换为 BGR
图像，通过矩阵计算恢复到 BGR 颜色空间，并根据需要添加 alpha 通道。</li>
</ul>
<p>RGB月YUV相互转换的计算方法与YCrCb计算方法类似，以及RGB，BGR，
RGBA，BGRA与其他色域转换实现类似，使用不用参数的方式复用上述色域转换代码。</p>
<h2 id="python绑定">6. Python绑定</h2>
<p>OpenCV的python绑定通过一个Python脚本对C++的接口函数解析实现，重载的函数会在python绑定中生成多个绑定，根据参数类型试探的方式选择正确的重载函数。</p>
<p>OpenCV对输入和输出有一个通用结构，分别是InputArray和OutputArray，这两个结构会在python绑定中自动与Mat（UMat，GpuMat）进行转换，在python调用中，这些算法接口可以传入numpy结构或者任意一种Mat结构。由于后端加速器包在OpenCV-Contrib中，为了避免修改OpenCV主仓库，所以InputArray和OutputArray无法识别到AscendMat，也就无法做自动转换。</p>
<p>为了提供一致的使用体验，在提供CANN模块接口时，需要提供InputArray/OutputArray的接口，也需要提供AscendMat的接口，在CANN模块内部做InputArray/OutputArray与AscendMat的类型转换。</p>
<p>该接口既提供C++接口，也提供python绑定接口。该接口是使用CANN模块的入口，根据OpenCV的项目要求，需要提供详细的Doxygen描述，以生成标准的doc手册。</p>
<h2 id="ascendc支持">7. AscendC支持</h2>
<p>AscendC是自定义算子的编程语言，为了提高算子的执行效率，最好的方式使用AscendC来编写合适的融合算子。在OpenCV昇腾支持中，AscendC支持是一个实验性质的特性。</p>
<p>整体AscendC应该当做昇腾支持的一个子模块，于OpenCV一同链接到二进制中。需要实现AscendC编译链接框架，并且实现一个简单的算子（Threshold)，当做自定义算子的样例。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opencv</span><br><span class="line">   └── opencv_cann.so</span><br><span class="line">          └──  ascendc_kernels.so</span><br></pre></td></tr></table></figure>
<h2 id="dvpp支持">8. DVPP支持</h2>
<p>DVPP是昇腾AI处理器内置的图像处理单元，专门用于图像和视频的处理和加速，可以通过AscendCL的媒体数据处理接口进行调用，提供了强大的媒体处理硬加速能力。在OpenCV昇腾支持中，DVPP支持主要提供高性能算子特性。</p>
<p>DVPP支持应作为昇腾支持的子模块，由于其数据对齐及读取格式等约束不同于执行于AI
core和AI
cpu的Aclop算子，需为其单独设计运行时管理、内存管理及数据处理流程的类及方法，初步设计如下：</p>
<figure>
<img
src="https://raw.githubusercontent.com/MengqingCao/BlogImage/main/imgs/20241113103844.png"
alt="DVPP类图" />
<figcaption aria-hidden="true">DVPP类图</figcaption>
</figure>
<p>其中，DvppOperatorDesc管理DVPP初始化、重置，及创建通道、输入、输出、获取结果等算子运行流程管理，WrapperFunctions完成具体的参数配置、算子调用等。</p>
<h2 id="测试">9. 测试</h2>
<h3 id="功能测试">9.1 功能测试</h3>
<p>功能测试使用OpenCV的功能测试框架，验证所有的接口以及不同入参组合，将Ascend执行结果与CPU结果做比对，要求误差在允许范围内。</p>
<h4 id="ascendmat验证">9.1.1 AscendMat验证</h4>
<p><strong>AscendMat 的构造函数测试</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证 AscendMat 的默认构造函数和自定义构造函数的正确性。</li>
<li>验证 AscendMat 构造时传递的大小、类型、值是否正确分配。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>使用默认构造函数创建 AscendMat 对象，并检查默认分配器是否正确。</li>
<li>设置和获取自定义分配器，确保分配器设置和获取功能正常。</li>
<li>创建指定大小和类型的 AscendMat
对象，并检查行数、列数、深度和通道数是否与预期一致。</li>
<li>使用特定值填充 AscendMat 对象，并检查填充值是否正确。</li>
<li>从主机内存创建 AscendMat 对象，并检查数据的正确性。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>默认构造函数应使用默认分配器。</li>
<li>自定义分配器应成功设置和获取。</li>
<li>创建的 AscendMat 对象的大小、类型和填充值应与预期匹配。</li>
<li>从主机内存构造的 AscendMat 数据应与输入数据一致。</li>
</ul>
<p><strong>AscendMat 的赋值操作测试</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证 AscendMat 对象间的赋值操作是否能正确复制数据和元数据。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建两个 AscendMat 对象，一个使用自定义分配器分配内存。</li>
<li>将一个 AscendMat
对象赋值给另一个，并检查行数、列数、深度、通道数及数据指针是否一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>赋值操作应正确复制行数、列数、深度、通道数及数据指针。</li>
</ul>
<p>**</p>
<p><strong>AscendMat 的 setTo 方法测试</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证 AscendMat 的 setTo
方法能否正确将矩阵的所有元素设置为指定值。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个 AscendMat 对象，并使用 setTo
方法将其所有元素设置为随机生成的标量值。</li>
<li>下载数据到主机并与手动创建的矩阵数据进行比较。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>AscendMat 对象应成功设置所有元素，并且与主机上的期望数据一致。</li>
</ul>
<p><strong>AscendMat 的 convertTo 方法测试</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证 AscendMat 的 convertTo 方法能否正确转换矩阵数据类型。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个 AscendMat 对象，并使用随机生成的标量值初始化。</li>
<li>使用 convertTo 方法将矩阵数据类型转换为另一个类型。</li>
<li>下载数据到主机并与手动创建的转换后矩阵进行比较。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>AscendMat
对象应成功转换数据类型，且转换后的数据与主机上的期望数据一致。</li>
</ul>
<p><strong>合并测试（MERGE）</strong> <strong>测试目的</strong>:</p>
<ul>
<li>验证merge函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建三个单通道的矩阵m1、m2和m3。</li>
<li>使用merge函数在CPU上合并这三个矩阵为一个多通道矩阵。</li>
<li>在Ascend后端上分别通过数组和向量的方式合并矩阵，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的合并结果应与CPU的结果一致。</li>
</ul>
<h4 id="算术算法验证">9.1.2 算术算法验证</h4>
<p>算术算法需要验证一下几种类型：</p>
<p><strong>基础矩阵运算操作</strong>:</p>
<ul>
<li>测试包括矩阵加法、减法、乘法、除法、按位与、按位或、按位异或等操作。</li>
<li>验证以上操作在 Ascend 设备与 CPU 上的结果一致性。</li>
</ul>
<p><strong>带掩码的矩阵运算</strong>:</p>
<ul>
<li>验证矩阵运算操作在引入掩码后的正确性，包括掩码的生成与使用。</li>
</ul>
<p><strong>带缩放参数的矩阵运算</strong>:</p>
<ul>
<li>验证矩阵乘法和除法操作在引入缩放参数后的正确性。</li>
</ul>
<p><strong>流管理</strong>:</p>
<ul>
<li>测试 Ascend Stream
的创建、运算调度与结果同步功能，确保异步处理与同步处理的结果一致。</li>
</ul>
<p>测试方法如下：</p>
<p><strong>测试目的</strong>:</p>
<ul>
<li>验证加法、减法、乘法、除法、按位与、按位或、按位异或，加权和，阈值计算等操作方法计算结果是否正确。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个矩阵对象，并使用随机数据进行填充。</li>
<li>使用需要验证的算法进行计算，分别使用默认流，以及显式创建的流进行计算。</li>
<li>比较CPU和Ascend后端的结果，确保他们的精度一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的合并结果应与CPU的结果一致。</li>
</ul>
<h4 id="图像核心算法验证">9.1.3 图像核心算法验证</h4>
<p><strong>拆分测试（SPLIT）</strong> <strong>测试目的</strong>:</p>
<ul>
<li>验证split函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个多通道的矩阵m，并使用split函数在CPU上将其拆分为三个单通道矩阵。</li>
<li>使用Ascend后端进行同样的拆分操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的拆分结果应与CPU的结果一致。</li>
</ul>
<p><strong>转置测试（TRANSPOSE）</strong> <strong>测试目的</strong>:</p>
<ul>
<li>验证transpose函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个随机矩阵cpuMat。</li>
<li>使用transpose函数在CPU上转置矩阵，并存储结果。</li>
<li>在Ascend后端上进行相同的转置操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的转置结果应与CPU的结果一致。</li>
</ul>
<p><strong>翻转测试（FLIP）</strong> <strong>测试目的</strong>:</p>
<ul>
<li>验证flip函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个随机矩阵cpuMat。</li>
<li>使用flip函数在CPU上分别以不同的翻转模式（水平、垂直、同时翻转）进行翻转，并存储结果。</li>
<li>在Ascend后端上执行相同的翻转操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的翻转结果应与CPU的结果一致。</li>
</ul>
<p><strong>旋转测试（ROTATE）</strong> <strong>测试目的</strong>:</p>
<ul>
<li>验证rotate函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个随机矩阵cpuMat。</li>
<li>使用rotate函数在CPU上以不同的旋转模式进行旋转，并存储结果。</li>
<li>在Ascend后端上进行相同的旋转操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的旋转结果应与CPU的结果一致。</li>
</ul>
<p><strong>裁剪测试（CROP）</strong> <strong>测试目的</strong>:</p>
<ul>
<li>验证裁剪操作在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个矩阵cpuMat，并定义一个裁剪区域Rect b。</li>
<li>使用Mat对象在CPU上执行裁剪操作，并存储结果。</li>
<li>在Ascend后端上执行相同的裁剪操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的裁剪结果应与CPU的结果一致。</li>
</ul>
<p><strong>调整大小测试（RESIZE）</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证resize函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个随机矩阵cpuMat。</li>
<li>使用resize函数在CPU上对矩阵进行调整大小操作，使用不同的插值方法，并存储结果。</li>
<li>在Ascend后端上进行相同的调整大小操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的调整大小结果应与CPU的结果一致。</li>
</ul>
<p><strong>裁剪及调整大小测试（CROP_RESIZE）</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证cropResize函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个随机矩阵cpuMat，并定义裁剪区域Rect b，目的矩阵大小Size
dsize。</li>
<li>依次使用crop和resize函数在CPU上对矩阵进行裁剪和调整大小操作，使用不同的插值方法，并存储结果。</li>
<li>在Ascend后端上调用cropResize函数执行相同操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的调整大小结果应与CPU的结果一致。</li>
</ul>
<p><strong>裁剪及调整大小测试（COPY_MAKE_BORDER）</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证copyMakeborder函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个随机矩阵cpuMat，并定义top、bottom、left、right四个方向的border宽度。</li>
<li>使用copyMakeBorder函数在CPU上对矩阵进行填充边框操作，使用不同的边框填充插值方法，并存储结果。</li>
<li>在Ascend后端上调用copyMakeborder函数执行相同操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的调整大小结果应与CPU的结果一致。</li>
</ul>
<p><strong>裁剪及调整大小测试（CROP_RESIZE_MAKE_BORDER）</strong>
<strong>测试目的</strong>:</p>
<ul>
<li>验证cropResizeMakeborder函数在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>创建一个随机矩阵cpuMat，并定义裁剪区域Rect b，目的矩阵大小Size
dsize，top、bottom、left、right四个方向的border宽度。</li>
<li>依次使用crop、resize和copyMakeBorder函数在CPU上对矩阵进行裁剪、调整大小和填充边框操作，使用不同的边框填充插值方法，并存储结果。</li>
<li>在Ascend后端上调用cropResizeMakeborder函数执行相同操作，并将结果下载到主机端。</li>
<li>比较CPU和Ascend后端的结果，确保它们一致。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的调整大小结果应与CPU的结果一致。</li>
</ul>
<h4 id="色域转换算法验证">9.1.4 色域转换算法验证</h4>
<p>色域转换算法验证需要覆盖以下转换操作：</p>
<ul>
<li>BGR到BGRA</li>
<li>BGRA到BGR</li>
<li>BGR到RGBA</li>
<li>RGBA到BGR</li>
<li>BGR到RGB</li>
<li>BGRA到RGBA</li>
<li>BGR到灰度图</li>
<li>RGB到灰度图</li>
<li>灰度图到BGR</li>
<li>灰度图到BGRA</li>
<li>BGRA到灰度图</li>
<li>RGBA到灰度图</li>
<li>RGB到XYZ</li>
<li>BGR到XYZ</li>
<li>XYZ到BGR</li>
<li>XYZ到RGB</li>
<li>BGR到YCrCb</li>
<li>RGB到YCrCb</li>
<li>YCrCb到BGR</li>
<li>YCrCb到RGB</li>
<li>BGR到YUV</li>
<li>RGB到YUV</li>
<li>YUV到BGR</li>
<li>YUV到RGB</li>
</ul>
<p><strong>测试目的</strong>:</p>
<ul>
<li>验证色域转换操作在Ascend后端的实现是否与CPU一致。</li>
</ul>
<p><strong>测试步骤</strong>:</p>
<ol type="1">
<li>生成随机图像矩阵。</li>
<li>执行颜色空间转换。</li>
<li>比较CPU和NPU计算结果，确认在允许误差范围内。</li>
</ol>
<p><strong>预期结果</strong>:</p>
<ul>
<li>Ascend后端的计算结果应与CPU的结果一致。</li>
</ul>
<h4 id="其他说明">9.1.5 其他说明</h4>
<p>所有功能测试需要同步测试C++接口以及Python接口，保证二者可用性及准确性。</p>
<h3 id="性能测试">9.2 性能测试</h3>
<p>性能测试使用OpenCV性能测试框架，使用不同图像大小，不同图像数据类型，将数据在Ascend
NPU上执行多次，计算每次运行的平均时间。</p>
<p><strong>初始化</strong>:</p>
<ol type="1">
<li>生成给定尺寸的测试矩阵。</li>
<li>构造随机输入矩阵，并完成算子预热。</li>
</ol>
<p><strong>Ascend NPU 测试</strong>:</p>
<ol type="1">
<li>设置 Ascend 设备 (cv::cann::setDevice)，并上传矩阵至
AscendMat。</li>
<li>在 TEST_CYCLE() 内执行操作（如 merge、split 等）。</li>
<li>复位 Ascend 设备 (cv::cann::resetDevice)。</li>
</ol>
<p><strong>CPU 测试</strong>:</p>
<ul>
<li>使用 OpenCV 自带的方法执行相同操作。</li>
</ul>
<p><strong>验证</strong>:</p>
<ul>
<li>每个测试用例执行完后，通过 SANITY_CHECK_NOTHING() 检查无异常。</li>
</ul>
<p><strong>性能验证结果如下</strong>：</p>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 26%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 13%" />
<col style="width: 7%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr>
<th>算子/平均运算耗时*(ms)</th>
<th>CPU</th>
<th>GPU</th>
<th>NPU</th>
<th>相对CPU性能提升</th>
<th></th>
<th>相对GPU性能提升</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>16 Intel(R) Xeon(R) Gold 6151</td>
<td>Nvidia V100</td>
<td>Ascend 310P</td>
<td>GPU</td>
<td>NPU</td>
<td>NPU</td>
</tr>
<tr>
<td>merge</td>
<td>83.50</td>
<td>48.00</td>
<td>30.50</td>
<td>42.51%</td>
<td>63.47%</td>
<td>36.46%</td>
</tr>
<tr>
<td>split</td>
<td>142.00</td>
<td>59.00</td>
<td>81.50</td>
<td>58.45%</td>
<td>42.61%</td>
<td>-38.14%</td>
</tr>
<tr>
<td>flip</td>
<td>221.25</td>
<td>77.75</td>
<td>1069.00</td>
<td>64.86%</td>
<td>-383.16%</td>
<td>-1274.92%</td>
</tr>
<tr>
<td>crop</td>
<td>49.25</td>
<td>61.50</td>
<td>75.75</td>
<td>-24.87%</td>
<td>-53.81%</td>
<td>-23.17%</td>
</tr>
<tr>
<td>transpose</td>
<td>446.00</td>
<td>84.50</td>
<td>277.50</td>
<td>81.05%</td>
<td>37.78%</td>
<td>-228.40%</td>
</tr>
<tr>
<td>resize</td>
<td>136.50</td>
<td>188.75</td>
<td>229.75</td>
<td>-38.28%</td>
<td>-68.32%</td>
<td>-21.72%</td>
</tr>
<tr>
<td>threshold</td>
<td>172.75</td>
<td>174.00</td>
<td>251.50</td>
<td>-0.72%</td>
<td>-45.59%</td>
<td>-44.54%</td>
</tr>
<tr>
<td>rotate</td>
<td>584.50</td>
<td>76.50</td>
<td>1067.50</td>
<td>86.91%</td>
<td>-82.63%</td>
<td>-1295.42%</td>
</tr>
<tr>
<td>add</td>
<td>529.63</td>
<td>324.50</td>
<td>268.75</td>
<td>38.73%</td>
<td>49.26%</td>
<td>17.18%</td>
</tr>
<tr>
<td>addWeighted</td>
<td>553.88</td>
<td>326.38</td>
<td>422.88</td>
<td>41.07%</td>
<td>23.65%</td>
<td>-29.57%</td>
</tr>
<tr>
<td>subtract</td>
<td>528.25</td>
<td>355.63</td>
<td>269.75</td>
<td>32.68%</td>
<td>48.94%</td>
<td>24.15%</td>
</tr>
<tr>
<td>multiply</td>
<td>534.63</td>
<td>354.25</td>
<td>265.88</td>
<td>33.74%</td>
<td>50.27%</td>
<td>24.95%</td>
</tr>
<tr>
<td>divide</td>
<td>542.50</td>
<td>355.63</td>
<td>266.75</td>
<td>34.45%</td>
<td>50.83%</td>
<td>24.99%</td>
</tr>
<tr>
<td>bitwise_add</td>
<td>529.25</td>
<td>355.13</td>
<td>266.00</td>
<td>32.90%</td>
<td>49.74%</td>
<td>25.10%</td>
</tr>
<tr>
<td>bitwise_or</td>
<td>529.50</td>
<td>354.63</td>
<td>266.00</td>
<td>33.03%</td>
<td>49.76%</td>
<td>24.99%</td>
</tr>
<tr>
<td>bitwise_xor</td>
<td>529.88</td>
<td>354.50</td>
<td>268.00</td>
<td>33.10%</td>
<td>49.42%</td>
<td>24.40%</td>
</tr>
<tr>
<td>bitwise_not</td>
<td>324.38</td>
<td>177.38</td>
<td>448.38</td>
<td>45.32%</td>
<td>-38.23%</td>
<td>-152.78%</td>
</tr>
<tr>
<td>cvtColor</td>
<td>57.31</td>
<td>66.96</td>
<td>118.76</td>
<td>-16.83%</td>
<td>-107.21%</td>
<td>-77.35%</td>
</tr>
</tbody>
</table>
<h3 id="ci">9.3. CI</h3>
<p>OpenCV社区没有昇腾测试设备，后续昇腾相关特性提交需要经过昇腾设备的CI验证。</p>
<ol type="1">
<li>向社区贡献昇腾机器用于CI验证。</li>
<li>使用PR label的方式，仅针对昇腾相关特性运行CI。</li>
<li>使用容器的方式运行CI，将Dockerfile合入OpenCV的基础设施仓库，并归档容器镜像。</li>
<li>配置github workflow，配置昇腾CI相关逻辑。</li>
</ol>
<h2 id="教程">10. 教程</h2>
<h3 id="样例">10.1 样例</h3>
<p>为了方便用户使用该模块，需要提供调用样例，选择一个常见的图像预处理逻辑，实现C++版本以及Python版本的样例代码。</p>
<h3 id="使用文档">10.2 使用文档</h3>
<p>按照社区的文档规范，编写模块使用指南，该使用指南将会构建到OpenCV
doc中，作为教程供用户参考。</p>
<table>
<colgroup>
<col style="width: 88%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr>
<th>PR</th>
<th>代码量</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv_contrib"></a><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv_contrib/pull/3552">Add operators
support for Ascend NPU (CANN backend)</a></td>
<td>+5880 -0</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pull/24488">Link
lib_acl_op_compiler when compile with CANN</a></td>
<td>+12 -0</td>
</tr>
<tr>
<td><a
target="_blank" rel="noopener" href="https://github.com/opencv-infrastructure/opencv-gha-dockerfile/pull/24">Base
OpenEuler 22.03.SP2 docker image for CI.</a></td>
<td>+71 -0</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/opencv/ci-gha-workflow/pull/120">Added
CI pipeline with openEuler22.03.SP2 and Ascend310</a></td>
<td>+120 -0</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv_contrib/pull/3614">Enable
AscendC kernel operator</a></td>
<td>+697 -92</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/opencv/ci-gha-workflow/pull/132">Update
openEuler image tag</a></td>
<td>+1 -1</td>
</tr>
</tbody>
</table>
<p><a
target="_blank" rel="noopener" href="https://ascend.github.io/docs/sources/opencv/install.html">OpenCV昇腾开源使用手册</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/12/llama-cpp%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/11/12/llama-cpp%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/" class="post-title-link" itemprop="url">llama.cpp昇腾原生支持</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-12 15:33:29" itemprop="dateCreated datePublished" datetime="2024-11-12T15:33:29+08:00">2024-11-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:47:15" itemprop="dateModified" datetime="2025-01-23T15:47:15+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="项目背景">1. 项目背景</h2>
<p>llama.cpp
是一个开源项目，旨在将大模型高效地部署在低资源环境中，例如个人电脑或移动设备。这个项目由
Georgi Gerganov 创建，目标是通过优化和精简，使得 LLaMA 模型能够在不依赖
GPU 的情况下高效运行。llama.cpp 支持多平台和多后端，且兼容大部分
Transformer 模型和部分 CLIP
模型，便于在各种环境中部署。其模块化设计包括模型分片、KV
缓存、推理引擎和输出处理，适合边缘计算、隐私保护和低成本推理场景，帮助用户在普通设备上实现大模型推理。</p>
<h3 id="目标">1.1 目标</h3>
<p>开发基于昇腾的llama.cpp后端，实现昇腾runtime和核心算子。后端使用CANN和昇腾算子库的能力来加速大模型的推理。使得常见的模型能够在llama.cpp中使用昇腾推理，加速推理效率。</p>
<h3 id="项目概述">1.2 项目概述</h3>
<p><strong>昇腾后端和Runtime接入</strong></p>
<p>在 llama.cpp 中，为Ascend加速器提供接口适配层，使 llama.cpp
的模型推理请求能通过接口层传递至 Ascend Runtime。</p>
<p>涉及：</p>
<ul>
<li>设备接入，支持多卡接入；</li>
<li>内存管理和Tensor管理；</li>
<li>Stream，Event管理；</li>
</ul>
<p><strong>昇腾算子</strong></p>
<p>为了支持大部分的模型推理，需要实现43个算子。这些算子可以利用aclnn的算子能力构建，如果aclnn的算子不足以满足llama.cpp的算子，</p>
<p>则：</p>
<ul>
<li>优先使用aclnn算子组合的方式实现功能；</li>
<li>使用AscendC编写自定义算子。</li>
</ul>
<p>对算子的需求，可用性大于性能，为了减少开发工作量并快速完成支持，不考虑acl
op算子。能使用aclnn组合的算子，优先使用算子组合实现。</p>
<p><strong>精度和性能</strong></p>
<ul>
<li><p>实现的算子需要通过llama.cpp的精度对比测试，以及内存越界检查，确保实现的算子实现正确。</p></li>
<li><p>910B算子性能需要超过Intel CPU水平（以Intel(R) Xeon(R) Gold 6348
CPU @ 2.60GHz为例）。</p></li>
<li><p>910B模型推理（llama3
8B）性能延迟不高于100ms，吞吐率不低于300token/s。</p></li>
</ul>
<p><strong>多芯片支持</strong></p>
<ul>
<li>首先支持910B系列芯片，包括主要的模型端到端推理，q4_0，q8_0量化格式；</li>
<li>然后支持310P（910A）系列芯片，除了q4_0外(310P不支持4bit量化)，其他功能应当与910B芯片能力持平；</li>
<li>最后尝试支持310B系列芯片，310B的支持程度以aclnn和AscendC库的支持情况而定。</li>
</ul>
<p><strong>文档和用户指南</strong></p>
<ul>
<li><p>用户指南，介绍文档结构和使用说明，帮助用户理解如何在 llama.cpp
中配置和使用 Ascend 后端；</p></li>
<li><p>安装配置步骤，详细说明 Ascend
后端的安装流程，包括环境依赖、编译步骤及配置方法，以确保用户可以顺利完成安装；</p></li>
<li><p>常见问题和解决方法，总结用户在使用 Ascend
后端时可能遇到的问题，并提供解决方案，如内存溢出、兼容性问题和性能调优建议等。</p></li>
</ul>
<h2 id="设计思路">2. 设计思路</h2>
<h3 id="llama.cpp项目架构">2.1 llama.cpp项目架构</h3>
<figure>
<img
src="https://cdn-0.plantuml.com/plantuml/png/TPJFRjim3CRlVWgYzxn03qE3D1s29LZMRBiCnS1PT2fKfZo9pluKU_T9egkq1f9B1FBJxoCfmZTHCCZOkoPGAyX7Ht2rtU9k2Jjlo5q1HkZp2PuRIBzlMuz6SmyQkFFX5mO3Uunn2dqQaSN-HR6Ufr2v0OV1MH7BnuVcN_FQyiDNM67xI9ayEYgsJwVlRObDpYhOi53eoLWKWdkAevCNstioOqkG_zWW2wnyFuoYPSmCDznH84xoDHyjgwETWjNoa1npFMz8chgbaqt27J8UgIUMkSF7KT8Ls0VVKeofvsBXDVfSPzUZW4edi19pQuFdI77ETGvxN4GA9me5gSSNv7A_IIsPmLj-Ium9-NEaA4hKHrqitftdV0rVajyvK-UHKni--QUKhUgv83LwiYRPtA9WKpD5frqdqNjY2YY9CrKzfno8JOJwEhNcHX55FrszJaaP0yVph9g6lH04UtmKy9rkRXb2hKwN6zcKyEU073iVUWgwrB64DzowCQjdQziG6yWMCwCwGxtT3ycDukkjWNNNsTOIjtykGUeSbB8AFiR5tg7a-WehaNxOLCfbX4vfwfW631Hphw2BfitTMKOtxTn5aCwuwDU_XKA-abThsUPjXivhJgTPd-kCkqcd_5dv3m00"
alt="llama.cpp架构图" />
<figcaption aria-hidden="true">llama.cpp架构图</figcaption>
</figure>
<p>llama.cpp的核心功能主要涉及以上几个部分：</p>
<p><strong>模型管理</strong></p>
<p>llama.cpp不仅支持llama，而且支持多种大语言模型和一些clip模型。llama.cpp使用模型管理模块来搭建模型结构，包括算子，量化等并且加载gguf模型的信息和模型权重。由于llama.cpp支持模型拆分的功能，以便于支持多卡推理和GPU/CPU混合推理，所以模型结构会进行合适的拆分，并且管理子图之间的数据拷贝。</p>
<p><strong>kv-cache</strong></p>
<p>kv-cache有助于加速attention的计算速度，将历史的kv信息做缓存。kv-cache会直接当做算子融合到模型中，kv-cache模块本身负责cache的管理，包括cache写入，更新和替换。</p>
<p><strong>server和api接口</strong></p>
<p>llama.cpp提供了一个简单的服务端，提供api接口。server支持并发推理。</p>
<p><strong>推理引擎</strong></p>
<p>llama.cpp对推理引擎进行了抽象，以便于支持不同的后端。推理引擎负责管理设备的内存，流，事件，多卡以及GPU/CPU数据拷贝。并且计算由模型管理模块构建的模型图。</p>
<h3 id="昇腾后端接入方法">2.2 昇腾后端接入方法</h3>
<p>llama.cpp提供了一系列抽象接口来接入后端加速器：</p>
<ol type="1">
<li><strong><code>ggml_backend_cann_device_interface</code></strong>：用于描述设备接口的模块，定义了设备的基本功能。</li>
<li><strong><code>ggml_backend_cann_interface</code></strong>：用于管理后端通用接口的模块，包含常见的张量异步处理和图计算功能。</li>
<li><strong><code>ggml_backend_cann_buffer_type_host</code></strong>：负责分配主机缓冲区，确保与后端设备内存的接口兼容。</li>
<li><strong><code>ggml_cann_compute_forward</code></strong>：主计算模块，负责分派和执行各个算子操作。</li>
</ol>
<p>在 <code>ggml_cann_compute_forward</code> 中，所有的算子都作为 case
分支进行注册，表示算子名称对应具体操作，例如
<code>GGML_OP_ADD</code>、<code>GGML_OP_MUL</code> 等等。</p>
<p>昇腾接入需要实现llama.cpp的runtime接口，并且实现推理所必须的算子。</p>
<h2 id="实现原理">3. 实现原理</h2>
<h3 id="运行时">3.1 运行时</h3>
<p>runtime提供了多个抽象接口，第一阶段主要目标是基本功能支持，所以仅需要支持必要的接口。其中split
tensor功能和图推理功能暂时不实现。llama.cpp的后端接入主要是通过注册三组接口实现，分别是设备访问接口，资源管理接口，内存管理接口。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_device_i ggml_backend_cann_device_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name                = */</span> ggml_backend_cann_device_get_name,</span><br><span class="line">    <span class="comment">/* .get_description         = */</span> ggml_backend_cann_device_get_description,</span><br><span class="line">    <span class="comment">/* .get_memory              = */</span> ggml_backend_cann_device_get_memory,</span><br><span class="line">    <span class="comment">/* .get_type                = */</span> ggml_backend_cann_device_get_type,</span><br><span class="line">    <span class="comment">/* .get_props               = */</span> ggml_backend_cann_device_get_props,</span><br><span class="line">    <span class="comment">/* .init_backend            = */</span> ggml_backend_cann_device_init,</span><br><span class="line">    <span class="comment">/* .get_buffer_type         = */</span> ggml_backend_cann_device_get_buffer_type,</span><br><span class="line">    <span class="comment">/* .get_host_buffer_type    = */</span> ggml_backend_cann_device_get_host_buffer_type,</span><br><span class="line">    <span class="comment">/* .buffer_from_host_ptr    = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .supports_op             = */</span> ggml_backend_cann_supports_op,</span><br><span class="line">    <span class="comment">/* .supports_buft           = */</span> ggml_backend_cann_supports_buft,</span><br><span class="line">    <span class="comment">/* .offload_op              = */</span> ggml_backend_cann_offload_op,</span><br><span class="line">    <span class="comment">/* .event_new               = */</span> ggml_backend_cann_device_event_new,</span><br><span class="line">    <span class="comment">/* .event_free              = */</span> ggml_backend_cann_device_event_free,</span><br><span class="line">    <span class="comment">/* .event_synchronize       = */</span> ggml_backend_cann_device_event_synchronize,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>此接口 <code>ggml_backend_cann_device_interface</code> 为 CANN 后端在
llama.cpp 中提供了一个通用的设备访问与操作抽象层，便于整合并统一管理
CANN 设备资源。通过实现接口中的各个函数，用户可以控制 CANN
设备的初始化、资源分配、操作支持检测等关键功能，从而确保 llama.cpp
中的模型计算能够顺利利用 CANN 的加速能力。</p>
<p>以下是接口中各函数的功能描述：</p>
<ul>
<li><p><strong>ggml_backend_cann_device_get_name</strong>：返回设备的名称，用于识别不同的设备类型。例如可以返回
"CANN 设备" 或者具体的设备型号。</p></li>
<li><p><strong>ggml_backend_cann_device_get_description</strong>：返回设备的详细描述信息，通常包含设备的硬件特性以及版本信息等，帮助用户理解设备特性。</p></li>
<li><p><strong>ggml_backend_cann_device_get_memory</strong>：获取设备的内存信息，包括总内存大小和当前可用内存，以便
llama.cpp 优化内存分配策略。</p></li>
<li><p><strong>ggml_backend_cann_device_get_type</strong>：返回设备类型，用于区分不同种类的设备（如
CPU、GPU、NPU 等），便于进行不同类型设备的适配。</p></li>
<li><p><strong>ggml_backend_cann_device_get_props</strong>：获取设备的属性信息，包括计算能力、内存带宽等。这些属性信息可用于优化计算分配和选择适合的算子。</p></li>
<li><p><strong>ggml_backend_cann_device_init</strong>：初始化后端设备，确保设备的资源和状态准备就绪。这一步通常在加载模型或开始计算之前调用。</p></li>
<li><p><strong>ggml_backend_cann_device_get_buffer_type</strong>：返回设备内存缓冲区的类型信息，帮助
llama.cpp 决定如何在设备端管理数据缓冲。</p></li>
<li><p><strong>ggml_backend_cann_device_get_host_buffer_type</strong>：返回主机端缓冲区类型，用于在主机和设备之间进行高效的数据交换。</p></li>
<li><p><strong>buffer_from_host_ptr</strong>：该接口可用于将主机端内存直接映射或转换为设备端缓冲区，若未来需求可扩展。</p></li>
<li><p><strong>ggml_backend_cann_supports_op</strong>： 检查 CANN
设备是否支持指定的操作（op），确保模型中的特定操作能够得到设备的加速支持。</p></li>
<li><p><strong>ggml_backend_cann_supports_buft</strong>：检查设备是否支持指定的缓冲区类型，确保数据在缓冲区类型上的一致性和兼容性。</p></li>
<li><p><strong>ggml_backend_cann_offload_op</strong>：将计算操作卸载到设备端执行，提升操作效率和加速模型推理过程。</p></li>
<li><p><strong>ggml_backend_cann_device_event_new</strong>：创建新的事件对象，用于异步操作的状态跟踪，如操作完成的通知。</p></li>
<li><p><strong>ggml_backend_cann_device_event_free</strong>：释放事件对象，清理事件资源，确保内存不被泄漏。</p></li>
<li><p><strong>ggml_backend_cann_device_event_synchronize</strong>：同步事件，确保指定的异步操作完成。这通常用于确保操作的执行顺序。</p></li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_i ggml_backend_cann_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name                = */</span> ggml_backend_cann_name,</span><br><span class="line">    <span class="comment">/* .free                    = */</span> ggml_backend_cann_free,</span><br><span class="line">    <span class="comment">/* .set_tensor_async        = */</span> ggml_backend_cann_set_tensor_async,</span><br><span class="line">    <span class="comment">/* .get_tensor_async        = */</span> ggml_backend_cann_get_tensor_async,</span><br><span class="line">    <span class="comment">/* .cpy_tensor_async        = */</span> ggml_backend_cann_cpy_tensor_async,</span><br><span class="line">    <span class="comment">/* .synchronize             = */</span> ggml_backend_cann_synchronize,</span><br><span class="line">    <span class="comment">/* .graph_plan_create       = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_free         = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_update       = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_compute      = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_compute           = */</span> ggml_backend_cann_graph_compute,</span><br><span class="line">    <span class="comment">/* .event_record            = */</span> ggml_backend_cann_event_record,</span><br><span class="line">    <span class="comment">/* .event_wait              = */</span> ggml_backend_cann_event_wait,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>ggml_backend_cann_interface</code> 接口提供了 CANN 后端在
llama.cpp 中的资源管理、异步数据传输、计算图执行等功能接口，实现了与
CANN 后端的深度集成。通过该接口，llama.cpp
可以高效地管理张量的异步操作、事件记录、同步及图计算，确保计算任务能够顺畅运行在
CANN 设备上。</p>
<p>以下是接口中各函数的功能描述：</p>
<ul>
<li><p><strong>ggml_backend_cann_name</strong>：
返回后端名称，通常用于标识该后端为 CANN 后端。</p></li>
<li><p><strong>ggml_backend_cann_free</strong>：释放后端资源，确保内存和其他资源在后端不再使用时被正确回收。</p></li>
<li><p><strong>ggml_backend_cann_set_tensor_async</strong>：异步设置张量数据到设备端，为后续计算提供数据准备。异步设置可提高数据传输的效率。</p></li>
<li><p><strong>ggml_backend_cann_get_tensor_async</strong>
：异步获取张量数据，方便在计算完成后从设备端提取数据，避免阻塞主线程。</p></li>
<li><p><strong>ggml_backend_cann_cpy_tensor_async</strong>：异步复制张量数据，支持设备端和主机端之间的数据交互或设备内部的数据拷贝，以便于多任务并行处理。</p></li>
<li><p><strong>ggml_backend_cann_synchronize</strong>
：同步操作，确保所有异步任务完成，通常用于确保张量操作和事件顺序执行。</p></li>
<li><p><strong>graph_plan_create</strong>：该接口目前未实现。将来可用于创建计算图执行计划，优化计算图的操作顺序和资源分配。</p></li>
<li><p><strong>graph_plan_free</strong>：该接口目前未实现。可以释放计算图计划的资源，确保内存使用的高效管理。</p></li>
<li><p><strong>graph_plan_update</strong>：该接口目前未实现。可用于在图执行过程中动态更新计算计划，以适应运行时的资源情况。</p></li>
<li><p><strong>graph_plan_compute</strong>：该接口目前未实现。未来可能用于执行图计划中的所有操作，便于更复杂的任务调度。</p></li>
<li><p><strong>ggml_backend_cann_graph_compute</strong>：执行计算图中的所有节点操作，是核心计算接口之一。该函数负责协调图中的计算任务，使之并行或顺序执行。</p></li>
<li><p><strong>ggml_backend_cann_event_record</strong>：记录事件，用于标记特定操作的时间点，便于在异步计算中追踪进度和执行状态。</p></li>
<li><p><strong>ggml_backend_cann_event_wait</strong>：等待特定事件完成，通常用于确保在后续操作开始前当前任务已完成，以保持计算图的执行正确性。</p></li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_buffer_type_i ggml_backend_cann_buffer_type_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name         = */</span> ggml_backend_cann_buffer_type_name,</span><br><span class="line">    <span class="comment">/* .alloc_buffer     = */</span> ggml_backend_cann_buffer_type_alloc_buffer,</span><br><span class="line">    <span class="comment">/* .get_alignment    = */</span> ggml_backend_cann_buffer_type_get_alignment,</span><br><span class="line">    <span class="comment">/* .get_max_size     = */</span> <span class="literal">NULL</span>,  <span class="comment">// defaults to SIZE_MAX</span></span><br><span class="line">    <span class="comment">/* .get_alloc_size   = */</span> ggml_backend_cann_buffer_type_get_alloc_size,</span><br><span class="line">    <span class="comment">/* .is_host          = */</span> ggml_backend_cann_buffer_type_is_host,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>ggml_backend_cann_buffer_type_interface</code> 结构体定义了
CANN
后端缓冲区类型的接口，它提供了一组操作缓冲区属性和行为的函数接口。这个接口使得
CANN 后端的缓冲区能够在 llama.cpp
中被正确管理和使用，确保内存分配、对齐、大小等操作的一致性和高效性。</p>
<p>以下是 <code>ggml_backend_cann_buffer_type_interface</code>
结构体中各字段的功能描述：</p>
<ul>
<li><strong>ggml_backend_cann_buffer_type_name</strong>：返回缓冲区类型的名称。该函数用于标识当前缓冲区类型，主要用于调试和日志记录。</li>
<li><strong>ggml_backend_cann_buffer_type_alloc_buffer</strong>：用于分配缓冲区的内存。通过该函数，llama.cpp
可以请求 CANN 后端分配指定大小的内存块，用于存储数据和张量。</li>
<li><strong>ggml_backend_cann_buffer_type_get_alignment</strong>：返回缓冲区的对齐方式。内存对齐对于性能至关重要，因为不适当的对齐可能导致
CPU 或 GPU
在访问数据时的效率降低。该函数可以确保数据在内存中的对齐符合硬件的要求。</li>
<li><strong>get_max_size</strong> ：该字段指示缓冲区的最大尺寸，若设置为
<code>NULL</code>，则默认最大值为
<code>SIZE_MAX</code>，即没有固定的尺寸限制。此函数适用于不希望为缓冲区大小设定上限的场景。</li>
<li><strong>ggml_backend_cann_buffer_type_get_alloc_size</strong>
：获取缓冲区实际分配的内存大小。该函数确保返回正确的分配大小，便于用户跟踪内存使用情况。</li>
<li><strong>ggml_backend_cann_buffer_type_is_host</strong>：判断缓冲区是否为主机缓冲区。该函数用于区分主机内存和设备内存，以便进行适当的内存管理和数据传输。</li>
</ul>
<p><strong>Host buffer</strong></p>
<p>Host buffer是一种特殊的buffer
type，用于在CPU上申请内存，用于一些中间数据的临时存储，为后端设备提供了以快速访问的内存区域。</p>
<p>Pin
memory，又称“锁页内存”或“固定内存”，是指将主机内存中的一部分内存固定在物理内存上，以便快速传输至计算设备（如GPU或NPU）。通常情况下，操作系统会将不经常使用的内存页移至虚拟内存中，这可能导致数据传输时出现额外的内存访问延迟。而使用Pin
memory则可以避免这种情况，因为锁页内存不会被系统交换出物理内存，从而大大加速数据传输过程。</p>
<p>Host buffer使用Pin
memory实现，用户加速Host和Device之间的内存拷贝速度。Host
buffer与buffer_type的结构相同，以接口注册的方式提供Host
buffer的能力。</p>
<p><strong>Split Tensor</strong></p>
<p>Split
Tensor用于在做复杂计算的时候充分利用多卡能力，llama.cpp中，对矩阵乘法，使用到了Split
Tensor，计算时会相乘的矩阵其中一个进行拆分，使用多卡进行并行计算，计算完成后做结果的合并。</p>
<p>Split
Tensor实现复杂，并且无法利用已有的aclnn算子，在本次设计中不考虑，待后续性能提升中考虑实现。</p>
<h3 id="算子">3.2 算子</h3>
<p>llama.cpp主要的推理是单算子推理功能，图推理功能在本次设计中暂不考虑实现。昇腾的单算子支持aclop以及aclnn两种调用方式。经过简单的demo进行性能对比，aclop编译执行的方式执行效率较低，主要算子均通过aclnn实现，aclnn不支持的算子使用aclnn基本算子组合的方式实现，后续需要使用AscendC将组合算子进行融合以提高性能。</p>
<h4 id="tensor转换">3.2.1 Tensor转换</h4>
<p>llama.cpp和昇腾算子对Tensor的定义有一定的差异，为了能够使用昇腾算子，需要在调用的时候对Tensor结构做转换。</p>
<p><strong>结构差异</strong></p>
<p>两者的Tensor基本上都是数据和dims，nelements，nstride，dtype的属性集合，但是有一些差异：</p>
<ul>
<li>llama.cpp的ne和nb的顺序是从内到外，也就是与传统意义的维度顺序相反，序号小的是最内的维度。</li>
<li>llama.cpp的stride的单位是字节，而aclnn的stride单位是元素。</li>
</ul>
<p><strong>广义broadcast</strong></p>
<p>当两个计算的tensor维度不同时，会尝试做broadcast，aclnn接口支持的是传统broadcast方式，而llama.cpp支持的是广义的broadcast：</p>
<ul>
<li>aclnn接口的broadcast仅会在Tensor的某个维度不同，但是其中一个Tensor的维度为1的时候发生；</li>
<li>llama.cpp的broadcast会在Tensor的某个维度不同，但一个Tensor的维度大小是另外一个的整数倍的时候发生。</li>
</ul>
<p>为了减少显示broadcast带来的性能和内存的开销，需要进行维度的调整，以便于利用算子的broadcast特性：</p>
<p>例如，Tensor A(9,5,2,7)， Tensor
B(9,10,2,7)，这两个Tensor对aclnn接口来说不可自动broadcast，但是对llama.cpp来说允许自动broadcast。当数据内容连续时，可以通过添加一个维度来兼容aclnn的broadcast规则。
通过将
A(9,5,2,7)转换成A'(9,5,1,2,7)，B(9,10,2,7)转换成B'(9,5,2,2,7)。仅通过调整dims信息，即可利用aclnn算子的自动broadcast能力。</p>
<h4 id="aclnn算子">3.2.1 aclnn算子</h4>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 43%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr>
<th>算子名称</th>
<th>描述</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>Elementwise Add</td>
<td>对两个张量进行逐元素加法，并将结果存储在目标张量中。</td>
<td><span class="math inline">\(dst(i)=src0(i)+src1(i)\)</span></td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td>对输入张量应用 Leaky ReLU 激活函数，并将结果存储在目标张量中。</td>
<td><span class="math display">\[\text{dst(i)}
=\begin{cases}\text{src(i)} &amp; \text{src(i)} \geq \text{0}
\\\text{negative-slope} \times \text{src(i)}&amp; \text{src(i)} &lt;
\text{0}\end{cases}\]</span></td>
</tr>
<tr>
<td>Arange</td>
<td>创建一个从 <code>start</code> 开始，到 <code>stop</code>
结束，每次增长 <code>step</code> 的 Tensor。</td>
<td><span class="math inline">\(\text {out }_{i+1}=\text {out }_i+\text
{step}\)</span></td>
</tr>
<tr>
<td>Clamp</td>
<td>将 input 张量的每个元素夹紧到区间 [min, max]
中，并将结果返回到新的张量中。</td>
<td><span class="math display">\[  \text{dst}(i)
=  \begin{cases}  \text{min} &amp; \text{src(i)} &lt; \text{min}
\\  \text{src(i)} &amp; \text{min} \leq \text{src(i)} \leq \text{min}
\\  \text{max} &amp; \text{src(i)} &gt;
\text{max}  \end{cases}  \]</span></td>
</tr>
<tr>
<td>Scale</td>
<td>使用 <code>scale</code> 缩放一个 Tensor
的所有元素，将结果返回到新的张量中。</td>
<td><span class="math inline">\(dst(i) = src(i) \times
scale\)</span></td>
</tr>
<tr>
<td>Argsort</td>
<td>将输入 Tensor 中的元素根据某个维度进行升序 / 降序排序，返回对应的
index 值。</td>
<td>-</td>
</tr>
<tr>
<td>Layer Norm</td>
<td>对指定层进行均值为 0、标准差为 1
的归一化计算，并将结果写入到新的张量中。</td>
<td><span class="math inline">\(out = \frac{x - E[x]}{\sqrt{Var[x] +
\epsilon}} \times w + b\)</span></td>
</tr>
<tr>
<td>Group Norm</td>
<td>计算输入的组归一化结果返回到新的张量中。</td>
<td>$ out &amp;= + \$</td>
</tr>
<tr>
<td>Acc</td>
<td>将 src 张量的数据累加到 dst 中。</td>
<td><span class="math inline">\(dst(i) = src(i) + dst(i)\)</span></td>
</tr>
<tr>
<td>Sum Rows</td>
<td>返回给定维度中输入张量每行的和。</td>
<td>-</td>
</tr>
<tr>
<td>Upsample Nearest2d</td>
<td>对由多个输入通道组成的输入信号应用最近邻插值算法进行上采样。</td>
<td>-</td>
</tr>
<tr>
<td>Pad</td>
<td>将 Tensor 填充到与目标 Tensor 相同的尺寸。</td>
<td>-</td>
</tr>
<tr>
<td>avg pool2d</td>
<td>对输入 Tensor 进行窗口为 kH×kW、步长为 sH×sW
的二维平均池化操作。</td>
<td><span class="math display">\[\text{out}\left(N_{i}, C_{i}, h,
w\right) = \frac{1}{k H \cdot k W} \sum_{m=0}^{k H-1} \sum_{n=0}^{k W-1}
\text{input}\left(N_{i}, C_{i}, \text{stride}[0] \times h + m,
\text{stride}[1] \times w + n\right)\]</span></td>
</tr>
<tr>
<td>max pooling</td>
<td>对于 dim=3 或 4 维的输入张量，进行最大池化操作。</td>
<td><span class="math inline">\(\text{out}\left(N_{i}, C_{i}, h,
w\right) = \max_{m=0}^{k H-1} \max_{n=0}^{k W-1}
\text{input}\left(N_{i}, C_{i}, \text{stride}[0] \times h + m,
\text{stride}[1] \times w + n\right)\)</span></td>
</tr>
<tr>
<td>rms norm</td>
<td>计算给定 Tensor 的均方根归一化函数，并将结果写入到输出 Tensor
中。</td>
<td><span
class="math inline">\(\text{RmsNorm}\left(x_i\right)=\frac{x_i}{\text{Rms}(\mathbf{x})}
g_i,&lt;br/&gt;   *\quad \text { where }
\text{Rms}(\mathbf{x})=\sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2+e p
s}\)</span></td>
</tr>
<tr>
<td>diag mask</td>
<td>将 Tensor 进行三角形掩码运算，将下三角部分保留，上三角部分置
1。</td>
<td>-</td>
</tr>
<tr>
<td>img2col</td>
<td>用于将二维 Tensor
数据转换成矩阵形式，以便于高效地进行卷积运算。</td>
<td>-</td>
</tr>
<tr>
<td>timestep_embedding</td>
<td>用于生成时间步嵌入。</td>
<td><span class="math inline">\(\text{dst}(t) =
[\sin(\frac{t}{10000^{2i/d}}),
\cos(\frac{t}{10000^{2i/d}})]\)</span></td>
</tr>
<tr>
<td>softmax</td>
<td>将输入的张量转化为概率分布，其值范围在 [0, 1] 之间，总和为 1。</td>
<td>$ (x_i) = $</td>
</tr>
<tr>
<td>matmul</td>
<td>计算两个 Tensor 的矩阵乘法，结果返回到新的 Tensor 中。</td>
<td>$ C_{ij} = <em>{k=1}^{n} A</em>{ik} B_{kj}$</td>
</tr>
<tr>
<td>Rope</td>
<td>算子是一种位置编码方法，通过旋转操作为输入序列引入位置信息，增强模型对位置关系的感知能力。</td>
<td><span class="math inline">\(\text{ROPE}(q, k) = \left[
q_{\text{even}} \cos(\theta) - q_{\text{odd}} \sin(\theta), ;
q_{\text{odd}} \cos(\theta) + q_{\text{even}} \sin(\theta)
\right]\)</span></td>
</tr>
<tr>
<td>repeat</td>
<td>对输入张量的元素沿特定维度重复，扩展原始数据的维度或增加相同数据的次数。</td>
<td><span class="math inline">\(\text{repeat}(x) = [x, x, \dots, x]
\quad (\text{repeated along specified dimension})\)</span></td>
</tr>
<tr>
<td>concat</td>
<td>将两个或多个张量在指定维度上拼接。</td>
<td><span class="math inline">\(\text{concat}(x_1, x_2, \dots, x_n) =
[x_1, x_2, \dots, x_n] \quad (\text{along specified
dimension})\)</span></td>
</tr>
<tr>
<td>Cast</td>
<td>将张量的数据类型从一种类型转换为另一种类型。</td>
<td>-</td>
</tr>
<tr>
<td>permute</td>
<td>重新排列张量的维度顺序。</td>
<td>-</td>
</tr>
<tr>
<td>exp</td>
<td>对 Tensor 的每个元素执行 exp 指数运算。</td>
<td><span class="math inline">\(\text{dst}_i =
e^{\text{src}_i}\)</span></td>
</tr>
<tr>
<td>Elementwise Mul</td>
<td>对两个张量对应元素进行乘法运算。</td>
<td><span class="math inline">\(z = x \times y\)</span></td>
</tr>
<tr>
<td>Cos</td>
<td>对张量的每个元素计算余弦值。</td>
<td><span class="math inline">\(y = \cos(x)\)</span></td>
</tr>
<tr>
<td>Sin</td>
<td>对张量的每个元素计算正弦值。</td>
<td><span class="math inline">\(y = \sin(x)\)</span></td>
</tr>
<tr>
<td>fill scalar</td>
<td>将张量的所有元素填充为指定的标量值。</td>
<td><span class="math inline">\(x[:] = \text{scalar}\)</span></td>
</tr>
<tr>
<td>pow tensor</td>
<td>将一个张量的每个元素提升到对应的指数幂。</td>
<td><span class="math inline">\(y = x^{\text{power}}\)</span></td>
</tr>
<tr>
<td>Alibi</td>
<td>一种相对位置嵌入策略，在注意力分数中加入线性偏置，帮助捕获相对位置信息。</td>
<td>$(i, j) = -m </td>
</tr>
<tr>
<td>repeat interleave</td>
<td>对张量的每个元素按指定次数重复，以在张量中插入更多的副本。</td>
<td><span class="math inline">\(\text{dst}(x, \text{repeats}) = [x_1,
x_1, \dots, x_1, x_2, x_2, \dots, x_2, \dots]\)</span></td>
</tr>
<tr>
<td>roll</td>
<td>将张量元素沿指定维度循环移动，即滚动。</td>
<td><span class="math inline">\(\text{roll}(x, \text{shift}) =
x_{\text{shifted along axis}}\)</span></td>
</tr>
<tr>
<td>index fill tensor</td>
<td>在张量的特定索引位置填充指定值。</td>
<td><span class="math inline">\(dst[\text{index}] =
\text{src}\)</span></td>
</tr>
</tbody>
</table>
<h4 id="ascendc算子">3.2.2 AscendC算子</h4>
<p>以下算子没有aclnn接口可调用，也无法使用基础算子组合，需要通过AscendC编程语言实现。为了简化算子的调用流程，采用kernel
call的方式进行调用。AscendC算子独立编译，以<code>.a</code>的方式链接到llama.cpp中。</p>
<p><strong>dup</strong></p>
<p>dup和copy语义相同，均为Tensor之间的拷贝，需要支持：</p>
<ul>
<li>量化Tensor和非量化Tensor之间的拷贝，拷贝过程中涉及到量化和反量化的计算过程。需要支持Q4_0和Q8_0两种量化格式。非量化格式需要支持fp32和fp16两种格式。</li>
<li>连续Tensor和非连续Tensor之间的拷贝（量化格式Tensor不涉及非连续场景）。</li>
</ul>
<p><strong>get rows</strong></p>
<p>从Tensor中按照index获取每行内容。</p>
<p>需要支持多种数据格式，包括fp32，fp16，Q4_0和Q8_0。获取后的数据均为fp32格式。</p>
<p>AscendC算子通过kernel launch的方式调用，调用时需要判断AI
core的数量，来配置合适的数量以提升执行效率。</p>
<p>为了兼容多种芯片，CMake时需要检测或根据提供的芯片类型进行编译和链接。</p>
<h3 id="内存管理">3.3 内存管理</h3>
<p>aclnn执行时，有些需要申请临时的NPU上内存做临时数据存储，频繁的内存分配和释放效率很低，需要内存池来提高内存分配性能。</p>
<p>在llama.cpp中，需要实现2中内存池：</p>
<p><strong>legacy pool</strong></p>
<p>使用N（256）个buffer做内存缓存，所有的内存释放必须放回内存池（防止异步执行访问到已释放内存），内存申请首先选择内存池中大小最合适的缓存，<strong>内存池为空则去申请内存</strong>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/47dd1352ecaef88457cf9dc7adba3f6e-image-20240806193006017-6f2692.png"
alt="legacy pool流程图" />
<figcaption aria-hidden="true">legacy pool流程图</figcaption>
</figure>
<p>会占用额外的内存，并且存在内存块查找的开销，并且，如果free的内存块超过N（256），则会出现assert失败问题。</p>
<p><strong>vmm pool</strong></p>
<p>使用虚拟内存，业务代码看到的是一段连续的内存，方便使用。实际上申请的物理内存是非连续的，当内存不足时申请一段物理内存映射到虚拟内存中。避免内存碎片和占用额外内存。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/14e4c332447ad345329843f1478b39a3-image-20240806193622691-6c7fee.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p>在虚拟内存中，申请的数据紧密排列，申请和销毁的顺序是相反的。比如，buffer1早于buffer2申请，那么buffer2必须要早于buffer1释放。在内存管理中，仅维护一个free指针，指示下一个buffer申请的起始地址。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/70ebc785a616889aa3601a77a8d74fe0-image-20240806194704808-3b08a3.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p><strong>异步计算的内存延迟释放</strong></p>
<p>由于所有的算子计算都是异步的，但是内存的申请和释放并不是异步的，所以，需要保证在异步计算完成之前，申请的内存是有效的。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/73704ff1cfbe476e085afc7fbfdf0c6a-image-20240806195347967-5ab4b4.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p>如图所示，当算子提交完成后，buffer3就会释放，free指针指向buffer3的起始地址。接着，下个算子开始执行，会从free指针开始申请内存，此时buffer3和buffer4是重叠的，但是由于stream中的算子计算有序，所以buffer3内的数据在完成计算之前，是不会被buffer4修改的。</p>
<h3 id="量化格式">3.4 量化格式</h3>
<p>以4bit量化为例：</p>
<p><strong>量化分组格式</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> QK4_0 32 <span class="comment">// 每组32个f32数据</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">    ggml_half d; <span class="comment">// 公共系数</span></span><br><span class="line">    <span class="type">uint8_t</span> qs[QK4_0 / <span class="number">2</span>]; <span class="comment">// 4bit存储的数据</span></span><br><span class="line">&#125; block_q4_0;</span><br></pre></td></tr></table></figure>
<p><strong>量化算法描述</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quantize_row_q4_0_reference</span><span class="params">(<span class="type">const</span> <span class="type">float</span> * restrict x, block_q4_0 * restrict y, <span class="type">int64_t</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="type">static</span> <span class="type">const</span> <span class="type">int</span> qk = QK4_0;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">assert</span>(k % qk == <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> nb = k / qk;</span><br><span class="line">		</span><br><span class="line">  	<span class="comment">// 1. 找到绝对值最大的数的值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nb; i++) &#123;</span><br><span class="line">        <span class="type">float</span> amax = <span class="number">0.0f</span>; <span class="comment">// absolute max</span></span><br><span class="line">        <span class="type">float</span> max  = <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; qk; j++) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> v = x[i*qk + j];</span><br><span class="line">            <span class="keyword">if</span> (amax &lt; <span class="built_in">fabsf</span>(v)) &#123;</span><br><span class="line">                amax = <span class="built_in">fabsf</span>(v);</span><br><span class="line">                max  = v;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 2. 公共系数是第一步的值除以 -8</span></span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> d  = max / <span class="number">-8</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> id = d ? <span class="number">1.0f</span>/d : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        y[i].d = <span class="built_in">GGML_FP32_TO_FP16</span>(d);</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 3. 对组内的所有数据，除以公共系数，然后按以下顺序存储</span></span><br><span class="line">        <span class="comment">// 量化前： 1,2,3,4,5,6,7,8 ...... 30,31</span></span><br><span class="line">      	<span class="comment">// 量化后： 1,17,2,18,3,19 ...... 16,32</span></span><br><span class="line">        <span class="comment">// 也就是数据按顺序先填充量化后组的低4位，然后再填充高4位。</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; qk/<span class="number">2</span>; ++j) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x0 = x[i*qk + <span class="number">0</span>    + j]*id;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x1 = x[i*qk + qk/<span class="number">2</span> + j]*id;</span><br><span class="line">						<span class="comment">// 量化的值+8.5，向上去整并转无符号数。</span></span><br><span class="line">            <span class="type">const</span> <span class="type">uint8_t</span> xi0 = <span class="built_in">MIN</span>(<span class="number">15</span>, (<span class="type">int8_t</span>)(x0 + <span class="number">8.5f</span>));</span><br><span class="line">            <span class="type">const</span> <span class="type">uint8_t</span> xi1 = <span class="built_in">MIN</span>(<span class="number">15</span>, (<span class="type">int8_t</span>)(x1 + <span class="number">8.5f</span>));</span><br><span class="line"></span><br><span class="line">            y[i].qs[j]  = xi0;</span><br><span class="line">            y[i].qs[j] |= xi1 &lt;&lt; <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>NPU善于做向量和矩阵计算，按字节的计算，以及位计算性能不佳。
所以需要调整数据存储格式。</p>
<p>在set
tensor过程中，使用cpu做tensor的内存调整，让后续的计算能够充分利用NPU能力。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/3580ce93f92d2075f1ee71f434f9d54a-image-20240806191922718-496986.png"
alt="量化内存格式调整示意图" />
<figcaption aria-hidden="true">量化内存格式调整示意图</figcaption>
</figure>
<p>如上图所示，输入的Tensor是按组存放的，每组存放了该组的公共系数，以及32个数据的量化后的值，int4类型量化值是先填充高4位，再填充低4位。为了昇腾的计算效率，在做这类伪量化算法时，将原始Tensor拆解成2个Tensor，一个按顺序记录所有的值，另外一个记录每一组的公共系数，并且值和公共系数按32:1的方式对应。然后昇腾算子按照weight和group
scale的方式输入进算子，能够提高量化后Tensor的执行效率。</p>
<p>所有的内存布局修改的时机是set tensor和get
tensor过程中，对于整个程序来说，对内存布局的修改是不感知的，拷贝到NPU上时进行布局修改，从NPU下载时再进行布局复原，这样，及时设计CPU，NPU混合运算，也不会影响数据的正确性。</p>
<p>对于更加复杂的量化方式，例如q5_0，需要设计到位运算，此类量化由于性能问题尚未支持。</p>
<h3 id="代码风格和注释">3.5 代码风格和注释</h3>
<p>llama.cpp社区对代码风格没有详细的要求，社区仅要求”清除所有尾随空格，使用
4 个空格缩进，括号在同一行<code>void * ptr</code>，int &amp;
a`”。并且，对注释也没有明确的要求。为了能够保持一致的风格，以及方便社区开发者了解昇腾后端的业务逻辑，需要在编写代码时遵循一致的编码规范和详尽的注释。</p>
<ul>
<li>除了社区要求的内容之外，其他代码规范需要遵循google编码规范；</li>
<li>注释需要包含函数和变量的介绍，参数和返回值说明，算子相关代码需要注释算法的数学公式。其他的复杂逻辑按需求添加注释；</li>
<li>注释需要符合doxygen风格，以便于生成方便阅读的手册。</li>
</ul>
<h2 id="测试和验证">4. 测试和验证</h2>
<p>本设计文档主要是昇腾的后端支持，llama.cpp已经做了后端抽象，所有测试用例可以复用社区的内容。针对社区用例没有看护到的部分，添加必要的用例来看护。</p>
<h3 id="runtime测试">4.1 Runtime测试</h3>
<p>Runtime测试主要是验证设备注册，内存分配，stream和event管理相关功能。</p>
<p><strong>设备注册和卸载（单卡，多卡）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证昇腾设备可以正常注册到llama.cpp中，支持单卡和多卡注册。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>调用设备注册接口，注册单卡以及多卡；</li>
<li>查看设备信息是否正常获取。</li>
</ol>
<p><strong>预期结果</strong></p>
<p>无报错信息，并根据<code>ASCEND_VISIBLE_DEVICES</code>的设置情况，能够正常获取到对应的设备信息。</p>
<p><strong>buffer和Tensor创建</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证昇腾后端可以正常的创建buffer以及llama.cpp的Tensor结构；</li>
</ul>
<p><strong>测试步骤</strong></p>
<ul>
<li>构造若干个Tensor结构，并给这些Tensor分配内存buffer；</li>
</ul>
<p><strong>预期结果</strong></p>
<ul>
<li>内存完成分配，无错误信息。</li>
</ul>
<p><strong>Tensor的上传和下载（同步，异步）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证数据可以正确的上传和下载。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机数据；</li>
<li>将数据拷贝到创建好的Tensor中；</li>
<li>将Tensor中的数据下载；</li>
<li>与原始随机数据进行对比。</li>
<li>分别使用同步拷贝和异步拷贝，重复以上过程。</li>
</ol>
<p><strong>预期结果</strong></p>
<ul>
<li>数据比对与原始数据相同。</li>
</ul>
<p><strong>Tensor卡间拷贝（包括event同步）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证卡间拷贝以及事件同步的正确性。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机数据；</li>
<li>将数据拷贝到卡1的Tensor中；</li>
<li>开启卡1和卡2的卡间拷贝开关；</li>
<li>在卡1的stream提交卡1Tensor向卡2Tensor拷贝的任务；</li>
<li>在拷贝流中插入卡2的event事件；</li>
<li>在卡2的流中等待event事件；</li>
<li>从卡2中下载Tensor数据；</li>
<li>与原始数据做比对。</li>
</ol>
<p><strong>预期结果</strong></p>
<ol type="1">
<li>数据比对与原始数据相同；</li>
<li>卡2event同步正确，在卡1stream中构造耗时操作，确保event能够等待拷贝动作结束。</li>
</ol>
<p><strong>量化拷贝验证</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>量化Tensor拷贝需要调整内存布局，验证量化Tensor的拷贝结果正确。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机的量化Tensor；</li>
<li>将量化Tensor上传到设备上；</li>
<li>使用aclrtmemcpy直接拷贝数据；</li>
<li>从设备上将Tensor下载下来；</li>
<li>与原始数据作对比。</li>
</ol>
<p><strong>预期结果</strong></p>
<ol type="1">
<li>步骤3的memcpy的结果与原始数据不同，因为上传过程做了内存布局调整；</li>
<li>步骤5数据对比与原始数据相同。</li>
</ol>
<h3 id="算子单元测试">4.2 算子单元测试</h3>
<p>单元测试复用社区的单元测试用例(test-backend-ops)，包含1500多个用例。其覆盖的场景有：</p>
<ul>
<li>算子多shape多dtype验证，保证该算子所有的输出输出的shape和dtype类型都能够覆盖；</li>
<li>算子的精度验证，用例会构造随机数据，分别在设备上和CPU上运行，最后对比精度，两个Tensor的归一化方差需要小于
1e-6。</li>
<li>计算结果越界检查，由于推理过程中，Tensor是紧密排列，所以每个tensor的计算结果不能越界，否则会损坏其他tensor的数据，用例会在每个输入和输出tensor前后分别放置一个随机tensor，通过对比随机tensor的计算前后的结果，来检查是否存在越界行为。</li>
</ul>
<p>单元测试用例会判断后端的算子支持情况，理论上，所有支持的算子（包括shape和dtype）都需要通过该测试用例集的验证。</p>
<h3 id="性能测试">4.3 性能测试</h3>
<p>算子的性能测试用例与单元测试用例相同，区别是性能测试用例不会验证精度，也不会创建随机tensor用作越界检查。性能测试会构造一个特殊的图，包含最多8192个计算节点，然后交给后端进行推理，并计算平均每次的执行时间，以及数据吞吐率。</p>
<ul>
<li>910B对于简单算子（包括直接调用aclnn接口的，或者做了简单的参数调整的）性能要超过Intel主流CPU的性能。</li>
<li>对于复杂算子（包括构造多个临时tensor，以及需要多个算子组合的）暂不做算子的性能要求。</li>
<li>非910B芯片，不做性能要求。</li>
</ul>
<p>910B模型推理（llama3
8B）整体性能，token延迟需要小于100ms（人类的阅读速度大致是10个token/s，延迟小于100ms，可以满足人类的阅读需求），吞吐需要超过300token/s（0.6
* A100 vllm llama3 8B的推理性能）。</p>
<p>以下为 Qwen 2.5 全系列模型在昇腾 910B 上的推理性能表现汇总数据，包括
Qwen2.5 0.5B、1.5B、3B 的 Q8_0 和Q4_0
量化的推理性能数据作为对比参考：</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 27%" />
<col style="width: 16%" />
<col style="width: 15%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><strong>Model</strong></th>
<th style="text-align: left;"><strong>Tokens</strong> <strong>/</strong>
<strong>Second</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Util</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Mem</strong></th>
<th style="text-align: left;"><strong>NPU Card（64G/Card）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B FP16</td>
<td style="text-align: left;">42 tokens/second</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B FP16</td>
<td style="text-align: left;">35 tokens/second</td>
<td style="text-align: left;">Util 11~13%</td>
<td style="text-align: left;">Mem 10%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B FP16</td>
<td style="text-align: left;">29 tokens/second</td>
<td style="text-align: left;">Util 15~16%</td>
<td style="text-align: left;">Mem 15%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 7B FP16</td>
<td style="text-align: left;">32 tokens/second</td>
<td style="text-align: left;">Util 16~21%</td>
<td style="text-align: left;">Mem 16%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 14B FP16</td>
<td style="text-align: left;">19 tokens/second</td>
<td style="text-align: left;">Util 19~22%</td>
<td style="text-align: left;">Mem 28%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 32B FP16</td>
<td style="text-align: left;">10.5 tokens/second</td>
<td style="text-align: left;">Util 10~45%</td>
<td style="text-align: left;">Mem 54%</td>
<td style="text-align: left;">双卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 72B FP16</td>
<td style="text-align: left;">6 tokens/second</td>
<td style="text-align: left;">Util 10~60%</td>
<td style="text-align: left;">Mem 78%</td>
<td style="text-align: left;">三卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B Q8_0</td>
<td style="text-align: left;">6.5 tokens/second</td>
<td style="text-align: left;">Util 2~5%</td>
<td style="text-align: left;">Mem 6%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B Q4_0</td>
<td style="text-align: left;">6 tokens/second</td>
<td style="text-align: left;">Util 4~5%</td>
<td style="text-align: left;">Mem 6%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B Q8_0</td>
<td style="text-align: left;">3.5 tokens/second</td>
<td style="text-align: left;">Util 4~11%</td>
<td style="text-align: left;">Mem 8%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B Q4_0</td>
<td style="text-align: left;">17~18 tokens/second</td>
<td style="text-align: left;">Util 9~12%</td>
<td style="text-align: left;">Mem 7%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B Q8_0</td>
<td style="text-align: left;">3.2 tokens/second</td>
<td style="text-align: left;">Util 10~15%</td>
<td style="text-align: left;">Mem 10%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B Q4_0</td>
<td style="text-align: left;">14.5 tokens/second</td>
<td style="text-align: left;">Util 8~15%</td>
<td style="text-align: left;">Mem 8%</td>
<td style="text-align: left;">单卡</td>
</tr>
</tbody>
</table>
<p>对其中的 Qwen 2.5 0.5B FP16 模型进行并发测试的性能表现如下：</p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 31%" />
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><strong>Concurrency</strong></th>
<th style="text-align: left;"><strong>Tokens</strong> <strong>/</strong>
<strong>Second</strong></th>
<th style="text-align: left;"><strong>Throughput</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Util</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Mem</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">39 tokens/second</td>
<td style="text-align: left;">39</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">38 tokens/second</td>
<td style="text-align: left;">76</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">37.66 tokens/second</td>
<td style="text-align: left;">113</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">34.25 tokens/second</td>
<td style="text-align: left;">137</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">31 tokens/second</td>
<td style="text-align: left;">155</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">28.16 tokens/second</td>
<td style="text-align: left;">169</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">27.57 tokens/second</td>
<td style="text-align: left;">193</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">26.87 tokens/second</td>
<td style="text-align: left;">215</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">26 tokens/second</td>
<td style="text-align: left;">234</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">10</td>
<td style="text-align: left;">26.9 tokens/second</td>
<td style="text-align: left;">269</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">20</td>
<td style="text-align: left;">20.3 tokens/second</td>
<td style="text-align: left;">406</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 8%</td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">10.34 tokens/second</td>
<td style="text-align: left;">517</td>
<td style="text-align: left;">Util 3~5%</td>
<td style="text-align: left;">Mem 8%</td>
</tr>
<tr>
<td style="text-align: left;">100</td>
<td style="text-align: left;">4.17 tokens/second</td>
<td style="text-align: left;">417</td>
<td style="text-align: left;">Util 2~5%</td>
<td style="text-align: left;">Mem 9%</td>
</tr>
</tbody>
</table>
<h3 id="模型精度验证">4.4 模型精度验证</h3>
<p>除了算子的精度验证以外，对模型需要做整体的精度验证，以避免在数据加载拷贝，kv_cache操作等过程中出现错误。</p>
<p><strong>eval-callback</strong></p>
<p>llama.cpp社区提供了一个精度对比工具：eval-callback，这个工具会执行一次推理过程，并将推理过程中所有涉及的算子的计算结果进行打印。通过对比相同seed情况下的NPU和CPU的推理结果，判断整个推理过程是否存在异常。</p>
<p>需要注意的是，tensor的内容在会存在微小的差异，这不属于精度异常。</p>
<p><strong>CPU推理对比</strong></p>
<p>使用llama3模型，使用相同的seed，分别在NPU和CPU上进行相同的推理内容，理论上前数百token应该完全一致。由于存在精度的微小差异，推理累计的过程中，在长回复的后段，可能会出现细微差异。</p>
<h3 id="模型支持验证">4.5 模型支持验证</h3>
<p>目前，llama.cpp支持以下模型以及多种量化格式，我们仅关注fp16，Q8_0和Q4_0三种dtype。</p>
<p>模型支持的原则是不存在不支持的算子，检查方式是查看切图的情况，如果出现了大量子图（超过100），说明存在算子不支持，已经fallback到CPU进行推理，此类模型虽然能够完成推理，但是推理性能较低。</p>
<table>
<thead>
<tr>
<th style="text-align: center;">模型</th>
<th style="text-align: center;">FP16</th>
<th style="text-align: center;">Q8_0</th>
<th style="text-align: center;">Q4_0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AquilaChat2-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Baichuan-7b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Baichuan2-7B-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bitnet_b1_58-large</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bloom-560m</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bloomz-alpaca-560m</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">c4ai-command-r-35B-v01</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">chatglm3-6B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">chinese-alpaca-2-1.3b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">CodeShell-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td
style="text-align: center;">deepseek-ai_deepseek-coder-1.3B-base</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">deepseek-ai_DeepSeek-V2-Lite</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">deepseek-coder-6.7B-instruct</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-V2-Lite-64x1.5B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">falcon-7b-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">flan-t5-large</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">gemma-2-9b-it</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">glm-4-9B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">gpt2</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Gpt2-163M</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">granite-3B-code-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">GritLM-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">internlm2_5-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">koala-7B-HF</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7b-chat-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-Smaug-8B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-Chinese-7b-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama3-8B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama3-8b-chinese</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">mamba-130m-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">mpt-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">OLMo-1B-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">OpenELM-3B-Instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Orion-14b-base</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">phi1</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">phi2</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">Phi-3-mini-4k-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">plamo-13b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">pythia-70M</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-1.5B-Instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Refact-1_6B-fim</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">SmolLM-135M</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">stablelm-zephyr</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">stablelm-2-zephyr-1_6b</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">starcoderbase-1b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">starcoder2-3b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">vigogne-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">xverse-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Yi-6b-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
</tbody>
</table>
<h3 id="社区ci">4.6 社区CI</h3>
<p>目前由于资源限制，暂时无法向社区提供开发机和CI机器，但是需要保证编译通过，防止社区的重构导致的昇腾后端被破坏的问题。编译不需要昇腾硬件，可以使用社区的CI机器。</p>
<ul>
<li>提供昇腾构建的容器镜像，避免配置复杂的环境。</li>
<li>提供github workflow的job，添加昇腾的CI验证，并作为门禁。</li>
</ul>
<h2 id="ollama支持">5. Ollama支持</h2>
<p>Ollama
是一个旨在提升本地大型语言模型（LLM）运行效率和灵活性的开源平台，快速在本地部署启动大模型的应用。Ollama
的设计初衷是通过优化硬件加速和支持更高效的推理计算，帮助开发者和研究人员更方便地在本地部署和运行
LLM，从而不依赖云计算资源或其他昂贵的基础设施。，Ollama使用llama.cpp作为推理引擎。一条命令可以完成安装和模型拉起。</p>
<p><strong>安装</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>
<p><strong>运行</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3</span><br></pre></td></tr></table></figure>
<p>除此之外，Ollama还有有一个模型的仓库，保存有海量的gguf模型，其兼容openAI
API，有着众多的前端应用。</p>
<p>为了能够充分的利用llama.cpp的昇腾后端能力，简化昇腾使用门槛，同时需要完成Ollama的昇腾适配。简单来说，OIlama需要适配一下几个关键部分：</p>
<p><strong>构建</strong></p>
<p>Ollama会构建llama.cpp工程，并将二进制打包到ollama的二进制文件中，在构建ollama的过程中，需要完成llama.cpp的昇腾版本的构建。</p>
<p><strong>NPU检测</strong></p>
<p>Ollama在运行时会检测NPU硬件，显存容量等，来判断模型是否能够运行，以及合理的模型拆分方式，所以需要在ollama中实现必要的昇腾硬件检测接口。</p>
<p><strong>拉起</strong></p>
<p>Ollama运行模型时，会拉起对应后端的llama.cpp服务器，这里需要根据硬件检测的结果来拉起NPU版本的llama.cpp服务器。</p>
<p>这里仅做Ollama兼容昇腾后端的简单洞察，不做详细设计，社区方案已完成，PR提交中。</p>
<h2 id="社区跟进">6. 社区跟进</h2>
<p>llama.cpp是一个非常活跃的社区，平均每天有十几个提交的合入，包括大量的重构和大粒度特性的合入。昇腾后端需要紧跟社区的发展路线，根据社区的重构和特性进行适配。</p>
<p>同时，在社区也存在对昇腾后端的需求，以及问题反馈，需要及时完成解决。</p>
<p>社区没有要求SLA，原则上，简单问题修复和重构适配应当在5个工作日内完成，特性需求根据实际情况灵活处理。</p>
<h2 id="文档和说明">7. 文档和说明</h2>
<p>为了帮助llama.cpp的昇腾用户，需要编写详尽的文档，包括环境搭建，构建，运行，模型和数据类型支持情况以及贡献指导等。</p>
<h3 id="社区doc">7.1 社区doc</h3>
<ul>
<li>在社区README添加Ascend的支持描述，并且提供跳转链接。</li>
<li>提供环境搭建步骤，包括操作系统版本，昇腾驱动和CANN的版本要求和安装方法。</li>
<li>提供Dockerfile，包含llama.cpp所需的环境配置，能够避免复杂的环境部署。</li>
<li>提供构建，运行的命令。</li>
<li>提供模型和数据类型支持情况。</li>
<li>提供issue和PR提交规范。</li>
</ul>
<h3 id="昇腾开源手册">7.2 昇腾开源手册</h3>
<p>为了方便中文用户，以及昇腾社区入口的用户，还需要在昇腾开源文档中提供中文版的step
by step构建和推理手册。</p>
<table>
<colgroup>
<col style="width: 85%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th>PR</th>
<th>代码量</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/6035">[CANN]
Add Ascend NPU backend #6035</a></td>
<td>+10,756 −8</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/8867">[CANN]
Add doc and docker image #8867</a></td>
<td>+329 −0</td>
</tr>
</tbody>
</table>
<p>其他参与review的PR和issue见<a
target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/issues?q=+label%3A%22Ascend+NPU%22+">链接</a>。</p>
<p><a
target="_blank" rel="noopener" href="https://ascend.github.io/docs/sources/llama_cpp/index.html">llama.cpp昇腾开源使用手册</a>。</p>
<h2 id="项目引用">8. 项目引用</h2>
<p><strong>ollama</strong></p>
<p><a
target="_blank" rel="noopener" href="https://github.com/ollama/ollama">Ollama</a>是一款专注于在本地运行大型语言模型的工具，旨在简化模型的部署和使用，提供高性能且无需云端依赖的AI推理体验，使用llama.cpp作为推理引擎，以git
submodule的方式引用llama.cpp代码。目前已与2012同事一同完成设计并提交<a
target="_blank" rel="noopener" href="https://github.com/ollama/ollama/pull/5872">PR</a>。</p>
<p><strong>llama edge</strong></p>
<p><a target="_blank" rel="noopener" href="https://github.com/LlamaEdge/LlamaEdge">Llama
Edge</a>是一个为边缘设备优化的轻量级大语言模型框架，旨在支持本地化、高效的推理，以满足低延迟和有限资源的计算需求，使用llama.cpp作为其推理后端。llama
edge官方发表了一篇知乎的<a
target="_blank" rel="noopener" href="https://www.zhihu.com/question/624955377/answer/13849002583?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_psn=1833491376626614272&amp;utm_source=wechat_session&amp;utm_id=0">回复</a>以及一篇<a
target="_blank" rel="noopener" href="https://www.secondstate.io/articles/llm-agents-on-ascend/">官方文档</a>。</p>
<p><strong>llamabox&amp;gpu stack</strong></p>
<p><a
target="_blank" rel="noopener" href="https://github.com/gpustack/llama-box">Llamabox</a>是一个便捷的平台，提供开箱即用的大语言模型部署方案，使用户能够轻松运行和管理AI模型；而<a
target="_blank" rel="noopener" href="https://github.com/gpustack/gpustack">gpustack</a>是一项云服务，专为高性能计算和AI模型训练优化，提供灵活的GPU资源共享和管理功能，其使用了llama.cpp作为其推理后端之一。有一篇使用gpustack使用昇腾推理的实践<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/KZ-kuNUx03BPo9vlK1OPxg">文章</a>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/01/Triton-%E5%AF%B9-PyTorch%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/11/01/Triton-%E5%AF%B9-PyTorch%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7/" class="post-title-link" itemprop="url">Triton 对 PyTorch的重要性</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-01 15:45:03" itemprop="dateCreated datePublished" datetime="2024-11-01T15:45:03+08:00">2024-11-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:45:51" itemprop="dateModified" datetime="2025-01-23T15:45:51+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="为什么需要编译">为什么需要编译</h2>
<p>相比于Eager模式，编译有以下优势：</p>
<ul>
<li><p><strong>优化性能</strong>：通过全局优化，减少算子之间的数据拷贝来提升执行速度。</p></li>
<li><p><strong>降低延迟</strong>：提前优化代码逻辑，减少运行时解释开销，适用于实时应用。</p></li>
<li><p><strong>复杂优化策略</strong>：实现操作融合、并行执行等高级优化。</p></li>
<li><p><strong>更快的算法支持</strong>：不需要实现对应的各种融合算法，完成lower和pass即可适配。</p></li>
<li><p><strong>静态分析与错误检查</strong>：提前捕捉潜在错误，提高代码质量。</p></li>
</ul>
<p>所以编译模式是一定要支持的技术路线。</p>
<h2 id="为什么pytorch编译需要triton">为什么PyTorch编译需要Triton</h2>
<figure>
<img src="https://pytorch.org/assets/images/pytorch-2.0-img12.png"
alt="PyTorch 2.0" />
<figcaption aria-hidden="true">PyTorch 2.0</figcaption>
</figure>
<p><strong>为什么选择Inductor</strong></p>
<p>PyTorch有多个后端编译器，都可以从FX
Graph进行模型编译。Intree的有cudagraph，onnxrt，openxla，tvm以及inductor，out
of
tree的有nvFuser和AITemplate等。Inductor相较于其他编译器，有以下优势，并逐渐成为PyTorch主力发展的编译器。</p>
<ul>
<li><p><strong>动态编译</strong>：能够根据输入数据的特性在运行时生成优化代码，实现最佳性能。</p></li>
<li><p><strong>实时优化</strong>：允许在推理过程中对计算图进行修改和优化，无需重新编译整个模型。</p></li>
<li><p><strong>支持复杂操作</strong>：处理多种复杂操作和动态控制流，增强模型的表达能力。</p></li>
<li><p><strong>集成现有 API</strong>：与 PyTorch API
无缝集成，降低学习曲线，方便开发者使用。</p></li>
<li><p><strong>硬件适配</strong>：根据不同硬件生成特定优化代码，充分利用计算能力，目前支持CPU和GPU。</p></li>
<li><p><strong>配置选项</strong>：提供多种参数和选项，允许用户自定义优化策略，增强灵活性。</p></li>
<li><p><strong>性能调优</strong>：在运行时监控性能，根据实际情况自动选择最优执行路径。</p></li>
</ul>
<p>Inductor会做很多通用优化，使得使用Inductor
IR的后端均可以享受到这些优化的能力。</p>
<ul>
<li>Integrating a new backend at the AtenIR/PrimsIR level is
straightforward and provides more freedom to the new backend to optimize
the captured graph. But it might be suboptimal performance if the
backend lacks the DL compiler capability because the decomposed
operations for a single operation might need more memory and worse data
locality compared with the single operation if the decomposed operations
can't be fused.</li>
<li>Integrating at the Inductor loop IR level can significantly simplify
the complexity of design and implementation by leveraging the Inductor's
fusion capability and other optimizations directly. The new backend just
needs to focus on how to generate optimal code for a particular
device.</li>
</ul>
<p><strong>为什么选择Triton</strong></p>
<p>Inductor目前支持三个后端：OpenMP，Halide以及Trition</p>
<p>Triton 是一种用于并行编程的语言和编译器。它的目标是提供一个基于
Python
的编程环境，以便高效地编写自定义的深度神经网络（DNN）计算核心，这些核心能够在现代
GPU 硬件上以最大吞吐量运行。</p>
<p>AI算子编写2个最关键的点：</p>
<ol type="1">
<li>编写的复杂程度；</li>
<li>算子执行的效率。</li>
</ol>
<p>Triton的目标是使用简单的实现方法（类似Python语言），让绝大部分的开发者能够开发出媲美CUDA专家编写出的算子执行效率。</p>
<figure>
<img
src="https://triton-lang.org/main/_images/sphx_glr_01-vector-add_001.png"
alt="01 vector add" />
<figcaption aria-hidden="true">01 vector add</figcaption>
</figure>
<figure>
<img
src="https://triton-lang.org/main/_images/sphx_glr_02-fused-softmax_001.png"
alt="02 fused softmax" />
<figcaption aria-hidden="true">02 fused softmax</figcaption>
</figure>
<figure>
<img
src="https://triton-lang.org/main/_images/sphx_glr_03-matrix-multiplication_001.png"
alt="03 matrix multiplication" />
<figcaption aria-hidden="true">03 matrix multiplication</figcaption>
</figure>
<figure>
<img
src="https://triton-lang.org/main/_images/sphx_glr_05-layer-norm_001.png"
alt="05 layer norm" />
<figcaption aria-hidden="true">05 layer norm</figcaption>
</figure>
<figure>
<img
src="https://triton-lang.org/main/_images/sphx_glr_08-grouped-gemm_001.png"
alt="08 grouped gemm" />
<figcaption aria-hidden="true">08 grouped gemm</figcaption>
</figure>
<p>上图可以看出，使用简单的算子实现可以与cuBLAS以及Torch
aten算子相同甚至更高的执行效率。</p>
<p>除此之外，FlagGems算子库，unsloth微调框架也基于Triton编写算子，并且字节等互联网公司也在使用Triton来高效的编写后端无关的高性能算子。</p>
<h2 id="triton架构">Triton架构</h2>
<figure>
<img
src="https://picx.zhimg.com/80/v2-a971477a47f3192f1881bfdfb1458855_1440w.png?source=d16d100b"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Nvidia GPU: TritonDSL -------------&gt; ttir -&gt; ttgir -&gt; ptx -&gt; cubin</span><br><span class="line">                       ast.parse   ^     \/   ^     \/</span><br><span class="line">                                   | pass|   | pass |</span><br><span class="line">                                   +-----+   +--=---+</span><br><span class="line">                                   |                |</span><br><span class="line">                                   |-------MLIR-----|</span><br></pre></td></tr></table></figure>
<p>Triton的工作原理就是将TritonDSL一步步转换成适合于硬件执行的二进制的过程。</p>
<ul>
<li>Triton利用llvm的MLIR框架，注册TritonIR以及TritonGpuIR，以及相应的pass和convert;</li>
<li>首先Triton会将代码用ast.parse解析，将ast语法书转换成ttir；</li>
<li>使用注册的pass和convert函数，对ttir进行优化并转换成ttgir；</li>
<li>继续用pass和convert函数，对ttgir转换成llvm ir；</li>
<li>使用nvidia提供的工具将llvm ir转换成ptx文件；</li>
<li>使用nvidia提供的工具将ptx文件编译成二进制cubin。</li>
</ul>
<h2 id="洞察启示">洞察启示</h2>
<ol type="1">
<li>Triton是昇腾支持Torch.compile的必经之路；</li>
<li>Triton的支持对昇腾的三方库原生支持有很大的帮助；</li>
<li>借助triton-share的能力，可以进一步将ttir转换成linalg
IR，后者对昇腾亲和，减少适配工作量；</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/05/06/PyTorch%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/06/PyTorch%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/" class="post-title-link" itemprop="url">PyTorch源码编译</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-06 15:44:01" itemprop="dateCreated datePublished" datetime="2024-05-06T15:44:01+08:00">2024-05-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:44:21" itemprop="dateModified" datetime="2025-01-23T15:44:21+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>安装conda</strong></p>
<p>python最低要求 3.8版本及以上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/miniconda3</span><br><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh</span><br><span class="line">bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3</span><br><span class="line">rm -rf ~/miniconda3/miniconda.sh</span><br><span class="line"></span><br><span class="line">~/miniconda3/bin/conda init bash</span><br></pre></td></tr></table></figure>
<p><strong>下载源码</strong></p>
<p>从<a
target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch">pytorch社区下载源码</a>，并下载git的submodule。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/pytorch/pytorch</span><br><span class="line">cd pytorch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">if</span> you are updating an existing checkout</span></span><br><span class="line">git submodule sync</span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure>
<p>拉代码特别是拉三方的库的时候，会有访问不到的情况，可以重试，或者配置http代理以及ssh代理:</p>
<p>http代理：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export http_proxy=host:ip</span><br><span class="line">export https_proxy=host:ip</span><br></pre></td></tr></table></figure>
<p>ssh代理：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/config</span><br><span class="line"></span><br><span class="line">Host github.com</span><br><span class="line">    User git</span><br><span class="line">    ProxyCommand nc -v -x localhost:7890 %h %p</span><br></pre></td></tr></table></figure>
<p><strong>安装依赖</strong></p>
<p>pip最好配置下清华源，不然会很慢。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install --upgrade pip</span><br><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<p>安装pytorch依赖，gcc需要支持C++ 17，需要使用gcc 9.4.0及以上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install cmake ninja</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Run this <span class="built_in">command</span> from the PyTorch directory after cloning the <span class="built_in">source</span> code using the “Get the PyTorch Source“ section below</span></span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p><strong>编译</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CMAKE_PREFIX_PATH=$&#123;CONDA_PREFIX:-&quot;$(dirname $(which conda))/../&quot;&#125;</span><br><span class="line">python setup.py develop</span><br></pre></td></tr></table></figure>
<p>一些有用的编译时的环境变量：</p>
<ul>
<li><code>DEBUG=1</code> will enable debug builds (-g -O0)</li>
<li><code>REL_WITH_DEB_INFO=1</code> will enable debug symbols with
optimizations (-g -O3)</li>
<li><code>USE_DISTRIBUTED=0</code> will disable distributed (c10d, gloo,
mpi, etc.) build.</li>
<li><code>USE_MKLDNN=0</code> will disable using MKL-DNN.</li>
<li><code>USE_CUDA=0</code> will disable compiling CUDA (in case you are
developing on something not CUDA related), to save compile time.</li>
<li><code>BUILD_TEST=0</code> will disable building C++ test
binaries.</li>
<li><code>USE_FBGEMM=0</code> will disable using FBGEMM (quantized 8-bit
server operators).</li>
<li><code>USE_NNPACK=0</code> will disable compiling with NNPACK.</li>
<li><code>USE_QNNPACK=0</code> will disable QNNPACK build (quantized
8-bit operators).</li>
<li><code>USE_XNNPACK=0</code> will disable compiling with XNNPACK.</li>
<li><code>USE_FLASH_ATTENTION=0</code> and
<code>USE_MEM_EFF_ATTENTION=0</code> will disable compiling flash
attention and memory efficient kernels respectively</li>
</ul>
<p><strong>验证</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">a + b</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p>[1] <a
target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch?tab=readme-ov-file#from-source">PyTorch
installation from source.</a></p>
<p>[2] <a
target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md">PyTorch
contributing doc.</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/25/%E6%8E%A8%E7%90%86%E5%85%A8%E5%9C%BA%E6%99%AF%E6%B4%9E%E5%AF%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/25/%E6%8E%A8%E7%90%86%E5%85%A8%E5%9C%BA%E6%99%AF%E6%B4%9E%E5%AF%9F/" class="post-title-link" itemprop="url">推理全场景洞察</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-12-25 14:20:26 / 修改时间：14:25:58" itemprop="dateCreated datePublished" datetime="2023-12-25T14:20:26+08:00">2023-12-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="推理场景">推理场景</h2>
<ul>
<li><p><strong>计算机视觉：</strong>图像分类，目标检测，人脸识别，图像生成。</p></li>
<li><p><strong>自然语言处理（NLP)：</strong>翻译，语音识别，文本分类，内容审核。</p></li>
<li><p><strong>自动驾驶</strong></p></li>
<li><p><strong>生物学：</strong>医学影响分析，基因分析，蛋白质预测。</p></li>
<li><p><strong>金融：</strong>信用，风控。</p></li>
<li><p><strong>制造业：</strong>质量控制，成本控制。</p></li>
<li><p><strong>零售业：</strong>推荐广告系统，库存管理。</p></li>
<li><p><strong>能源：</strong>智能电网。</p></li>
<li><p><strong>大模型推理：</strong>LLM，多模态模型推理。</p></li>
</ul>
<h2 id="推理的一般流程">推理的一般流程</h2>
<p>以目标检测为例：</p>
<ol type="1">
<li><p><strong>预训练模型获取</strong>：从
Huggingface或者其他预训练模型平台上下载模型；</p></li>
<li><p><strong>模型转换：</strong>根据使用的推理框架，将下载的模型转换成对应的格式；</p></li>
<li><p><strong>数据预处理：</strong>视频解码，并从流中抓取图像，将图像进行裁剪，旋转，翻转，调整色彩空间，标准化等操作；</p></li>
<li><p><strong>模型推理：</strong>将处理完成的图像数据输入模型进行推理，得到推理结果；</p></li>
<li><p><strong>后处理：</strong>将模型推理结果进行处理，获取目标框，标签等信息；</p></li>
<li><p><strong>结果展示：</strong>将原始图像和推理结果进行融合，给检测的目标加上框和目标类型标签。</p></li>
</ol>
<h2 id="推理技术栈">推理技术栈</h2>
<p><strong>Nvidia</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116101903233.png"
alt="image-20231116101903233" />
<figcaption aria-hidden="true">image-20231116101903233</figcaption>
</figure>
<p><strong>昇腾</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116101849666.png"
alt="image-20231116101849666" />
<figcaption aria-hidden="true">image-20231116101849666</figcaption>
</figure>
<p>推理技术栈自下而上一般为：</p>
<ul>
<li>硬件：参与推理的硬件，例如CPU，GPU和NPU等，Nvidia目前常用的GPU为Volta-&gt;Turing-&gt;Ampere-&gt;Hopper等架构，Ascend为310系列和910系列芯片，
采用Davinci架构。</li>
<li>驱动和基础软件：此类软件包括加速卡的驱动程序，异构计算运行时（CUDA
RT, CANN
RT)，kernel开发调试工具等。除此之外，Ascend还提供了常用算子库。</li>
<li>推理引擎：推理引擎一般提供模型转换，模型优化，以及模型推理功能，并且提供运行的性能指标供性能分析和自动负载均衡。
大部分推理引擎都原生支持CUDA，对昇腾的原生支持较弱。</li>
<li>推理服务化：推理服务化工具一般提供restful和rpc接口，模型服务化部署。另外可以配合容器技术，调度技术和负载均衡等实现自动扩缩容，提高推理速度，提高资源利用率。基本上大多深度学习框架均提供了服务化部署能力，其中Triton支持多种后端，并提供了友好的接入接口。</li>
<li>行业应用：针对特定行业的预处理，pipeline或者相关的SDK用于简化行业应用的开发复杂度，甚至通过配置可以直接在行业内应用。</li>
<li>其他配套：其他配套例如预训练模型的仓库，模型调优工具，算法加速库以及边缘计算平台等。</li>
</ul>
<h2 id="推理流程中涉及的软件">推理流程中涉及的软件</h2>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116102732237.png"
alt="image-20231116102732237" />
<figcaption aria-hidden="true">image-20231116102732237</figcaption>
</figure>
<ul>
<li>模型预处理hub：hub不仅能够保存预训练模型，并且能够通过代码api的方式直接下载并加载模型。昇腾的Model
Zoo需要手动下载模型，Huggingface的预训练模型可以通过python
api下载和使用。</li>
<li>模型转换：ONNX是一种开发的模型格式，可以与常见的深度学习框架进行转换。除此之外，其他的框架一般提供有限的模型转换能力，大多是同一个框架内的不同模型格式的转换。</li>
<li>预处理：计算机视觉中，Nvidia自研了部分适配GPU的加速库，并且常见的OpenCV，torchvision也原生支持GPU。目前对昇腾的支持较弱，目前仅有mxBase和OpenCV实现了少量的常用接口。昇腾推理的CV预处理，还需要依赖CPU处理。自然语言处理库由于算法的特殊性，无法充分利用并行计算能力，上述库基本上都仅在CPU上运行。Nvidia提供了推荐系统大量数据并能处理能力的库NVTabular，昇腾在此方面可以使用mxRec提供的加速能力。</li>
<li>模型分析优化：推理框架一般分为优化和运行两部分，其中优化部分对传入的模型根据底层架构进行优化。并且在执行过程中可以通过组件监控模型的运行情况，以用来模型调优，或者提供弹性扩缩容能力。</li>
<li>推理框架：推理框架是推理业务的重点，不同的推理框架能力各有优劣：
<ul>
<li><strong>TensorRT (TensorRT by NVIDIA):</strong>
<ul>
<li>优点：
<ul>
<li>面向 NVIDIA GPU 的深度学习推理优化库。</li>
<li>针对高性能、低延迟的推理任务进行了优化。</li>
</ul></li>
<li>缺点：
<ul>
<li>仅适用于 NVIDIA GPU，不具备跨平台性。</li>
</ul></li>
</ul></li>
<li><strong>ONNX Runtime:</strong>
<ul>
<li>优点：
<ul>
<li>开放的模型表示格式，允许在不同框架之间共享和部署模型。</li>
<li>支持多种深度学习框架，如TensorFlow、PyTorch、Caffe等。</li>
</ul></li>
<li>缺点：
<ul>
<li>部分框架的支持可能不如原生框架的性能优越。</li>
</ul></li>
</ul></li>
<li><strong>OpenVINO</strong>
<ul>
<li>优点：
<ul>
<li>多平台支持，支持多深度学习框架。</li>
<li>针对各设备硬件进行优化，能在多种设备上高性能推理。</li>
</ul></li>
<li>缺点：
<ul>
<li>大量优化针对Intel硬件，对其他硬件厂商的优化有限。</li>
<li>开源版本功能限制，有些特性需要商业版支持。</li>
</ul></li>
</ul></li>
<li><strong>ncnn， TNN， MNN，ARMNN</strong>
<ul>
<li>优点：
<ul>
<li>面向移动端和嵌入式CPU或GPU，轻量级，弱依赖。</li>
<li>支持多种模型类型，有模型转换能力。</li>
</ul></li>
<li>缺点：
<ul>
<li>非嵌入式平台（ARMNN在非ARM平台）支持较弱。</li>
</ul></li>
</ul></li>
</ul></li>
<li>推理服务化：主流的深度学习框架基本上都提供了服务化能力，可以通过restful或者rpc接口进行模型推理。其中Triton设计更为灵活，能够方便的集成不同的后端，目前已经支持主流深度学习框架的推理服务。目前还没有支持昇腾推理框架，但是可以通过pytorch插件或者ONNX
runtime进行推理。</li>
<li>后处理：后处理将图例结果进行加工处理，并展示推理结果，所需软件与预处理大致相同。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116102541095.png"
alt="image-20231116102541095" />
<figcaption aria-hidden="true">image-20231116102541095</figcaption>
</figure>
<p>除了推理流程中的软件之外，还有行业应用，边缘计算，以及算法加速库。Nvidia和昇腾在这些领域均有涉及。</p>
<h2 id="总结">总结</h2>
<ol type="1">
<li>昇腾在主流深度学习框架，推理框架以及推理服务化软件中，原生支持较弱，大部分框架在设计之初均考虑GPU支持，目前已经支持昇腾的框架多为后期开发。如果框架在后端支持上设计不够友好，接入难度较高。</li>
<li>计算机视觉预处理能力与GPU能力差距较大，包括OpenCV，torchvision等开源CV软件均原生支持GPU，并且Nvidia还有自研的图像预处理库，昇腾仅支持少量高频使用的接口，并且性能还存在差距。</li>
<li>Nvidia和开源在框架，应用软件和加速库的使用上较为容易，社区活跃，文档完整规范，学习成本低。昇腾相关软件使用门槛较高，使用上相较而言较为繁琐。</li>
<li>建议在推理全流程中选择一个技术路线，做昇腾支持，在功能，性能上追平或超过友商，然后再考虑自研更适合昇腾场景的自研软件。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/12/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E9%AB%98%E9%A2%91%E6%8E%A5%E5%8F%A3%E6%96%B0%E5%A2%9E%E6%98%87%E8%85%BE%E6%94%AF%E6%8C%81-%E4%B8%8A%E7%BA%BF%E6%98%87%E8%85%BECI%E6%9C%8D%E5%8A%A1%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/12/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E9%AB%98%E9%A2%91%E6%8E%A5%E5%8F%A3%E6%96%B0%E5%A2%9E%E6%98%87%E8%85%BE%E6%94%AF%E6%8C%81-%E4%B8%8A%E7%BA%BF%E6%98%87%E8%85%BECI%E6%9C%8D%E5%8A%A1%E5%99%A8/" class="post-title-link" itemprop="url">OpenCV图像处理高频接口新增昇腾支持&上线昇腾CI服务器</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-12 15:42:41" itemprop="dateCreated datePublished" datetime="2023-12-12T15:42:41+08:00">2023-12-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:43:00" itemprop="dateModified" datetime="2025-01-23T15:43:00+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231205091902226.png"
alt="LOGO" />
<figcaption aria-hidden="true">LOGO</figcaption>
</figure>
<p>在当今数字化时代，图像处理不再是一个陌生的领域，而是推动科学、技术和创新的引擎。OpenCV（开源计算机视觉库）凭借其丰富、强大的功能和灵活性在图像处理领域占据引领者地位。为了进一步加速OpenCV图像处理，即将推出的<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv">OpenCV</a>
4.9.0版本提供了图像处理相关的高频使用接口的<a
target="_blank" rel="noopener" href="https://www.hiascend.com">昇腾（Ascend）</a>支持。这意味着在计算机视觉领域的数据预处理和后处理流程中，可以充分发挥昇腾软硬件的强大算力和计算效率，加速图像处理操作。</p>
<p>除此之外，2023年10月17日，OpenCV社区正式接纳昇腾+<a
target="_blank" rel="noopener" href="https://www.openeuler.org/">openEuler</a>作为持续集成（Continuous
Integration，CI）系统的操作系统之一，这意味着未来OpenCV的代码修改均会在昇腾+openEuler环境中进行自动化构建及测试，保障了OpenCV在openEuler上支持的稳定性。</p>
<h2 id="图像处理昇腾接口使用">图像处理昇腾接口使用</h2>
<p>本次提供的以昇腾为后端的图像处理接口封装在<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv_contrib">OpenCV扩展包(contrib)</a>的cannops模块中，包括图像矩阵的算术运算，通道拆分合并，图片裁剪，翻转，调整大小，转置等图像处理的Python和C++接口，处理精度与CPU后端的计算结果相同。</p>
<p>那么如何调用昇腾后端实现图像处理呢？除了对昇腾必要的初始化、去初始化之外，用户无需学习<a
target="_blank" rel="noopener" href="https://www.hiascend.com/zh/software/cann">CANN</a>
API，仅需要将原来的接口添加cann包名（C++接口为使用cann命名空间），即可使用昇腾算力：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># cann初始化</span></span><br><span class="line">cv2.cann.initAcl()</span><br><span class="line"><span class="comment"># 选择执行运算的设备编号</span></span><br><span class="line">cv2.cann.setDevice(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像处理（以图像旋转为例）</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;/path/to/image&#x27;</span>)</span><br><span class="line"><span class="comment"># 添加cann包名调用昇腾接口</span></span><br><span class="line">ret = cv2.cann.rotate(img, <span class="number">0</span>)</span><br><span class="line">cv2.imwrite(<span class="string">&#x27;/path/to/result&#x27;</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cann去初始化</span></span><br><span class="line">cv2.cann.finalizeAcl()</span><br></pre></td></tr></table></figure>
<center>
昇腾接口使用方法
</center>
<p>详细<a
target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d9/d56/tutorial_ascend_npu_image_processing.html">示例代码</a>和<a
target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d6/df4/group__cannops.html">接口列表</a>请参考OpenCV<a
target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/index.html">官方文档</a>。</p>
<h2 id="opencv官方支持昇腾ci">OpenCV官方支持昇腾CI</h2>
<p>OpenCV社区正式接纳昇腾+<a
target="_blank" rel="noopener" href="https://www.openeuler.org/">openEuler</a>作为持续集成（Continuous
Integration，CI）系统的操作系统之一，对昇腾和openEuler提供上游原生支持，给广大OpenCV及昇腾用户提供了稳定性保障。让我们跟随下面两个问题，了解昇腾CI是如何维护稳定性的。</p>
<p><strong>持续集成做了什么</strong>？</p>
<p>CI提供了一个高效、自动化的开发环境，在开发者提交代码后，会触发一系列自动构建及测试，以确保软件系统的稳定性和质量。持续集成系统不仅缩短了开发周期，也为开发者提供了更快速、可靠的反馈机制，从而推动了软件开发的创新和进步。</p>
<p><strong>OpenCV支持昇腾CI意味着什么？</strong></p>
<p>未来OpenCV社区的代码提交将会在昇腾CI环境下自动化构建和测试，有助于确保OpenCV在昇腾上的稳定性和性能。同时为后续OpenCV的昇腾相关贡献代码的提交提供了稳定可靠的测试平台，有助于推动OpenCV+昇腾的蓬勃发展。感谢OpenCV和华为计算团队的共同努力。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231124150817615.png"
alt="OpenCV社区CI的昇腾支持" />
<figcaption aria-hidden="true">OpenCV社区CI的昇腾支持</figcaption>
</figure>
<center>
OpenCV社区CI的昇腾支持
</center>
<h2 id="opencv简介">OpenCV简介</h2>
<p>OpenCV是一个开源的跨平台计算机视觉库，提供了图像处理，机器学习，视频分析等功能，支持多种开发语言，多平台和多类型后端。
其中图像处理模块提供了矩阵的算术和逻辑运算，图像处理，视频编解码等，被广泛运用于计算机视觉领域的数据预处理和后处理。</p>
<p>除了图像处理模块，OpenCV
DNN模块已经在2022年12月支持了昇腾作为推理后端，支持读取包括ONNX，TensorFLow，Caffeine，Darknet在内的多种主流格式的深度网络模型，并支持在多平台，多设备中运行，目前已经达到了主流神经网络推理框架的性能。在昇腾算力的加持下，推理性能进一步提升，<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/nqjUrrxofcjjg82ne144SQ">3毫秒即可完成ResNet50推理</a>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2261701741068_.pic.jpg"
alt="OpenCV架构示意图" />
<figcaption aria-hidden="true">OpenCV架构示意图</figcaption>
</figure>
<center>
OpenCV架构示意图
</center>
<h2 id="cann简介">CANN简介</h2>
<p>CANN是华为推出的异构计算架构，是上层应用调用底层昇腾处理器的桥梁，在本次OpenCV昇腾支持中是至关重要的一层。同时，昇腾+CANN支持昇思MindSpore，OpenCV
DNN、PyTorch、TensorFlow、飞桨、ONNX等多种深度学习框架，以极致性能、极简开发、开放生态为目标，助力昇腾构建全场景人工智能平台。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231122144232937.png"
alt="CANN AI异构计算框架" />
<figcaption aria-hidden="true">CANN AI异构计算框架</figcaption>
</figure>
<center>
CANN AI异构计算框架
</center>
<h2 id="结语">结语</h2>
<p>未来OpenCV会进一步完善能使用昇腾后端的接口，并持续优化接口性能，进一步发挥昇腾算力性能，提升用户体验。</p>
<p>目前，用户需要自行编译OpenCV最新代码才能体验到接口的昇腾支持，编译方法可以参考OpenCV
wiki中提供的<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/Huawei-CANN-Backend">详细指导</a>，由于cannops模块在OpenCV扩展包中，编译还需要下载扩展包源码以及配置扩展包的保存路径:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/opencv/opencv_contrib.git</span><br><span class="line">$ <span class="built_in">cd</span> &lt;opencv_build_directory&gt;</span><br><span class="line">$ cmake -DOPENCV_EXTRA_MODULES_PATH=&lt;opencv_contrib&gt;/modules &lt;opencv_source_directory&gt;</span><br><span class="line">$ make -j5</span><br></pre></td></tr></table></figure>
<p>如果大家在使用上遇到任何问题，欢迎反馈到OpenCV扩展包GitHub仓库的<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv_contrib/issues">Issues</a>页面，我们会及时为大家解决问题。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/12/AscendC-vs-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/12/AscendC-vs-CUDA/" class="post-title-link" itemprop="url">AscendC vs CUDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-10-12 09:35:14 / 修改时间：09:38:04" itemprop="dateCreated datePublished" datetime="2023-10-12T09:35:14+08:00">2023-10-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="什么是ascendccuda编程">什么是AscendC/CUDA编程</h2>
<blockquote>
<p>面向算子开发场景的编程语言Ascend
C，原生支持C和C++标准规范，最大化匹配用户开发习惯；通过多层接口抽象、自动并行计算、孪生调试等关键技术，极大提高算子开发效率，助力AI开发者低成本完成算子开发和模型调优部署。</p>
</blockquote>
<blockquote>
<p><strong>CUDA</strong>（<strong>C</strong>ompute
<strong>U</strong>nified <strong>D</strong>evice
<strong>A</strong>rchitecture，<strong>统一计算架构</strong>[<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/CUDA#cite_note-1">1]</a>）是由英伟达<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/NVIDIA">NVIDIA</a>所推出的一种<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/軟體">软</a><a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/计算机硬件">硬件</a>集成技术，是该公司对于<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GPGPU">GPGPU</a>的正式名称。透过这个技术，用户可利用NVIDIA的<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GPU">GPU</a>进行<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/图像处理">图像处理</a>之外的运算，亦是首次可以利用GPU作为C-编译器的开发环境。</p>
</blockquote>
<p><strong>一句话概括：AscendC/CUDA就是使用昇腾设备/GPU设备的编程接口。</strong></p>
<h2 id="与我们熟悉的编程有什么区别">与我们熟悉的编程有什么区别</h2>
<p><strong>内存</strong></p>
<p>Host编程仅考虑主存，所有的内存操作对象均为主存，不需要考虑CPU缓存，寄存器等，这些对程序开发完全透明。</p>
<p>Device编程需要了解每个运行单元能访问的内存类型，可以理解要手动管理一级二级缓存，例如，AscendC变成框架下，内存的类型有：</p>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 89%" />
</colgroup>
<thead>
<tr>
<th>枚举值</th>
<th>具体含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>GM</td>
<td>Global Memory，对应AI Core的外部存储。</td>
</tr>
<tr>
<td>VECIN</td>
<td>用于矢量计算，搬入数据的存放位置，在数据搬入Vector计算单元时使用此位置</td>
</tr>
<tr>
<td>VECOUT</td>
<td>用于矢量计算，搬出数据的存放位置，在将Vector计算单元结果搬出时使用此位置</td>
</tr>
<tr>
<td>VECCALC</td>
<td>用于矢量计算/矩阵计算，在计算需要临时变量时使用此位置</td>
</tr>
<tr>
<td>A1</td>
<td>用于矩阵计算，存放整块A矩阵，可类比CPU多级缓存中的二级缓存</td>
</tr>
<tr>
<td>B1</td>
<td>用于矩阵计算，存放整块B矩阵，可类比CPU多级缓存中的二级缓存</td>
</tr>
<tr>
<td>A2</td>
<td>用于矩阵计算，存放切分后的小块A矩阵，可类比CPU多级缓存中的一级缓存</td>
</tr>
<tr>
<td>B2</td>
<td>用于矩阵计算，存放切分后的小块B矩阵，可类比CPU多级缓存中的一级缓存</td>
</tr>
<tr>
<td>CO1</td>
<td>用于矩阵计算，存放小块结果C矩阵，可理解为Cube Out</td>
</tr>
<tr>
<td>CO2</td>
<td>用于矩阵计算，存放整块结果C矩阵，可理解为Cube Out</td>
</tr>
</tbody>
</table>
<p>不同的处理单元，不同的处理步骤访问的内存是不同的，需要开发者自行处理。</p>
<p><strong>编程模型</strong></p>
<p>Host编程一般为串行的，如果想启用并行处理需要手动开启多线程，或者SIMD(Single
Instruction, Multiple Data)。</p>
<p>Device编程一般为并行，SPMD(Single-Program
Multiple-Data)。在设备上启动多线程，共同处理一份数据。Device编程代码分为两个部分，Host侧执行的一般代码和在设备上执行的核函数(kernel
function)。</p>
<p>AscendC还需要注意的是流水线编程范式，流水线编程主要是为了加速数据拷贝，Device处理以及数据拷回的流程。因为DMA搬运单元，各个计算单元是并行工作的，使用流水线能够提高设备单元的使用率。</p>
<h2 id="device-的内部结构抽象">Device 的内部结构抽象</h2>
<p><strong>Ascend AI Core 内部抽象结构</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2021051922433521.png"
alt="达芬奇架构" />
<figcaption aria-hidden="true">达芬奇架构</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001706909793.png"
alt="AI Core抽象结构" />
<figcaption aria-hidden="true">AI Core抽象结构</figcaption>
</figure>
<p><strong>CUDA核心内部抽象结构</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/70.png"
alt="CUDA核心结构" />
<figcaption aria-hidden="true">CUDA核心结构</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/70-20231010170118685.png"
alt="CUDA核心结构" />
<figcaption aria-hidden="true">CUDA核心结构</figcaption>
</figure>
<p><strong>AI Core和Stream Multiprocessor的最主要区别是：</strong></p>
<ul>
<li><p><strong>AI
Core中是专用处理单元，包括Vector和Cube，分别用户向量和矩阵运算，能用向量和矩阵运算的操作效率会很高。</strong></p></li>
<li><p><strong>Stream
Multiprocessor基本上都是大量的int32核心，float32核心或者双精度核心，由于数量众多，所以并行能力更强。</strong></p></li>
</ul>
<h2 id="ascendc编程和cuda编程对比">AscendC编程和CUDA编程对比</h2>
<p><strong>AscendC</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;acl/acl.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tikicpulib.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel_operator.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;data_loader.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> AscendC;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __aicore__</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __aicore__ [aicore]</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BUFFER_NUM = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BLOCK_DIM = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*****************************Copy scalar to ubuf*****************************/</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">FlipTilingData</span> &#123;</span><br><span class="line">  <span class="type">uint32_t</span> height;</span><br><span class="line">  <span class="type">uint32_t</span> width;</span><br><span class="line">  <span class="type">uint32_t</span> channel;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> __aicore__ <span class="type">int32_t</span> <span class="title">align32</span><span class="params">(<span class="type">int32_t</span> n)</span> </span>&#123; <span class="keyword">return</span> ((n + <span class="number">31</span>) &amp; ~<span class="number">31</span>); &#125;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> __aicore__ <span class="type">int32_t</span> <span class="title">AlignDiv32</span><span class="params">(<span class="type">int32_t</span> n)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">align32</span>(n) / <span class="number">32</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  __ubuf__ tilingStruct* tilingDataPointer =                                \</span></span><br><span class="line"><span class="meta">      reinterpret_cast<span class="string">&lt;__ubuf__ tilingStruct*&gt;</span>(                             \</span></span><br><span class="line"><span class="meta">          (__ubuf__ uint8_t*)(tilingPointer));</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INIT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer);</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INIT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  __ubuf__ uint8_t* tilingUbPointer = (__ubuf__ uint8_t*)get_imm(0);     \</span></span><br><span class="line"><span class="meta">  copy_gm_to_ubuf(((__ubuf__ uint8_t*)(tilingUbPointer)),                \</span></span><br><span class="line"><span class="meta">                  ((__gm__ uint8_t*)(tilingPointer)), 0, 1,              \</span></span><br><span class="line"><span class="meta">                  AlignDiv32(sizeof(tilingStruct)), 0, 0);               \</span></span><br><span class="line"><span class="meta">  CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingUbPointer); \</span></span><br><span class="line"><span class="meta">  pipe_barrier(PIPE_ALL);</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GET_TILING_DATA(tilingData, tilingPointer) \</span></span><br><span class="line"><span class="meta">  INIT_TILING_DATA(FlipTilingData, tilingData, tilingPointer);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_ACL(x)                                                    \</span></span><br><span class="line"><span class="meta">  do &#123;                                                                  \</span></span><br><span class="line"><span class="meta">    aclError __ret = x;                                                 \</span></span><br><span class="line"><span class="meta">    <span class="keyword">if</span> (__ret != ACL_ERROR_NONE) &#123;                                      \</span></span><br><span class="line"><span class="meta">      std::cerr &lt;&lt; __FILE__ &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; __LINE__ &lt;&lt; <span class="string">&quot; aclError:&quot;</span> &lt;&lt; __ret \</span></span><br><span class="line"><span class="meta">                &lt;&lt; std::endl;                                           \</span></span><br><span class="line"><span class="meta">    &#125;                                                                   \</span></span><br><span class="line"><span class="meta">  &#125; while (0);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*******************************Kernel function*******************************/</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KernelFlip</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="title">KernelFlip</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Init</span><span class="params">(GM_ADDR input, GM_ADDR output, <span class="type">uint32_t</span> _height,</span></span></span><br><span class="line"><span class="params"><span class="function">                              <span class="type">uint32_t</span> _width, <span class="type">uint32_t</span> _channel)</span> </span>&#123;</span><br><span class="line">    <span class="type">uint32_t</span> blockNum = <span class="built_in">GetBlockNum</span>();</span><br><span class="line">    <span class="type">uint32_t</span> blockIdx = <span class="built_in">GetBlockIdx</span>();</span><br><span class="line"></span><br><span class="line">    rowLength = _height / blockNum;</span><br><span class="line">    startRowIdx = blockIdx * rowLength;</span><br><span class="line">    <span class="keyword">if</span> (startRowIdx + rowLength &gt; _height) &#123;</span><br><span class="line">      rowLength = _height - startRowIdx;</span><br><span class="line">    &#125;</span><br><span class="line">    width = _width;</span><br><span class="line">    height = _height;</span><br><span class="line">    channel = _channel;</span><br><span class="line">    rowSize = width * channel;</span><br><span class="line">    <span class="type">uint32_t</span> bufferSize = <span class="built_in">align32</span>(rowSize * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>));</span><br><span class="line"></span><br><span class="line">    inputGM.<span class="built_in">SetGlobalBuffer</span>((__gm__ <span class="type">uint8_t</span>*)input + startRowIdx * rowSize,</span><br><span class="line">                            rowLength * rowSize);</span><br><span class="line">    outputGM.<span class="built_in">SetGlobalBuffer</span>((__gm__ <span class="type">uint8_t</span>*)output + startRowIdx * rowSize,</span><br><span class="line">                             rowLength * rowSize);</span><br><span class="line">    pipe.<span class="built_in">InitBuffer</span>(inQueue, BUFFER_NUM, bufferSize);</span><br><span class="line">    pipe.<span class="built_in">InitBuffer</span>(outQueue, BUFFER_NUM, bufferSize);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Process</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int32_t</span> i = <span class="number">0</span>; i &lt; rowLength; i++) &#123;</span><br><span class="line">      <span class="built_in">CopyIn</span>(i);</span><br><span class="line">      <span class="built_in">Compute</span>(i);</span><br><span class="line">      <span class="built_in">CopyOut</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">CopyIn</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; local = inQueue.<span class="built_in">AllocTensor</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="built_in">DataCopy</span>(local, inputGM[loop * rowSize], rowSize);</span><br><span class="line">    inQueue.<span class="built_in">EnQue</span>(local);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Compute</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; inputLocal = inQueue.<span class="built_in">DeQue</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; outputLocal = outQueue.<span class="built_in">AllocTensor</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int32_t</span> i = <span class="number">0</span>; i &lt; width; i++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int32_t</span> c = <span class="number">0</span>; c &lt; channel; c++) &#123;</span><br><span class="line">        outputLocal.<span class="built_in">SetValue</span>(</span><br><span class="line">            i * channel + c,</span><br><span class="line">            inputLocal.<span class="built_in">GetValue</span>((width - i - <span class="number">1</span>) * channel + c));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    outQueue.<span class="built_in">EnQue</span>&lt;<span class="type">uint8_t</span>&gt;(outputLocal);</span><br><span class="line">    inQueue.<span class="built_in">FreeTensor</span>(inputLocal);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">CopyOut</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; local = outQueue.<span class="built_in">DeQue</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="built_in">DataCopy</span>(outputGM[loop * rowSize], local, rowSize);</span><br><span class="line">    outQueue.<span class="built_in">FreeTensor</span>(local);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  TPipe pipe;</span><br><span class="line">  TQue&lt;QuePosition::VECIN, BUFFER_NUM&gt; inQueue;</span><br><span class="line">  TQue&lt;QuePosition::VECOUT, BUFFER_NUM&gt; outQueue;</span><br><span class="line">  GlobalTensor&lt;<span class="type">uint8_t</span>&gt; inputGM, outputGM;</span><br><span class="line">  <span class="type">uint32_t</span> startRowIdx, rowLength, rowSize, height, width, channel;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*******************************kernel interface******************************/</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> <span class="function">__global__ __aicore__ <span class="type">void</span> <span class="title">flip</span><span class="params">(GM_ADDR input, GM_ADDR output,</span></span></span><br><span class="line"><span class="params"><span class="function">                                           GM_ADDR tiling)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">GET_TILING_DATA</span>(tilingData, tiling);</span><br><span class="line">  KernelFlip op;</span><br><span class="line">  op.<span class="built_in">Init</span>(input, output, tilingData-&gt;height, tilingData-&gt;width,</span><br><span class="line">          tilingData-&gt;channel);</span><br><span class="line">  op.<span class="built_in">Process</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">flip_do</span><span class="params">(<span class="type">uint32_t</span> blockDim, <span class="type">void</span>* l2ctrl, <span class="type">void</span>* stream, <span class="type">uint8_t</span>* input,</span></span></span><br><span class="line"><span class="params"><span class="function">             <span class="type">uint8_t</span>* output, <span class="type">uint8_t</span>* tiling)</span> </span>&#123;</span><br><span class="line">  flip&lt;&lt;&lt;blockDim, l2ctrl, stream&gt;&gt;&gt;(input, output, tiling);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************************caller************************************/</span></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">main</span><span class="params">(<span class="type">int32_t</span> argc, <span class="type">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">&quot; path/to/datafile&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> blockDim = BLOCK_DIM;</span><br><span class="line">  <span class="type">uint32_t</span> height, width, channel;</span><br><span class="line">  <span class="type">uint8_t</span>* data = <span class="built_in">readFile</span>(argv[<span class="number">1</span>], height, width, channel);</span><br><span class="line">  <span class="type">const</span> <span class="type">char</span>* resultFile = std::<span class="built_in">string</span>(argv[<span class="number">1</span>]).<span class="built_in">append</span>(<span class="string">&quot;.ret&quot;</span>).<span class="built_in">c_str</span>();</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> dataSize = width * height * channel * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>);</span><br><span class="line">  <span class="type">size_t</span> inputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> outputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> tilingSize = <span class="built_in">sizeof</span>(FlipTilingData);</span><br><span class="line"></span><br><span class="line">  <span class="type">uint8_t</span> *inputHost, *outputHost, *tilingHost;</span><br><span class="line">  <span class="type">uint32_t</span> shape[]&#123;height, width, channel&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line">  inputHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(inputByteSize);</span><br><span class="line">  outputHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(outputByteSize);</span><br><span class="line">  tilingHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(tilingHost, shape, tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(inputHost, data, dataSize);</span><br><span class="line"></span><br><span class="line">  AscendC::<span class="built_in">SetKernelMode</span>(KernelMode::AIV_MODE);</span><br><span class="line">  <span class="built_in">ICPU_RUN_KF</span>(flip, blockDim, inputHost, outputHost, tilingHost);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, outputHost);</span><br><span class="line"></span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)inputHost);</span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)outputHost);</span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)tilingHost);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclInit</span>(<span class="literal">nullptr</span>));</span><br><span class="line">  aclrtContext context;</span><br><span class="line">  <span class="type">int32_t</span> deviceId = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtSetDevice</span>(deviceId));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtCreateContext</span>(&amp;context, deviceId));</span><br><span class="line">  aclrtStream stream = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtCreateStream</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">  <span class="type">uint8_t</span> *inputDevice, *outputDevice, *tilingDevice;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;tilingHost), tilingSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;inputHost), inputByteSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;outputHost), outputByteSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;inputDevice, inputByteSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;outputDevice, outputByteSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;tilingDevice, tilingSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">memcpy</span>(tilingHost, shape, tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(inputHost, data, dataSize);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(inputDevice, inputByteSize, inputHost, inputByteSize,</span><br><span class="line">                        ACL_MEMCPY_HOST_TO_DEVICE));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(tilingDevice, tilingSize, tilingHost, tilingSize,</span><br><span class="line">                        ACL_MEMCPY_HOST_TO_DEVICE));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">flip_do</span>(blockDim, <span class="literal">nullptr</span>, stream, inputDevice, outputDevice, tilingDevice);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtSynchronizeStream</span>(stream));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(outputHost, outputByteSize, outputDevice,</span><br><span class="line">                        outputByteSize, ACL_MEMCPY_DEVICE_TO_HOST));</span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, outputHost);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(inputDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(outputDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(tilingDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(inputHost));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(outputHost));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(tilingHost));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtDestroyStream</span>(stream));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtDestroyContext</span>(context));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtResetDevice</span>(deviceId));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclFinalize</span>());</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="built_in">free</span>(data);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001706789009.png"
alt="流水线示例" />
<figcaption aria-hidden="true">流水线示例</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001658510374.png"
alt="数据切分示例" />
<figcaption aria-hidden="true">数据切分示例</figcaption>
</figure>
<p><strong>CUDA</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;data_loader.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">flip</span><span class="params">(<span class="type">uint8_t</span>* input, <span class="type">uint8_t</span>* output, <span class="type">uint32_t</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="type">uint32_t</span> width, <span class="type">uint32_t</span> channel)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> rowIdx = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">  <span class="type">int</span> rowSize = width * channel;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> row = rowIdx; row &lt; height; row += stride) &#123;</span><br><span class="line">    <span class="type">int</span> startOffset = row * rowSize;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> idx = <span class="number">0</span>; idx &lt; width; idx++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; channel; c++) &#123;</span><br><span class="line">        output[startOffset + idx * channel + c] =</span><br><span class="line">            input[startOffset + (width - idx - <span class="number">1</span>) * channel + c];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int32_t</span> argc, <span class="type">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">&quot; path/to/datafile&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">uint32_t</span> height, width, channel;</span><br><span class="line">  <span class="type">char</span> fileName[<span class="number">256</span>], resultFile[<span class="number">256</span>];</span><br><span class="line">  <span class="built_in">memset</span>(fileName, <span class="number">0</span>, <span class="number">256</span>);</span><br><span class="line">  <span class="built_in">memset</span>(resultFile, <span class="number">0</span>, <span class="number">256</span>);</span><br><span class="line">  <span class="built_in">strcpy</span>(fileName, argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="built_in">strcat</span>(resultFile, fileName);</span><br><span class="line">  <span class="built_in">strcat</span>(resultFile, <span class="string">&quot;.ret&quot;</span>);</span><br><span class="line">  <span class="type">uint8_t</span>* data = <span class="built_in">readFile</span>(fileName, height, width, channel);</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> dataSize = width * height * channel * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>);</span><br><span class="line">  <span class="type">size_t</span> inputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> outputByteSize = dataSize;</span><br><span class="line">  <span class="type">uint8_t</span> *input, *output;</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;input, inputByteSize);</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;output, outputByteSize);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">memcpy</span>(input, data, inputByteSize);</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">gridSize</span><span class="params">((height + blockSize.x - <span class="number">1</span>) / blockSize.x)</span></span>;</span><br><span class="line"></span><br><span class="line">  flip&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(input, output, height, width, channel);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, output);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(input);</span><br><span class="line">  <span class="built_in">cudaFree</span>(output);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/20/Compile-OpenCV-with-CANN-backend/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/20/Compile-OpenCV-with-CANN-backend/" class="post-title-link" itemprop="url">Compile OpenCV with CANN backend</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-20 15:35:05" itemprop="dateCreated datePublished" datetime="2023-09-20T15:35:05+08:00">2023-09-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:36:26" itemprop="dateModified" datetime="2025-01-23T15:36:26+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="prepare-environment">Prepare environment</h2>
<p>Simply you can use docker container, <a
target="_blank" rel="noopener" href="https://github.com/hipudding/opencv_contrib/blob/npu_support/modules/cannops/Dockerfile">Dockerfile
is here</a>.</p>
<p>Or install manually:</p>
<p><strong>driver</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Download driver here. https://www.hiascend.com/hardware/firmware-drivers</span><br><span class="line">chmod +x A300-3010-npu-driver_6.0.0_linux-x86_64.run</span><br><span class="line">./A300-3010-npu-driver_6.0.0_linux-x86_64.run --install</span><br></pre></td></tr></table></figure>
<p><strong>toolkit</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Download ascend_toolkit here. https://www.hiascend.com/en/software/cann/community</span><br><span class="line">chmod +x Ascend-cann-toolkit_7.0.RC1.alpha002_linux-x86_64.run</span><br><span class="line">./Ascend-cann-toolkit_7.0.RC1.alpha002_linux-x86_64.run --install</span><br></pre></td></tr></table></figure>
<h2 id="clone-repository">Clone repository</h2>
<p>Clone opencv repositories, In order to facilitate code reading, make
sure opencv_contrib is inside opencv dir.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:opencv/opencv.git</span><br><span class="line">cd opencv</span><br><span class="line">git clone git@github.com:opencv/opencv_contrib.git</span><br></pre></td></tr></table></figure>
<h2 id="build">Build</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cmake </span><br><span class="line">-DCMAKE_INSTALL_PREFIX=/home/hua/code/opencv/build/install </span><br><span class="line">-DWITH_DEBUG=1 </span><br><span class="line">-DBUILD_WITH_DEBUG_INFO=1 </span><br><span class="line">-DOPENCV_EXTRA_MODULES_PATH=/home/hua/code/opencv/opencv_contrib/modules </span><br><span class="line">-DWITH_CUDA=0 </span><br><span class="line">-DWITH_CANN=1 </span><br><span class="line">-DPYTHON3_EXECUTABLE=/home/hua/anaconda3/envs/py39/bin/python </span><br><span class="line">-DPYTHON_LIBRARY=/home/hua/anaconda3/envs/py39 </span><br><span class="line">-DPYTHON_INCLUDE_DIR=/home/hua/anaconda3/envs/py39/include/python3.9 </span><br><span class="line">-DBUILD_opencv_wechat_qrcode=OFF </span><br><span class="line">-DBUILD_opencv_xfeatures2d=OFF </span><br><span class="line">-DBUILD_opencv_face=OFF </span><br><span class="line">-DBUILD_opencv_dnn=OFF </span><br><span class="line">-DBUILD_opencv_features2d=OFF </span><br><span class="line">-DWITH_CAROTENE=OFF </span><br><span class="line">-DWITH_IPP=OFF </span><br><span class="line">-DBUILD_DOCS=ON</span><br><span class="line"></span><br><span class="line">make -j</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>Make sure CANN and python-dev is detected:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[cmake] --   CANN:                          YES</span><br><span class="line">[cmake] --     Include path                 /home/hua/Ascend/ascend-toolkit/latest/include /home/hua/Ascend/ascend-toolkit/latest/opp</span><br><span class="line">[cmake] --     Link libraries:              /home/hua/Ascend/ascend-toolkit/latest/acllib/lib64/libascendcl.so /home/hua/Ascend/ascend-toolkit/latest/lib64/libacl_op_compiler.so /home/hua/Ascend/ascend-toolkit/latest/opp/built-in/op_proto/lib/linux/x86_64/libopsproto.so /home/hua/Ascend/ascend-toolkit/latest/compiler/lib64/libgraph.so /home/hua/Ascend/ascend-toolkit/latest/compiler/lib64/libge_compiler.so /home/hua/Ascend/ascend-toolkit/latest/compiler/lib64/libgraph_base.so</span><br><span class="line">[cmake] -- </span><br><span class="line">[cmake] --   Python 3:</span><br><span class="line">[cmake] --     Interpreter:                 /home/hua/anaconda3/envs/py39/bin/python (ver 3.9.17)</span><br><span class="line">[cmake] --     Libraries:                   /home/hua/anaconda3/envs/py39/lib/libpython3.9.so (ver 3.9.17)</span><br><span class="line">[cmake] --     numpy:                       /home/hua/anaconda3/envs/py39/lib/python3.9/site-packages/numpy/core/include (ver 1.25.2)</span><br><span class="line">[cmake] --     install path:                lib/python3.9/site-packages/cv2/python-3.9</span><br></pre></td></tr></table></figure>
<h2 id="run-test">Run Test</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/opencv_test_cannops</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/31/OpenCV%E7%A4%BE%E5%8C%BA%E6%B4%9E%E5%AF%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/31/OpenCV%E7%A4%BE%E5%8C%BA%E6%B4%9E%E5%AF%9F/" class="post-title-link" itemprop="url">OpenCV社区洞察</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-08-31 15:37:43" itemprop="dateCreated datePublished" datetime="2023-08-31T15:37:43+08:00">2023-08-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:48:07" itemprop="dateModified" datetime="2025-01-23T15:48:07+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="opencv背景">OpenCV背景</h2>
<p>OpenCV是1998年在Intel公司内的CVL(计算机视觉库)项目，由Gary
Bradski发起，并由<a target="_blank" rel="noopener" href="https://github.com/vpisarev">Vadim
Pisarevsky</a>担任技术主管，于1999年开源，2000年首次公开发布。2008年OpenCV的核心成员加入<a
target="_blank" rel="noopener" href="http://www.willowgarage.com/">Willow Garage</a>和<a
target="_blank" rel="noopener" href="https://itseez3d.com/">Itseez</a>公司继续开发。Itseez公司在2016年被Intel收购，核心开发团队重回Intel。</p>
<p>目前主要由Intel公司赞助OpenCV核心开发团队，并且很多OpenCV的开发者是Intel的雇员。这是一个由Intel公司主导，OpenCV.org非盈利基金会运营的开源项目。2019年以来，核心开发团队由Intel，OpenCV中国团队和<a
target="_blank" rel="noopener" href="https://www.xperience.ai/">xperience.ai</a>公司组成。</p>
<h2 id="版本发布策略">版本发布策略</h2>
<p>目前每6个月发布一次版本，社区没有明确说明每个版本的生命周期。</p>
<p><strong>4.x版本</strong></p>
<table>
<thead>
<tr>
<th>版本号</th>
<th>发布时间</th>
<th>时间间隔</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.8.0</td>
<td>2023.6.29</td>
<td>\</td>
</tr>
<tr>
<td>4.7.0</td>
<td>2022.12.28</td>
<td>6个月</td>
</tr>
<tr>
<td>4.6.0</td>
<td>2022.6.7</td>
<td>6个月</td>
</tr>
<tr>
<td>4.5.5</td>
<td>2021.12.25</td>
<td>6个月</td>
</tr>
<tr>
<td>4.5.4</td>
<td>2021.10.10</td>
<td>2个月</td>
</tr>
<tr>
<td>4.5.3</td>
<td>2021.7.6</td>
<td>3个月</td>
</tr>
<tr>
<td>4.5.2</td>
<td>2021.4.3</td>
<td>3个月</td>
</tr>
<tr>
<td>4.5.1</td>
<td>2020.12.22</td>
<td>4个月</td>
</tr>
<tr>
<td>4.5.0</td>
<td>2020.10.12</td>
<td>2个月</td>
</tr>
</tbody>
</table>
<p>4.0.0版本发布于2018.11.18。</p>
<p><strong>3.x版本</strong></p>
<table>
<thead>
<tr>
<th>版本号</th>
<th>发布时间</th>
<th>时间间隔</th>
</tr>
</thead>
<tbody>
<tr>
<td>3.4.20</td>
<td>2023.6.27（仅打Tag）</td>
<td>\</td>
</tr>
<tr>
<td>3.4.19</td>
<td>2022.12.27（仅打Tag）</td>
<td>6个月</td>
</tr>
<tr>
<td>3.4.18</td>
<td>2022.6.5（仅打Tag）</td>
<td>6个月</td>
</tr>
<tr>
<td>3.4.17</td>
<td>2021.12.25（仅打Tag）</td>
<td>6个月</td>
</tr>
<tr>
<td>3.4.16</td>
<td>2021.10.10</td>
<td>2个月</td>
</tr>
<tr>
<td>3.4.15</td>
<td>2021.7.6</td>
<td>3个月</td>
</tr>
<tr>
<td>3.4.14</td>
<td>2021.4.2</td>
<td>3个月</td>
</tr>
<tr>
<td>3.4.13</td>
<td>2020.12.22</td>
<td>4个月</td>
</tr>
<tr>
<td>3.4.12</td>
<td>2020.10.12</td>
<td>2个月</td>
</tr>
</tbody>
</table>
<p>3.x版本最新的一个release版本3.4.16发布时间为2021.10，最后一个tag版本3.4.20发布时间为2021.6。</p>
<p>3.0发布于2015.6.24。</p>
<p><strong>2.x版本</strong></p>
<p>2.x版本的最新一个release版本是2.4.13.6，发布于2018.2.26，从发布时间上看，已经不再维护。</p>
<p><strong>社区没有明确说明每个大版本的支持周期</strong></p>
<h2 id="opencv基金会">OpenCV基金会</h2>
<h3 id="领导团队">领导团队</h3>
<ul>
<li><p>Gary Bradski (Itseez, Intel）</p></li>
<li><p>Anna Petrovicheva (Intel)</p></li>
<li><p>Vladimir Dudnik (Intel)</p></li>
<li><p>Stefano Fabri (Deeper)</p></li>
<li><p>Tatiana Khanova (Xperience.ai)</p></li>
<li><p>Satya Mallick (OpenCV CEO)</p></li>
<li><p>Vadim Pisarevsky (Huawei)</p></li>
<li><p>Vincent Rabaud (Google)</p></li>
<li><p>Edgar Riba (farm-ng)</p></li>
<li><p>Aleksandr Voron (N/A)</p></li>
</ul>
<p>领导团队（leadership meeting)，每周三 8:00 am, 太平洋时间，通过<a
target="_blank" rel="noopener" href="https://hangouts.google.com/">Hangouts</a>沟通，会议纪要记录在<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/Meeting_notes">github的wiki上</a>。</p>
<h3 id="开发团队和贡献者社区">开发团队和贡献者社区</h3>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/opencv/">github</a>社区活跃开发者</strong></p>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 31%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>姓名</th>
<th>社区职位</th>
<th>就职公司</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/asmorkalov">Alexander Smorkalov</a></td>
<td>活跃commitor，合入PR数量众多</td>
<td>Xperience.AI</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/vpisarev">Vadim Pisarevsky</a></td>
<td>OpenCV技术负责人</td>
<td>华为</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/alalek">Alexander Alekhin</a></td>
<td>活跃开发者，reviewer，commitor</td>
<td>Intel</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ilya-lavrenov">Ilya Lavrenov</a></td>
<td>活跃开发者</td>
<td>Itseez CTO</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/dkurt">Dmitry Kurtaev</a></td>
<td>活跃开发者，reviewer，commitor</td>
<td>YADRO</td>
</tr>
<tr>
<td>... ...</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>小粒度特性和bug fix可以用<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/issues">issue</a>跟踪，大粒度特性需要有<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/Evolution-Proposals">进化提案</a>跟踪。贡献社区需要参考OpenCV社区的<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/How_to_contribute">贡献指导</a>，所提交的代码需要符合社区编码<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/Coding_Style_Guide">规范</a>。</p>
<p><strong>领域主席</strong></p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 27%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr>
<th>领域</th>
<th>姓名</th>
<th>单位</th>
</tr>
</thead>
<tbody>
<tr>
<td>RISC-V</td>
<td>Mingjie Xing</td>
<td>中国科学院软件研究所</td>
</tr>
<tr>
<td>人脸识别与分析</td>
<td>Weihong Deng</td>
<td>北京邮电大学</td>
</tr>
<tr>
<td>人体检测</td>
<td>Andrea Pennisi</td>
<td>University of Antwerp</td>
</tr>
<tr>
<td>图像增强</td>
<td>Zhangyang "Atlas" Wang</td>
<td>The University of Texas at Austin</td>
</tr>
<tr>
<td>形状检测</td>
<td>Qi Jia</td>
<td>大连理工大学</td>
</tr>
<tr>
<td>文档</td>
<td>Dr. Vikas Ramachandra</td>
<td>Columbia University in the City of New York</td>
</tr>
<tr>
<td>辅助技术</td>
<td>Jagadish Mahendran</td>
<td>Bovi.ag</td>
</tr>
</tbody>
</table>
<p><strong>官方<a target="_blank" rel="noopener" href="https://forum.opencv.org/">论坛</a></strong></p>
<p>社区交流可以在官方论坛上互动。</p>
<h3 id="opencv合作伙伴">OpenCV合作伙伴</h3>
<ul>
<li>Intell, OpenCV 白金会员</li>
<li>黄金会员：Microsoft Azure, Google summer of Code, FUTUREWEI,
华为（成为黄金会员方式：捐献十万美金，开发者或者其他资源）</li>
<li>发展合作伙伴：KHADAS, ORBBEC, RunPod</li>
</ul>
<p>合作联系方式：contact@opencv.ai（美国），admin@opencv.org.cn
（中国）</p>
<h3 id="opencv中国团队">OpenCV中国团队</h3>
<p>OpenCV中国团队于2019年9月成立，
非营利目的，致力于OpenCV的开发、维护和推广工作。OpenCV中国团队由OpenCV项目发起人Gary
Bradski担任团队顾问，OpenCV技术负责人<a
target="_blank" rel="noopener" href="https://github.com/vpisarev">Vadim
Pisarevsky</a>领导技术开发，OpenCV中文社区创始人于仕琪博士担任团队负责人。</p>
<p>国内负责人和核心开发成员均为<a
target="_blank" rel="noopener" href="https://faculty.sustech.edu.cn/?cat=2&amp;tagid=yusq&amp;orderby=date&amp;iscss=1&amp;snapid=1&amp;go=1">于仕琪博士团队</a>，并且是于仕琪的研究助理，主要社区提交为DNN相关内容。</p>
<table>
<colgroup>
<col style="width: 3%" />
<col style="width: 13%" />
<col style="width: 28%" />
<col style="width: 37%" />
<col style="width: 5%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr>
<th>姓名</th>
<th>职位</th>
<th>github id</th>
<th>OpenCV贡献</th>
<th>社区职位</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>吴佳</td>
<td>研究助理</td>
<td><a target="_blank" rel="noopener" href="https://github.com/kaingwade">kaingwade</a></td>
<td><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pull/23531">1PR</a>
<font color=green>38 ++</font> <font color=red>0--</font></td>
<td>无</td>
<td></td>
</tr>
<tr>
<td>母自豪</td>
<td>研究助理，2018级研究生</td>
<td><a target="_blank" rel="noopener" href="https://github.com/zihaomu">zihaomu</a></td>
<td><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pulls?q=is%3Apr+is%3Aclosed+author%3Azihaomu">79PR</a>
<font color=green>42,381 ++</font> <font color=red>22,986 --</font></td>
<td>reviewer</td>
<td></td>
</tr>
<tr>
<td>冯远滔</td>
<td>研究助理，2018级研究生</td>
<td><a target="_blank" rel="noopener" href="https://github.com/fengyuentau">fengyuentau</a></td>
<td><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pulls?q=is%3Apr+is%3Aclosed+author%3Afengyuentau">51PR</a>
<font color=green>7,863 ++</font> <font color=red>2,053 --</font></td>
<td>reviewer</td>
<td>DNN支持CANN后端作者</td>
</tr>
<tr>
<td>钟万里</td>
<td>研究助理</td>
<td><a target="_blank" rel="noopener" href="https://github.com/WanliZhong">WanliZhong</a></td>
<td><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pulls?q=is%3Apr+is%3Aclosed+author%3AWanliZhong">18PR</a>
<font color=green>561 ++</font> <font color=red>147 --</font></td>
<td>reviewer</td>
<td></td>
</tr>
</tbody>
</table>
<p>OpenCV欢迎外部公司<a
target="_blank" rel="noopener" href="https://www.opencv.org.cn/?page_id=56">合作</a>，可由OpenCV中国团队指导，外部公司软件工程师开发，提交patch到OpenCV项目。可联系中国团队（admin@opencv.org.cn）洽谈。其中提到了在不同硬件平台上的OpenCV的加速，契合昇腾使能的诉求。</p>
<h3 id="社区运作方式">社区运作方式</h3>
<ul>
<li>代码仓库和版本控制：OpenCV代码托管在<a
target="_blank" rel="noopener" href="https://github.com/opencv">github</a>上，使用git做版本管理，社区成员在这些仓库中提交代码和PR，有reviewer进行代码review，并最终由commitor合入代码。OpenCV有三个主要代码库：
<ul>
<li>opencv：opencv主要代码库，包含关键数据结构和成熟算法，HAL方式在此库以3rd_party的方式贡献</li>
<li>opencv_contrib：opencv扩展模块库，依赖opencv_core，社区提交要求先进入此库，成熟后合入主库。cuda等算法均在此库，命名空间方式以独立模块方式在此库贡献</li>
<li>opencv_extra：opencv扩展数据库，存放测试数据，供测试使用</li>
</ul></li>
<li>讨论和沟通：OpenCV社区使用（邮件列表？未找到）<a
target="_blank" rel="noopener" href="https://github.com/opencv">github</a>，<a
target="_blank" rel="noopener" href="https://forum.opencv.org/">论坛</a>进行技术讨论和沟通</li>
<li>问题追踪和解决：OpenCV使用github issue进行问题追踪：
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv/issues">opencv</a> -
库和稳定模块的一般问题，与构建相关的问题</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv_contrib/issues">opencv_contrib</a>
-实验模块及其依赖项的问题</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/opencv-infrastructure/opencv.org/issues">opencv.org</a>
- 官方网站的问题</li>
</ul></li>
<li>文档和教程：最新的版本使用<a
target="_blank" rel="noopener" href="https://docs.opencv.org/">Doxygen</a>来展示文档，其中包含了<a
target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d9/df8/tutorial_root.html">使用教程</a></li>
<li>社区活动和会议：开发者可以参与<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/GSoC_2023">谷歌代码之夏</a>活动来提交新的想法和代码，社区领导团队每周三太平洋时间8:00
am在环聊上开展会议，并归档到<a
href="(https://github.com/opencv/opencv/wiki/Meeting_notes)">wiki</a>上。</li>
<li>教育和培训：社区联合<a
target="_blank" rel="noopener" href="https://bigvision.ai/">Bigision</a>提供了许多在线<a
target="_blank" rel="noopener" href="https://opencv.org/university/">课程</a>（收费），完成学习后可以获得毕业证书以及优秀证书，针对企业和组织也提供了培训计划。</li>
</ul>
<h2 id="昇腾接入方式">昇腾接入方式</h2>
<h3 id="贡献开源方式">贡献开源方式</h3>
<ol type="1">
<li>HAL：OpenCV在core，imgproc，feature2d这三个模块都提供了HAL机制，通过include
<code>hal_replacement.hpp</code>替换相关宏定义，决定真正的执行后端。</li>
<li>命名空间：使用独立模块，实现与cv相同的函数接口。使用命名空间的方式进行调用。</li>
</ol>
<p><strong>两种接入方式比较</strong></p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 45%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr>
<th>比较项</th>
<th>HAL</th>
<th>命名空间</th>
</tr>
</thead>
<tbody>
<tr>
<td>实现成本</td>
<td>低，进需要实现定义好的函数接口即可，不涉及Matrix对象，入参出参均为常见数据类型。</td>
<td>较高，需要自定义实现NPU上的Mat结构，代码生成等相关工作。</td>
</tr>
<tr>
<td>用户使用成本</td>
<td>对用户完全透明，无缝替换。</td>
<td>用户需要替换函数调用接口，有可能涉及Mat对象转换NPU
Mat对象操作。</td>
</tr>
<tr>
<td>性能</td>
<td>每个算子执行前后均涉及内存数据搬迁，无法异步执行，效率低。</td>
<td>计算过程中算子无需搬迁，可使用异步执行，效率高。</td>
</tr>
<tr>
<td>现有实现</td>
<td>carotene，Nvdia实现的SIMD加速库。</td>
<td>CUDA加速。</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">hipudding</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/hipudding" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
