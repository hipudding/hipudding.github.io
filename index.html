<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="mask-icon" href="https://dogecoin.com/assets/images/doge.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="hipudding&#39;s Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="hipudding&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="hipudding">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>hipudding's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">hipudding's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/hipudding" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/12/AscendC-vs-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/12/AscendC-vs-CUDA/" class="post-title-link" itemprop="url">AscendC vs CUDA</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-10-12 09:35:14 / 修改时间：09:35:41" itemprop="dateCreated datePublished" datetime="2023-10-12T09:35:14+08:00">2023-10-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="ascendc-vs-cuda">AscendC vs CUDA</h1>
<p>[TOC]</p>
<h2 id="什么是ascendccuda编程">什么是AscendC/CUDA编程</h2>
<blockquote>
<p>面向算子开发场景的编程语言Ascend
C，原生支持C和C++标准规范，最大化匹配用户开发习惯；通过多层接口抽象、自动并行计算、孪生调试等关键技术，极大提高算子开发效率，助力AI开发者低成本完成算子开发和模型调优部署。</p>
</blockquote>
<blockquote>
<p><strong>CUDA</strong>（<strong>C</strong>ompute
<strong>U</strong>nified <strong>D</strong>evice
<strong>A</strong>rchitecture，<strong>统一计算架构</strong>[<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/CUDA#cite_note-1">1]</a>）是由英伟达<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/NVIDIA">NVIDIA</a>所推出的一种<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/軟體">软</a><a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/计算机硬件">硬件</a>集成技术，是该公司对于<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GPGPU">GPGPU</a>的正式名称。透过这个技术，用户可利用NVIDIA的<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GPU">GPU</a>进行<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/图像处理">图像处理</a>之外的运算，亦是首次可以利用GPU作为C-编译器的开发环境。</p>
</blockquote>
<p><strong>一句话概括：AscendC/CUDA就是使用昇腾设备/GPU设备的编程接口。</strong></p>
<h2 id="与我们熟悉的编程有什么区别">与我们熟悉的编程有什么区别</h2>
<p><strong>内存</strong></p>
<p>Host编程仅考虑主存，所有的内存操作对象均为主存，不需要考虑CPU缓存，寄存器等，这些对程序开发完全透明。</p>
<p>Device编程需要了解每个运行单元能访问的内存类型，可以理解要手动管理一级二级缓存，例如，AscendC变成框架下，内存的类型有：</p>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 89%" />
</colgroup>
<thead>
<tr class="header">
<th>枚举值</th>
<th>具体含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GM</td>
<td>Global Memory，对应AI Core的外部存储。</td>
</tr>
<tr class="even">
<td>VECIN</td>
<td>用于矢量计算，搬入数据的存放位置，在数据搬入Vector计算单元时使用此位置</td>
</tr>
<tr class="odd">
<td>VECOUT</td>
<td>用于矢量计算，搬出数据的存放位置，在将Vector计算单元结果搬出时使用此位置</td>
</tr>
<tr class="even">
<td>VECCALC</td>
<td>用于矢量计算/矩阵计算，在计算需要临时变量时使用此位置</td>
</tr>
<tr class="odd">
<td>A1</td>
<td>用于矩阵计算，存放整块A矩阵，可类比CPU多级缓存中的二级缓存</td>
</tr>
<tr class="even">
<td>B1</td>
<td>用于矩阵计算，存放整块B矩阵，可类比CPU多级缓存中的二级缓存</td>
</tr>
<tr class="odd">
<td>A2</td>
<td>用于矩阵计算，存放切分后的小块A矩阵，可类比CPU多级缓存中的一级缓存</td>
</tr>
<tr class="even">
<td>B2</td>
<td>用于矩阵计算，存放切分后的小块B矩阵，可类比CPU多级缓存中的一级缓存</td>
</tr>
<tr class="odd">
<td>CO1</td>
<td>用于矩阵计算，存放小块结果C矩阵，可理解为Cube Out</td>
</tr>
<tr class="even">
<td>CO2</td>
<td>用于矩阵计算，存放整块结果C矩阵，可理解为Cube Out</td>
</tr>
</tbody>
</table>
<p>不同的处理单元，不同的处理步骤访问的内存是不同的，需要开发者自行处理。</p>
<p><strong>编程模型</strong></p>
<p>Host编程一般为串行的，如果想启用并行处理需要手动开启多线程，或者SIMD(Single
Instruction, Multiple Data)。</p>
<p>Device编程一般为并行，SPMD(Single-Program
Multiple-Data)。在设备上启动多线程，共同处理一份数据。Device编程代码分为两个部分，Host侧执行的一般代码和在设备上执行的核函数(kernel
function)。</p>
<p>AscendC还需要注意的是流水线编程范式，流水线编程主要是为了加速数据拷贝，Device处理以及数据拷回的流程。因为DMA搬运单元，各个计算单元是并行工作的，使用流水线能够提高设备单元的使用率。</p>
<h2 id="device-的内部结构抽象">Device 的内部结构抽象</h2>
<p><strong>Ascend AI Core 内部抽象结构</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2021051922433521.png"
alt="达芬奇架构" />
<figcaption aria-hidden="true">达芬奇架构</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001706909793.png"
alt="AI Core抽象结构" />
<figcaption aria-hidden="true">AI Core抽象结构</figcaption>
</figure>
<p><strong>CUDA核心内部抽象结构</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/70.png"
alt="CUDA核心结构" />
<figcaption aria-hidden="true">CUDA核心结构</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/70-20231010170118685.png"
alt="CUDA核心结构" />
<figcaption aria-hidden="true">CUDA核心结构</figcaption>
</figure>
<p><strong>AI Core和Stream Multiprocessor的最主要区别是：</strong></p>
<ul>
<li><p><strong>AI
Core中是专用处理单元，包括Vector和Cube，分别用户向量和矩阵运算，能用向量和矩阵运算的操作效率会很高。</strong></p></li>
<li><p><strong>Stream
Multiprocessor基本上都是大量的int32核心，float32核心或者双精度核心，由于数量众多，所以并行能力更强。</strong></p></li>
</ul>
<h2 id="ascendc编程和cuda编程对比">AscendC编程和CUDA编程对比</h2>
<p><strong>AscendC</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;acl/acl.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tikicpulib.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel_operator.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;data_loader.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> AscendC;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __aicore__</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __aicore__ [aicore]</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BUFFER_NUM = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BLOCK_DIM = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*****************************Copy scalar to ubuf*****************************/</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">FlipTilingData</span> &#123;</span><br><span class="line">  <span class="type">uint32_t</span> height;</span><br><span class="line">  <span class="type">uint32_t</span> width;</span><br><span class="line">  <span class="type">uint32_t</span> channel;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> __aicore__ <span class="type">int32_t</span> <span class="title">align32</span><span class="params">(<span class="type">int32_t</span> n)</span> </span>&#123; <span class="keyword">return</span> ((n + <span class="number">31</span>) &amp; ~<span class="number">31</span>); &#125;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> __aicore__ <span class="type">int32_t</span> <span class="title">AlignDiv32</span><span class="params">(<span class="type">int32_t</span> n)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">align32</span>(n) / <span class="number">32</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  __ubuf__ tilingStruct* tilingDataPointer =                                \</span></span><br><span class="line"><span class="meta">      reinterpret_cast<span class="string">&lt;__ubuf__ tilingStruct*&gt;</span>(                             \</span></span><br><span class="line"><span class="meta">          (__ubuf__ uint8_t*)(tilingPointer));</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INIT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer);</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INIT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  __ubuf__ uint8_t* tilingUbPointer = (__ubuf__ uint8_t*)get_imm(0);     \</span></span><br><span class="line"><span class="meta">  copy_gm_to_ubuf(((__ubuf__ uint8_t*)(tilingUbPointer)),                \</span></span><br><span class="line"><span class="meta">                  ((__gm__ uint8_t*)(tilingPointer)), 0, 1,              \</span></span><br><span class="line"><span class="meta">                  AlignDiv32(sizeof(tilingStruct)), 0, 0);               \</span></span><br><span class="line"><span class="meta">  CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingUbPointer); \</span></span><br><span class="line"><span class="meta">  pipe_barrier(PIPE_ALL);</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GET_TILING_DATA(tilingData, tilingPointer) \</span></span><br><span class="line"><span class="meta">  INIT_TILING_DATA(FlipTilingData, tilingData, tilingPointer);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_ACL(x)                                                    \</span></span><br><span class="line"><span class="meta">  do &#123;                                                                  \</span></span><br><span class="line"><span class="meta">    aclError __ret = x;                                                 \</span></span><br><span class="line"><span class="meta">    <span class="keyword">if</span> (__ret != ACL_ERROR_NONE) &#123;                                      \</span></span><br><span class="line"><span class="meta">      std::cerr &lt;&lt; __FILE__ &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; __LINE__ &lt;&lt; <span class="string">&quot; aclError:&quot;</span> &lt;&lt; __ret \</span></span><br><span class="line"><span class="meta">                &lt;&lt; std::endl;                                           \</span></span><br><span class="line"><span class="meta">    &#125;                                                                   \</span></span><br><span class="line"><span class="meta">  &#125; while (0);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*******************************Kernel function*******************************/</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KernelFlip</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="title">KernelFlip</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Init</span><span class="params">(GM_ADDR input, GM_ADDR output, <span class="type">uint32_t</span> _height,</span></span></span><br><span class="line"><span class="params"><span class="function">                              <span class="type">uint32_t</span> _width, <span class="type">uint32_t</span> _channel)</span> </span>&#123;</span><br><span class="line">    <span class="type">uint32_t</span> blockNum = <span class="built_in">GetBlockNum</span>();</span><br><span class="line">    <span class="type">uint32_t</span> blockIdx = <span class="built_in">GetBlockIdx</span>();</span><br><span class="line"></span><br><span class="line">    rowLength = _height / blockNum;</span><br><span class="line">    startRowIdx = blockIdx * rowLength;</span><br><span class="line">    <span class="keyword">if</span> (startRowIdx + rowLength &gt; _height) &#123;</span><br><span class="line">      rowLength = _height - startRowIdx;</span><br><span class="line">    &#125;</span><br><span class="line">    width = _width;</span><br><span class="line">    height = _height;</span><br><span class="line">    channel = _channel;</span><br><span class="line">    rowSize = width * channel;</span><br><span class="line">    <span class="type">uint32_t</span> bufferSize = <span class="built_in">align32</span>(rowSize * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>));</span><br><span class="line"></span><br><span class="line">    inputGM.<span class="built_in">SetGlobalBuffer</span>((__gm__ <span class="type">uint8_t</span>*)input + startRowIdx * rowSize,</span><br><span class="line">                            rowLength * rowSize);</span><br><span class="line">    outputGM.<span class="built_in">SetGlobalBuffer</span>((__gm__ <span class="type">uint8_t</span>*)output + startRowIdx * rowSize,</span><br><span class="line">                             rowLength * rowSize);</span><br><span class="line">    pipe.<span class="built_in">InitBuffer</span>(inQueue, BUFFER_NUM, bufferSize);</span><br><span class="line">    pipe.<span class="built_in">InitBuffer</span>(outQueue, BUFFER_NUM, bufferSize);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Process</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int32_t</span> i = <span class="number">0</span>; i &lt; rowLength; i++) &#123;</span><br><span class="line">      <span class="built_in">CopyIn</span>(i);</span><br><span class="line">      <span class="built_in">Compute</span>(i);</span><br><span class="line">      <span class="built_in">CopyOut</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">CopyIn</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; local = inQueue.<span class="built_in">AllocTensor</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="built_in">DataCopy</span>(local, inputGM[loop * rowSize], rowSize);</span><br><span class="line">    inQueue.<span class="built_in">EnQue</span>(local);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Compute</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; inputLocal = inQueue.<span class="built_in">DeQue</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; outputLocal = outQueue.<span class="built_in">AllocTensor</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int32_t</span> i = <span class="number">0</span>; i &lt; width; i++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int32_t</span> c = <span class="number">0</span>; c &lt; channel; c++) &#123;</span><br><span class="line">        outputLocal.<span class="built_in">SetValue</span>(</span><br><span class="line">            i * channel + c,</span><br><span class="line">            inputLocal.<span class="built_in">GetValue</span>((width - i - <span class="number">1</span>) * channel + c));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    outQueue.<span class="built_in">EnQue</span>&lt;<span class="type">uint8_t</span>&gt;(outputLocal);</span><br><span class="line">    inQueue.<span class="built_in">FreeTensor</span>(inputLocal);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">CopyOut</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; local = outQueue.<span class="built_in">DeQue</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="built_in">DataCopy</span>(outputGM[loop * rowSize], local, rowSize);</span><br><span class="line">    outQueue.<span class="built_in">FreeTensor</span>(local);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  TPipe pipe;</span><br><span class="line">  TQue&lt;QuePosition::VECIN, BUFFER_NUM&gt; inQueue;</span><br><span class="line">  TQue&lt;QuePosition::VECOUT, BUFFER_NUM&gt; outQueue;</span><br><span class="line">  GlobalTensor&lt;<span class="type">uint8_t</span>&gt; inputGM, outputGM;</span><br><span class="line">  <span class="type">uint32_t</span> startRowIdx, rowLength, rowSize, height, width, channel;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*******************************kernel interface******************************/</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> <span class="function">__global__ __aicore__ <span class="type">void</span> <span class="title">flip</span><span class="params">(GM_ADDR input, GM_ADDR output,</span></span></span><br><span class="line"><span class="params"><span class="function">                                           GM_ADDR tiling)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">GET_TILING_DATA</span>(tilingData, tiling);</span><br><span class="line">  KernelFlip op;</span><br><span class="line">  op.<span class="built_in">Init</span>(input, output, tilingData-&gt;height, tilingData-&gt;width,</span><br><span class="line">          tilingData-&gt;channel);</span><br><span class="line">  op.<span class="built_in">Process</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">flip_do</span><span class="params">(<span class="type">uint32_t</span> blockDim, <span class="type">void</span>* l2ctrl, <span class="type">void</span>* stream, <span class="type">uint8_t</span>* input,</span></span></span><br><span class="line"><span class="params"><span class="function">             <span class="type">uint8_t</span>* output, <span class="type">uint8_t</span>* tiling)</span> </span>&#123;</span><br><span class="line">  flip&lt;&lt;&lt;blockDim, l2ctrl, stream&gt;&gt;&gt;(input, output, tiling);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************************caller************************************/</span></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">main</span><span class="params">(<span class="type">int32_t</span> argc, <span class="type">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">&quot; path/to/datafile&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> blockDim = BLOCK_DIM;</span><br><span class="line">  <span class="type">uint32_t</span> height, width, channel;</span><br><span class="line">  <span class="type">uint8_t</span>* data = <span class="built_in">readFile</span>(argv[<span class="number">1</span>], height, width, channel);</span><br><span class="line">  <span class="type">const</span> <span class="type">char</span>* resultFile = std::<span class="built_in">string</span>(argv[<span class="number">1</span>]).<span class="built_in">append</span>(<span class="string">&quot;.ret&quot;</span>).<span class="built_in">c_str</span>();</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> dataSize = width * height * channel * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>);</span><br><span class="line">  <span class="type">size_t</span> inputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> outputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> tilingSize = <span class="built_in">sizeof</span>(FlipTilingData);</span><br><span class="line"></span><br><span class="line">  <span class="type">uint8_t</span> *inputHost, *outputHost, *tilingHost;</span><br><span class="line">  <span class="type">uint32_t</span> shape[]&#123;height, width, channel&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line">  inputHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(inputByteSize);</span><br><span class="line">  outputHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(outputByteSize);</span><br><span class="line">  tilingHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(tilingHost, shape, tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(inputHost, data, dataSize);</span><br><span class="line"></span><br><span class="line">  AscendC::<span class="built_in">SetKernelMode</span>(KernelMode::AIV_MODE);</span><br><span class="line">  <span class="built_in">ICPU_RUN_KF</span>(flip, blockDim, inputHost, outputHost, tilingHost);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, outputHost);</span><br><span class="line"></span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)inputHost);</span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)outputHost);</span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)tilingHost);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclInit</span>(<span class="literal">nullptr</span>));</span><br><span class="line">  aclrtContext context;</span><br><span class="line">  <span class="type">int32_t</span> deviceId = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtSetDevice</span>(deviceId));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtCreateContext</span>(&amp;context, deviceId));</span><br><span class="line">  aclrtStream stream = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtCreateStream</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">  <span class="type">uint8_t</span> *inputDevice, *outputDevice, *tilingDevice;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;tilingHost), tilingSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;inputHost), inputByteSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;outputHost), outputByteSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;inputDevice, inputByteSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;outputDevice, outputByteSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;tilingDevice, tilingSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">memcpy</span>(tilingHost, shape, tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(inputHost, data, dataSize);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(inputDevice, inputByteSize, inputHost, inputByteSize,</span><br><span class="line">                        ACL_MEMCPY_HOST_TO_DEVICE));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(tilingDevice, tilingSize, tilingHost, tilingSize,</span><br><span class="line">                        ACL_MEMCPY_HOST_TO_DEVICE));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">flip_do</span>(blockDim, <span class="literal">nullptr</span>, stream, inputDevice, outputDevice, tilingDevice);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtSynchronizeStream</span>(stream));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(outputHost, outputByteSize, outputDevice,</span><br><span class="line">                        outputByteSize, ACL_MEMCPY_DEVICE_TO_HOST));</span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, outputHost);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(inputDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(outputDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(tilingDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(inputHost));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(outputHost));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(tilingHost));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtDestroyStream</span>(stream));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtDestroyContext</span>(context));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtResetDevice</span>(deviceId));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclFinalize</span>());</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="built_in">free</span>(data);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001706789009.png"
alt="流水线示例" />
<figcaption aria-hidden="true">流水线示例</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001658510374.png"
alt="数据切分示例" />
<figcaption aria-hidden="true">数据切分示例</figcaption>
</figure>
<p><strong>CUDA</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;data_loader.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">flip</span><span class="params">(<span class="type">uint8_t</span>* input, <span class="type">uint8_t</span>* output, <span class="type">uint32_t</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="type">uint32_t</span> width, <span class="type">uint32_t</span> channel)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> rowIdx = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">  <span class="type">int</span> rowSize = width * channel;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> row = rowIdx; row &lt; height; row += stride) &#123;</span><br><span class="line">    <span class="type">int</span> startOffset = row * rowSize;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> idx = <span class="number">0</span>; idx &lt; width; idx++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; channel; c++) &#123;</span><br><span class="line">        output[startOffset + idx * channel + c] =</span><br><span class="line">            input[startOffset + (width - idx - <span class="number">1</span>) * channel + c];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int32_t</span> argc, <span class="type">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">&quot; path/to/datafile&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">uint32_t</span> height, width, channel;</span><br><span class="line">  <span class="type">char</span> fileName[<span class="number">256</span>], resultFile[<span class="number">256</span>];</span><br><span class="line">  <span class="built_in">memset</span>(fileName, <span class="number">0</span>, <span class="number">256</span>);</span><br><span class="line">  <span class="built_in">memset</span>(resultFile, <span class="number">0</span>, <span class="number">256</span>);</span><br><span class="line">  <span class="built_in">strcpy</span>(fileName, argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="built_in">strcat</span>(resultFile, fileName);</span><br><span class="line">  <span class="built_in">strcat</span>(resultFile, <span class="string">&quot;.ret&quot;</span>);</span><br><span class="line">  <span class="type">uint8_t</span>* data = <span class="built_in">readFile</span>(fileName, height, width, channel);</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> dataSize = width * height * channel * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>);</span><br><span class="line">  <span class="type">size_t</span> inputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> outputByteSize = dataSize;</span><br><span class="line">  <span class="type">uint8_t</span> *input, *output;</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;input, inputByteSize);</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;output, outputByteSize);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">memcpy</span>(input, data, inputByteSize);</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">gridSize</span><span class="params">((height + blockSize.x - <span class="number">1</span>) / blockSize.x)</span></span>;</span><br><span class="line"></span><br><span class="line">  flip&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(input, output, height, width, channel);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, output);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(input);</span><br><span class="line">  <span class="built_in">cudaFree</span>(output);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/29/Pytorch-DDP%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/29/Pytorch-DDP%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Pytorch DDP原理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-29 17:10:02 / 修改时间：17:13:19" itemprop="dateCreated datePublished" datetime="2023-06-29T17:10:02+08:00">2023-06-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>DDP=分布式数据并行(DISTRIBUTED DATA
PARALLEL)，和DP一样也是一种数据并行的方法，详细文档可以参考<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/ddp.html">官方手册</a></p>
<h2 id="ddp使用">DDP使用</h2>
<p>以下是使用NPU进行2个节点并行的示例代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch_npu</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    torch.npu.set_device(rank)</span><br><span class="line">    device = torch.device(<span class="string">&#x27;npu&#x27;</span>)</span><br><span class="line">    <span class="comment"># create default process group</span></span><br><span class="line">    dist.init_process_group(<span class="string">&quot;hccl&quot;</span>, rank=rank, world_size=world_size)</span><br><span class="line">    <span class="comment"># create local model</span></span><br><span class="line">    model = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(device)</span><br><span class="line">    <span class="comment"># construct DDP model</span></span><br><span class="line">    ddp_model = DDP(model, device_ids=[rank])</span><br><span class="line">    <span class="comment"># define loss function and optimizer</span></span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    optimizer = optim.SGD(ddp_model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    <span class="comment"># forward pass</span></span><br><span class="line">    outputs = ddp_model(torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(device))</span><br><span class="line">    labels = torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward pass</span></span><br><span class="line">    loss = loss_fn(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update parameters</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    world_size = <span class="number">2</span></span><br><span class="line">    mp.spawn(example,</span><br><span class="line">        args=(world_size,),</span><br><span class="line">        nprocs=world_size,</span><br><span class="line">        join=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Environment variables which need to be</span></span><br><span class="line">    <span class="comment"># set when using c10d&#x27;s default &quot;env&quot;</span></span><br><span class="line">    <span class="comment"># initialization mode.</span></span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_ADDR&quot;</span>] = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_PORT&quot;</span>] = <span class="string">&quot;29500&quot;</span></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="ddp原理">DDP原理</h2>
<h3 id="相关文件">相关文件</h3>
<table>
<colgroup>
<col style="width: 57%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="header">
<th>文件</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>torch/nn/parallel/distributed.py</td>
<td>DistributedDataParallel类实现</td>
</tr>
<tr class="even">
<td>torch/csrc/distributed/c10d/reducer.cpp</td>
<td>DDP reduce类实现文件</td>
</tr>
<tr class="odd">
<td>torch/csrc/distributed/c10d/reducer.hpp</td>
<td>DDP reduce接口文件</td>
</tr>
<tr class="even">
<td>torch/csrc/distributed/c10d/init.cpp</td>
<td>python C插件注册文件</td>
</tr>
</tbody>
</table>
<h3 id="进程交互">进程交互</h3>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230629163126602.png"
alt="image-20230629163126602" />
<figcaption aria-hidden="true">image-20230629163126602</figcaption>
</figure>
<ul>
<li>主线程通过torch.multiprocessing，fork出新的进程，满足word_size的要求，然后各个线程独立运行，主进程将自己的host和port放到环境变量中共子进程使用。</li>
<li>各个进程<strong><font color='orange'>初始化process
group</font></strong>，这些进程属于同一个进程组。</li>
<li>每个进程分别创建模型，DDP对象，定义loss函数和优化器。</li>
<li>创建DDP对象时，主进程会<strong><font color="red">同步模型参数</font></strong>，这一步完成后，各个进程的模型一致。</li>
<li>每个模型输入部分数据，这些数据需要调用者自行分割，可以使用DistributedSampler。</li>
<li>然后执行模型的训练过程，每次backward执行完后，调用注册的reducer
hook函数，做<strong><font color='green'>梯度聚合</font></strong>。</li>
</ul>
<h3 id="reducer">Reducer</h3>
<p>普通模型的DDP，除了每次反向执行后的梯度同步之外，进程之间没有其他通信的需求。DDP构造时，会在每个模型参数上配置回调函数(autograd_hook)，当参数的梯度计算完成后，会调用这个回调函数。为了能够一边计算，一边同步梯度数据，DDP将相邻参数分组，每个组叫一个bucket。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/72401724-d296d880-371a-11ea-90ab-737f86543df9.png"
alt="ddp_grad_sync.png" />
<figcaption aria-hidden="true">ddp_grad_sync.png</figcaption>
</figure>
<p>这是参数分组和allreduce的官方示意图，回调函数的执行流程如下：</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230629170221846.png"
alt="image-20230629170221846" />
<figcaption aria-hidden="true">image-20230629170221846</figcaption>
</figure>
<ul>
<li>每个参数梯度计算完成后，均会调用回调函数，该回调函数会将参数标记为ready。</li>
<li>然后检查bucket中的pending是否为0，也就是bucket的参数是否全部完成梯度计算，如果完成，则准备进行一步all
reduce。</li>
<li>all
reduce是一个异步方法，任务执行后不阻塞，继续执行。（如果某个进程的当前bucket没有执行完成，dist
backend会等待，执行完成后异步返回）。</li>
<li>当所有的bucket都发起all reduce后，等待每个bucket的all
reduce结果，所有结果都返回后，完成本次backward。</li>
</ul>
<h2 id="参考">参考</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/ddp.html">DISTRIBUTED
DATA PARALLEL</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187610959">原创 深度 PyTorch
DDP系列第二篇：实现原理与源代码解析</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/21/Pytorch-DP%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/21/Pytorch-DP%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Pytorch DP原理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-21 15:50:59" itemprop="dateCreated datePublished" datetime="2023-06-21T15:50:59+08:00">2023-06-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:15:22" itemprop="dateModified" datetime="2023-06-26T10:15:22+08:00">2023-06-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>DP(DataParallel)=数据并行，DP框架会将数据切片，然后模型拷贝，并在不同的backend上并行执行。<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html?highlight=data+parallel#torch.nn.DataParallel">官方手册说明</a></p>
<h2 id="dp使用">DP使用</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = LinearModel()</span><br><span class="line">model = torch.nn.DataParallel(model,device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>从代码中看到，DP仅支持cuda，xpu或者昇腾，并不支持cpu。</p>
<h2 id="dp原理">DP原理</h2>
<h3 id="相关文件">相关文件</h3>
<table>
<colgroup>
<col style="width: 45%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr class="header">
<th>文件</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>torch/nn/parallel/data_parallel.py</td>
<td>DP类实现</td>
</tr>
<tr class="even">
<td>torch/nn/parallel/_functions.py</td>
<td>DP中使用的自定义autograd Function</td>
</tr>
<tr class="odd">
<td>torch/nn/parallel/comm.py</td>
<td>线程间数据传递接口，例如，gather,scatter等</td>
</tr>
<tr class="even">
<td>torch/nn/parallel/replicate.py</td>
<td>模型拷贝相关</td>
</tr>
<tr class="odd">
<td>torch/nn/parallel/parallel_apply.py</td>
<td>多线程执行模型</td>
</tr>
<tr class="even">
<td>torch/nn/parallel/scatter_gather.py</td>
<td>scatter和gather的封装</td>
</tr>
<tr class="odd">
<td>torch/csrc/cuda/python_comm.cpp</td>
<td>C语言实现的数据传递函数extension注册</td>
</tr>
<tr class="even">
<td>torch/csrc/cuda/comm.cpp</td>
<td>C语言的数据传递实现</td>
</tr>
</tbody>
</table>
<h3 id="基本原理">基本原理</h3>
<p>DP基于单机多卡，所有的卡都参与训练。相对于非数据并行训练来说，DP会将一个batch数据切分为更小的batch，将数据复制到每一张计算的卡上，然后复制模型到所有的卡上，进行多线程执行，Forward计算完成后，会在device[0]上收集到一组预测值。device[0]计算loss，然后执行Backward计算梯度。（<strong>计算Backward没有看到创建线程的过程，单线程执行？？</strong>）</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/v2-5c5b0d8e3d7d6653a9ebd47bac93090c_720w.webp"
alt="DP原理图" />
<figcaption aria-hidden="true">DP原理图</figcaption>
</figure>
<h3 id="dp-wapper">DP wapper</h3>
<p>回顾DP使用方法，参与数据并行的模型进需要使用DataParalle包一层即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model,device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>可以理解，下图就是上述代码的实现，DP就是在原本的模型执行之前增加了数据切分(Scatter)，模型复制(Broadcast)，然后将模型的并行执行结果进行合并(Gather)。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230621152641246.png"
alt="image-20230621152641246" />
<figcaption aria-hidden="true">image-20230621152641246</figcaption>
</figure>
<h3 id="数据传递">数据传递</h3>
<p>DP的Forward函数核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, *inputs: <span class="type">Any</span>, **kwargs: <span class="type">Any</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">	    ...</span><br><span class="line">      <span class="comment"># 训练数据切分，并拷贝到每一个卡上</span></span><br><span class="line">      inputs, module_kwargs = self.scatter(inputs, kwargs, self.device_ids)</span><br><span class="line">      </span><br><span class="line">      ...</span><br><span class="line">      <span class="comment"># 模型数据拷贝到每一个卡上</span></span><br><span class="line">      replicas = self.replicate(self.module, self.device_ids[:<span class="built_in">len</span>(inputs)])</span><br><span class="line">      <span class="comment"># 每张卡启动一个线程执行，每个线程执行模型的forward函数</span></span><br><span class="line">      outputs = self.parallel_apply(replicas, inputs, module_kwargs)</span><br><span class="line">      <span class="comment"># 集合所有线程的预测结果</span></span><br><span class="line">      <span class="keyword">return</span> self.gather(outputs, self.output_device)</span><br></pre></td></tr></table></figure>
<p>由于是单机单进程，所以数据的拷贝过程相对简单，以上述代码的scatter和broadcast为例，数据切分，然后将切分后的数据移动到指定设备上。但是Tensor还是在同一个数组中。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230626101410741.png"
alt="image-20230626101410741" />
<figcaption aria-hidden="true">image-20230626101410741</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230621152441395.png"
alt="image-20230621152441395" />
<figcaption aria-hidden="true">image-20230621152441395</figcaption>
</figure>
<h3 id="自定义自动梯度方法">自定义自动梯度方法</h3>
<p>Pytorch实现了自动梯度计算，除了官方实现的梯度计算之外，还能够自定义梯度计算方法，具体描述参考<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">官方说明</a>。以下是代码中对Function类的使用举例说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Examples::</span><br><span class="line"></span><br><span class="line">    &gt;&gt;&gt; <span class="comment"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span></span><br><span class="line">    &gt;&gt;&gt; <span class="keyword">class</span> <span class="title class_">Exp</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line">    &gt;&gt;&gt;     @<span class="built_in">staticmethod</span></span><br><span class="line">    &gt;&gt;&gt;     <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, i</span>):</span><br><span class="line">    &gt;&gt;&gt;         result = i.exp()</span><br><span class="line">    &gt;&gt;&gt;         ctx.save_for_backward(result)</span><br><span class="line">    &gt;&gt;&gt;         <span class="keyword">return</span> result</span><br><span class="line">    &gt;&gt;&gt;</span><br><span class="line">    &gt;&gt;&gt;     @<span class="built_in">staticmethod</span></span><br><span class="line">    &gt;&gt;&gt;     <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">    &gt;&gt;&gt;         result, = ctx.saved_tensors</span><br><span class="line">    &gt;&gt;&gt;         <span class="keyword">return</span> grad_output * result</span><br><span class="line">    &gt;&gt;&gt;</span><br><span class="line">    &gt;&gt;&gt; <span class="comment"># Use it by calling the apply method:</span></span><br><span class="line">    &gt;&gt;&gt; <span class="comment"># xdoctest: +SKIP</span></span><br><span class="line">    &gt;&gt;&gt; output = Exp.apply(<span class="built_in">input</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>实现自定义自动梯度方法，需要继承Function类，并实现forward和backward方法，并且这两个方法均是静态的。使用自定义自动梯度方法的时候，不应该调用forward，而是调用Exp.apply，这样会返回forward的结果。相当于自己定义了算子的前向和后向的处理方法。当在执行反向传播的时候，会按反顺序调用自定义自动梯度方法的backward方法。</p>
<p>DP实现了三个自定义自动梯度方法，分别是Broadcast，Gather和Scatter，其反向传播方法内分别调用了Reduce_add，Scatter和Gather，即forward和backward中的动作是成对的。</p>
<h3 id="训练流程">训练流程</h3>
<p><strong>正向</strong></p>
<ul>
<li>①：将输入的一个batch切分成三个更小的batch</li>
<li>②：将模型的参数复制三份，分别发送到三个device上</li>
<li>③：启动三个线程，每个线程在不同的device上进行原始模型的前向计算</li>
<li>④：将三个线程的输出结果拼接到一起</li>
</ul>
<p><strong>Loss</strong></p>
<ul>
<li>在device[0]上计算Loss</li>
</ul>
<p><strong>反向</strong></p>
<ul>
<li>④：将Loss切分，分别发送给对应的模型</li>
<li>③：执行三个模型的backward方法，更新梯度（<strong>Backward
Engine多线程处理</strong>）</li>
<li>②：将三个模型上的梯度相加，并储存到Original
Module中（<strong>直接相加？</strong>）</li>
<li>①：将三个模型的梯度信息拼接到一起</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230621144922423.png"
alt="image-20230621144922423" />
<figcaption aria-hidden="true">image-20230621144922423</figcaption>
</figure>
<h3 id="官方建议">官方建议</h3>
<ol type="1">
<li>不建议使用DP来进行数据并行，因为这不是一个真正的并发执行（python
GIL），并且，由于数据的转移，反而会减慢模型的执行，建议使用DDP。</li>
<li>所有拷贝来的数据都是一次性的，每次迭代后都会销毁重建，所以这些模型上如果有数据修改，将不会持久化。（<strong>Input最后做的gather是为了下一层网络的输入</strong>）</li>
</ol>
<h2 id="参考">参考</h2>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html?highlight=data+parallel#torch.nn.DataParallel">Pytorch
DATAPARALLEL</a></p>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">Extending
torch.autograd</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343951042">PyTorch 源码解读之
DP &amp; DDP：模型并行和分布式训练解析</a></p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187610959">[原创][深度][PyTorch]
DDP系列第二篇：实现原理与源代码解析</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/13/mpi%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/13/mpi%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">mpi入门</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-13 16:32:00" itemprop="dateCreated datePublished" datetime="2023-06-13T16:32:00+08:00">2023-06-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-21 15:59:43" itemprop="dateModified" datetime="2023-06-21T15:59:43+08:00">2023-06-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>MPI全称叫做<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%A8%8A%E6%81%AF%E5%82%B3%E9%81%9E%E4%BB%8B%E9%9D%A2">消息传递接口</a>（Message
passing interface）</p>
<h2 id="安装mpi4py">安装mpi4py</h2>
<p>Python可以使用<a
target="_blank" rel="noopener" href="https://github.com/mpi4py/mpi4py">mpi4py库</a>，使用pip安装的话需要提前安装好openmpi库：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libopenmpi-dev</span><br></pre></td></tr></table></figure>
<p>由于Anaconda存在的缘故，pip安装编译会报找不到一些so库。解决方法见<a
target="_blank" rel="noopener" href="https://github.com/mpi4py/mpi4py/issues/343">issue</a></p>
<p>不过既然Anaconda存在，可以直接使用conda安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge mpi4py openmpi</span><br></pre></td></tr></table></figure>
<h2 id="名词介绍">名词介绍</h2>
<ul>
<li>world: 一个进程组，聚集通信在这个进程组中进行</li>
<li>rank: 当前进程（自己）的编号，0是master进程</li>
<li>world_size: 进程组中进程的个数</li>
</ul>
<h2 id="点对点通信">点对点通信</h2>
<p><strong>举例</strong></p>
<ul>
<li>send(isend 非阻塞)</li>
<li>recv(irecv 非阻塞)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    comm.send(matrix, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; send matrix &#123;&#125; to other processes&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    matrix_recv = comm.recv()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; recv matrix &#123;&#125; from master&quot;</span>.<span class="built_in">format</span>(rank, matrix_recv))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mpiexec -np 2 python mpi_test.py </span><br><span class="line"></span><br><span class="line">process 1 recv matrix </span><br><span class="line">[[ 0  1 -7  3 -7 -1 -2  2 -3  7]</span><br><span class="line"> [ 2 -6  7  4 -5 -1  0 -4  2  4]</span><br><span class="line"> [ 0  1  3  5  8 -3 -8 -4 -3 -7]</span><br><span class="line"> [ 1 -9 -8  5 -9 -7 -7 -6 -6 -2]</span><br><span class="line"> [-2  3 -3 -4  3  7  0  2  7  8]</span><br><span class="line"> [-3 -6 -4 -5 -5 -1  2 -3 -9  3]</span><br><span class="line"> [-6  6  5  4 -5  8 -2 -2  3  6]</span><br><span class="line"> [ 1  7 -6 -7  0 -2  8  4  7 -6]</span><br><span class="line"> [-2 -7 -5 -4 -9  0 -3  8 -2  7]</span><br><span class="line"> [-3  2  2  6  0 -8 -5  0 -4  5]] from master</span><br><span class="line"> </span><br><span class="line">process 0 send matrix </span><br><span class="line">[[ 0  1 -7  3 -7 -1 -2  2 -3  7]</span><br><span class="line"> [ 2 -6  7  4 -5 -1  0 -4  2  4]</span><br><span class="line"> [ 0  1  3  5  8 -3 -8 -4 -3 -7]</span><br><span class="line"> [ 1 -9 -8  5 -9 -7 -7 -6 -6 -2]</span><br><span class="line"> [-2  3 -3 -4  3  7  0  2  7  8]</span><br><span class="line"> [-3 -6 -4 -5 -5 -1  2 -3 -9  3]</span><br><span class="line"> [-6  6  5  4 -5  8 -2 -2  3  6]</span><br><span class="line"> [ 1  7 -6 -7  0 -2  8  4  7 -6]</span><br><span class="line"> [-2 -7 -5 -4 -9  0 -3  8 -2  7]</span><br><span class="line"> [-3  2  2  6  0 -8 -5  0 -4  5]] to other processes</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="聚集通信">聚集通信</h2>
<p><strong>举例</strong></p>
<ul>
<li><p>bcast</p></li>
<li><p>scatter</p></li>
<li><p>gather(all_gather所有的进程均收到gather结果)</p></li>
<li><p>reduce(all_reduce所有的进程均收到reduce结果)</p></li>
</ul>
<p><strong>bcast</strong>
广播消息到所有进程组内进程，调用者如果是master，则是发送；其他进程是接收</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    matrix = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="built_in">int</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; use empty matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line">matrix = comm.bcast(matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; bcast matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">process 1 use empty matrix [[0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]]</span><br><span class="line"> </span><br><span class="line">process 0 prepare random matrix [[ 5 -8  8  4  1 -7 -8 -6  1  3]</span><br><span class="line"> [-6 -5  6 -1 -8  5  7  2  3  7]</span><br><span class="line"> [-2 -9 -9  7  0  8 -4  2 -8 -1]</span><br><span class="line"> [ 3 -1  3  3 -7  7 -5 -5 -9 -7]</span><br><span class="line"> [ 0 -4 -7 -8 -8 -8  7  2  2  1]</span><br><span class="line"> [-2  0  1  7  6  3  8 -1  1  8]</span><br><span class="line"> [ 6 -4  7 -7  6 -1 -4  6 -6 -9]</span><br><span class="line"> [-3  2  8 -7  8 -8  8  3 -1 -3]</span><br><span class="line"> [ 6  7  2 -2  1 -5  1  2  1 -2]</span><br><span class="line"> [ 2 -3 -2 -3 -7  6  7 -8 -9  4]]</span><br><span class="line"> </span><br><span class="line">process 0 bcast matrix [[ 5 -8  8  4  1 -7 -8 -6  1  3]</span><br><span class="line"> [-6 -5  6 -1 -8  5  7  2  3  7]</span><br><span class="line"> [-2 -9 -9  7  0  8 -4  2 -8 -1]</span><br><span class="line"> [ 3 -1  3  3 -7  7 -5 -5 -9 -7]</span><br><span class="line"> [ 0 -4 -7 -8 -8 -8  7  2  2  1]</span><br><span class="line"> [-2  0  1  7  6  3  8 -1  1  8]</span><br><span class="line"> [ 6 -4  7 -7  6 -1 -4  6 -6 -9]</span><br><span class="line"> [-3  2  8 -7  8 -8  8  3 -1 -3]</span><br><span class="line"> [ 6  7  2 -2  1 -5  1  2  1 -2]</span><br><span class="line"> [ 2 -3 -2 -3 -7  6  7 -8 -9  4]]</span><br><span class="line"> </span><br><span class="line">process 1 bcast matrix [[ 5 -8  8  4  1 -7 -8 -6  1  3]</span><br><span class="line"> [-6 -5  6 -1 -8  5  7  2  3  7]</span><br><span class="line"> [-2 -9 -9  7  0  8 -4  2 -8 -1]</span><br><span class="line"> [ 3 -1  3  3 -7  7 -5 -5 -9 -7]</span><br><span class="line"> [ 0 -4 -7 -8 -8 -8  7  2  2  1]</span><br><span class="line"> [-2  0  1  7  6  3  8 -1  1  8]</span><br><span class="line"> [ 6 -4  7 -7  6 -1 -4  6 -6 -9]</span><br><span class="line"> [-3  2  8 -7  8 -8  8  3 -1 -3]</span><br><span class="line"> [ 6  7  2 -2  1 -5  1  2  1 -2]</span><br><span class="line"> [ 2 -3 -2 -3 -7  6  7 -8 -9  4]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>scatter</strong>会将发送的数据拆分，然后分给进程组中的进程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    line = np.array_split(matrix, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    line = <span class="literal">None</span></span><br><span class="line">one_piece = comm.scatter(line)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; bcast matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, one_piece))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mpiexec --oversubscribe -np 10 python mpi_test.py</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据的分组数量和进程数要对应。 默认mpi最大进程数量和cpu相同，可以添加--oversubscribe解除限制</span></span><br><span class="line"></span><br><span class="line">process 0 prepare random matrix [[-9 -2 -6  4  1  7  5  5 -1 -3]</span><br><span class="line"> [-4  6  3  5 -9 -6  7  3  0 -1]</span><br><span class="line"> [-9 -6  2  5 -1  2 -2 -5 -8 -7]</span><br><span class="line"> [ 3 -4 -1 -9 -2  5  4 -8  0  1]</span><br><span class="line"> [ 3  1 -2  6 -4  0 -2 -3  6  8]</span><br><span class="line"> [ 6  5 -8 -7 -3 -7 -5 -8  5  8]</span><br><span class="line"> [-3 -9  2 -2  0 -7  6 -5  5 -2]</span><br><span class="line"> [-6 -4  8  5 -6  6  2 -3 -2  4]</span><br><span class="line"> [-5  7  3 -2  4 -8  3 -5  7 -5]</span><br><span class="line"> [ 0  4 -9 -1 -5 -2 -1 -5  7  7]]</span><br><span class="line">process 0 bcast matrix [[-9 -2 -6  4  1  7  5  5 -1 -3]]</span><br><span class="line">process 2 bcast matrix [[-9 -6  2  5 -1  2 -2 -5 -8 -7]]</span><br><span class="line">process 4 bcast matrix [[ 3  1 -2  6 -4  0 -2 -3  6  8]]</span><br><span class="line">process 6 bcast matrix [[-3 -9  2 -2  0 -7  6 -5  5 -2]]</span><br><span class="line">process 1 bcast matrix [[-4  6  3  5 -9 -6  7  3  0 -1]]</span><br><span class="line">process 3 bcast matrix [[ 3 -4 -1 -9 -2  5  4 -8  0  1]]</span><br><span class="line">process 5 bcast matrix [[ 6  5 -8 -7 -3 -7 -5 -8  5  8]]</span><br><span class="line">process 8 bcast matrix [[-5  7  3 -2  4 -8  3 -5  7 -5]]</span><br><span class="line">process 9 bcast matrix [[ 0  4 -9 -1 -5 -2 -1 -5  7  7]]</span><br><span class="line">process 7 bcast matrix [[-6 -4  8  5 -6  6  2 -3 -2  4]]</span><br></pre></td></tr></table></figure>
<p><strong>gather</strong>收集所有分发后的数据，比如，我们需要对scatter分发的所有数据+1，然后再收集回来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    line = np.array_split(matrix, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    line = <span class="literal">None</span></span><br><span class="line">line = comm.scatter(line)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> line:</span><br><span class="line">    item += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">recv_result = comm.gather(line)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.vstack(recv_result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; get calculated matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">process 0 prepare random matrix </span><br><span class="line">[[-2  4  0  4 -2 -1  7  3 -1  4]</span><br><span class="line"> [-4 -8 -1 -9  8  7  8 -3 -6  1]</span><br><span class="line"> [ 1 -8  0  6 -6 -8 -7  2 -9 -1]</span><br><span class="line"> [ 7 -7  0 -4 -4  0 -2  4 -7  7]</span><br><span class="line"> [-7  3  0 -8  5 -8 -2  0  7  2]</span><br><span class="line"> [-6 -9 -3 -8  0  0  0 -7 -4 -5]</span><br><span class="line"> [ 2 -6  3 -8  8 -8 -2  0 -2 -9]</span><br><span class="line"> [ 8  8 -8 -8  7  8  4 -3 -9  3]</span><br><span class="line"> [ 8 -4  4  2  8 -2  2  7  0 -6]</span><br><span class="line"> [ 3  1 -3  2 -8 -6 -4 -5  3  0]]</span><br><span class="line"> </span><br><span class="line">process 0 get calculated matrix </span><br><span class="line">[[-1  5  1  5 -1  0  8  4  0  5]</span><br><span class="line"> [-3 -7  0 -8  9  8  9 -2 -5  2]</span><br><span class="line"> [ 2 -7  1  7 -5 -7 -6  3 -8  0]</span><br><span class="line"> [ 8 -6  1 -3 -3  1 -1  5 -6  8]</span><br><span class="line"> [-6  4  1 -7  6 -7 -1  1  8  3]</span><br><span class="line"> [-5 -8 -2 -7  1  1  1 -6 -3 -4]</span><br><span class="line"> [ 3 -5  4 -7  9 -7 -1  1 -1 -8]</span><br><span class="line"> [ 9  9 -7 -7  8  9  5 -2 -8  4]</span><br><span class="line"> [ 9 -3  5  3  9 -1  3  8  1 -5]</span><br><span class="line"> [ 4  2 -2  3 -7 -5 -3 -4  4  1]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>reduce</strong> 将收集到的数据进行规约，例如SUM，MAX等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    line = np.array_split(matrix, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    line = <span class="literal">None</span></span><br><span class="line">line = comm.scatter(line)</span><br><span class="line">sum_result = comm.reduce(line, op=MPI.SUM)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; get calculated sum result &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, sum_result))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">process 0 prepare random matrix </span><br><span class="line">[[ 2 -9 -9 -8 -9 -8 -7  8 -6  5]</span><br><span class="line"> [ 0  6  6 -2 -6  4 -5 -7  3 -4]</span><br><span class="line"> [-9  2  6 -5 -1 -6  5 -2  3  4]</span><br><span class="line"> [ 7  4 -6 -5  1  4 -5  5 -1  1]</span><br><span class="line"> [ 1  2 -8 -7  5  2 -6 -4  5 -8]</span><br><span class="line"> [-8  0  7  1  0 -8  3 -8 -2  3]</span><br><span class="line"> [ 6  5  2 -7 -6 -3 -7  4  2 -1]</span><br><span class="line"> [ 2 -8  8 -1 -3 -9  0  4 -9 -9]</span><br><span class="line"> [-4 -7 -1 -8  4  7  1  6  4  5]</span><br><span class="line"> [-2  6  3  3  7  7 -4  1 -7 -3]]</span><br><span class="line"> </span><br><span class="line">process 0 get calculated sum result [[ -5   1   8 -39  -8 -10 -25   7  -8  -7]]</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/12/DeepSpeed-%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/12/DeepSpeed-%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">DeepSpeed 安装使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-12 16:57:33" itemprop="dateCreated datePublished" datetime="2023-06-12T16:57:33+08:00">2023-06-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-19 09:24:00" itemprop="dateModified" datetime="2023-06-19T09:24:00+08:00">2023-06-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="deepspeedubuntucpu">DeepSpeed+Ubuntu+CPU</h2>
<p>目前CPU支持很有限，仅支持部分推理。环境配置可参考<a
target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed/commit/1f72082fc0ea159d0de46886d8e713dda7df9ce2#diff-69a0fa9b9a28351868fc14c345088d9af3cf3560538482a386dc616ebc023fe3">CI配置</a></p>
<p><strong>安装intel_extension_for_pytorch</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install intel_extension_for_pytorch</span><br><span class="line">python -m pip install oneccl_bind_pt==2.0 -f https://developer.intel.com/ipex-whl-stable-cpu</span><br></pre></td></tr></table></figure>
<p><strong>安装oneCCL</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/oneapi-src/oneCCL</span><br><span class="line">cd oneCCL</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">source ./_install/env/setvars.sh</span><br></pre></td></tr></table></figure>
<p><strong>安装Transformers(用于跑用例)</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huggingface/transformers</span><br><span class="line">cd transformers</span><br><span class="line">git rev-parse --short HEAD</span><br><span class="line">pip install .</span><br></pre></td></tr></table></figure>
<p><strong>安装DeepSpeed</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install DeepSpeed</span><br></pre></td></tr></table></figure>
<p><strong>ds_report</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed C++/CUDA extension op report</span><br><span class="line">--------------------------------------------------</span><br><span class="line">NOTE: Ops not installed will be just-in-time (JIT) compiled at</span><br><span class="line">      runtime if needed. Op compatibility means that your system</span><br><span class="line">      meet the required dependencies to JIT install the op.</span><br><span class="line">--------------------------------------------------</span><br><span class="line">JIT compiled ops requires ninja</span><br><span class="line">ninja .................. [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">op name ................ installed .. compatible</span><br><span class="line">--------------------------------------------------</span><br><span class="line">deepspeed_not_implemented  [NO] ....... [OKAY]</span><br><span class="line">deepspeed_ccl_comm ....... [NO] ....... [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed general environment info:</span><br><span class="line">torch install path ............... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/torch&#x27;]</span><br><span class="line">torch version .................... 2.0.1+cu117</span><br><span class="line">deepspeed install path ........... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/deepspeed&#x27;]</span><br><span class="line">deepspeed info ................... 0.9.4+e5fe5f65, e5fe5f65, master</span><br><span class="line">deepspeed wheel compiled w. ...... torch 0.0</span><br></pre></td></tr></table></figure>
<h2 id="deepspeedubuntugpu">DeepSpeed+Ubuntu+GPU</h2>
<p><strong>操作系统</strong></p>
<p>华为云上gpu的镜像是16.04的，很多依赖的软件版本过低，建议升级到20.04，DeepSpeedExample有些需要高版本的glibc，可以直接升级到22.04。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt-get dist-upgrade</span><br><span class="line">sudo do-release-upgrade</span><br></pre></td></tr></table></figure>
<p><strong>GPU驱动和cuda</strong></p>
<ul>
<li><p><a
target="_blank" rel="noopener" href="https://www.nvidia.cn/Download/index.aspx?lang=cn">驱动下载页面</a>选择相应的型号，选择cuda
11.7</p></li>
<li><p><a
target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=runfile_local">CUDA
toolkit下载页面</a>选择系统版本，然后下载runfile（使用apt总是会升级驱动和cuda到最新版本，所以直接下载二进制安装）。</p></li>
</ul>
<p><strong>pip源</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<p><strong>安装<a
target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">pytorch</a></strong></p>
<p>DeepSpeed
0.9.2（pip版本）目前依赖的是pytorch1.13.1，安装对应的版本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117</span><br></pre></td></tr></table></figure>
<p><strong>triton==1.0.0</strong></p>
<p>DeepSpeed依赖triton版本是1.0.0，pip仓库无法直接安装，git上下载源码编译安装。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编译依赖</span></span><br><span class="line">sudo apt-get install llvm-11 llvm-11-*</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">源码安装</span></span><br><span class="line">wget https://github.com/openai/triton/archive/refs/tags/v1.0.zip</span><br><span class="line">unzip v1.0.zip</span><br><span class="line">cd triton/python</span><br><span class="line">pip install cmake</span><br><span class="line">pip install .</span><br></pre></td></tr></table></figure>
<p><strong>安装DeepSpeed</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">预编译算子(编译不通过：https://github.com/microsoft/DeepSpeed/issues/425)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">workground: Setting NVCC_PREPEND_FLAGS=<span class="string">&quot;--forward-unknown-opts&quot;</span></span></span><br><span class="line">DS_BUILD_OPS=1 pip install deepspeed --global-option=&quot;build_ext&quot; --global-option=&quot;-j8&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JIT_load</span></span><br><span class="line">pip install deepspeed</span><br></pre></td></tr></table></figure>
<p><strong>ds_report</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed C++/CUDA extension op report</span><br><span class="line">--------------------------------------------------</span><br><span class="line">NOTE: Ops not installed will be just-in-time (JIT) compiled at</span><br><span class="line">      runtime if needed. Op compatibility means that your system</span><br><span class="line">      meet the required dependencies to JIT install the op.</span><br><span class="line">--------------------------------------------------</span><br><span class="line">JIT compiled ops requires ninja</span><br><span class="line">ninja .................. [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">op name ................ installed .. compatible</span><br><span class="line">--------------------------------------------------</span><br><span class="line">async_io ............... [NO] ....... [OKAY]</span><br><span class="line">cpu_adagrad ............ [NO] ....... [OKAY]</span><br><span class="line">cpu_adam ............... [NO] ....... [OKAY]</span><br><span class="line">fused_adam ............. [NO] ....... [OKAY]</span><br><span class="line">fused_lamb ............. [NO] ....... [OKAY]</span><br><span class="line">quantizer .............. [NO] ....... [OKAY]</span><br><span class="line">random_ltd ............. [NO] ....... [OKAY]</span><br><span class="line">sparse_attn ............ [NO] ....... [OKAY]</span><br><span class="line">spatial_inference ...... [NO] ....... [OKAY]</span><br><span class="line">transformer ............ [NO] ....... [OKAY]</span><br><span class="line">stochastic_transformer . [NO] ....... [OKAY]</span><br><span class="line">transformer_inference .. [NO] ....... [OKAY]</span><br><span class="line">utils .................. [NO] ....... [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed general environment info:</span><br><span class="line">torch install path ............... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/torch&#x27;]</span><br><span class="line">torch version .................... 1.13.1+cu117</span><br><span class="line">deepspeed install path ........... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/deepspeed&#x27;]</span><br><span class="line">deepspeed info ................... 0.9.2, unknown, unknown</span><br><span class="line">torch cuda version ............... 11.7</span><br><span class="line">torch hip version ................ None</span><br><span class="line">nvcc version ..................... 11.7</span><br><span class="line">deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7</span><br></pre></td></tr></table></figure>
<p><strong>运行例子</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/microsoft/DeepSpeedExamples.git</span><br><span class="line">cd DeepSpeedExamples/training/cifar</span><br><span class="line">./run_ds.py</span><br></pre></td></tr></table></figure>
<p>python报错，修改方法：</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">diff --git a/training/cifar/cifar10_deepspeed.py b/training/cifar/cifar10_deepspeed.py</span></span><br><span class="line"><span class="comment">index 33ea569..d1117c3 100755</span></span><br><span class="line"><span class="comment">--- a/training/cifar/cifar10_deepspeed.py</span></span><br><span class="line"><span class="comment">+++ b/training/cifar/cifar10_deepspeed.py</span></span><br><span class="line"><span class="meta">@@ -159,7 +159,7 @@</span> def imshow(img):</span><br><span class="line"></span><br><span class="line"> # get some random training images</span><br><span class="line"> dataiter = iter(trainloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # show images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="meta">@@ -309,7 +309,7 @@</span> print(&#x27;Finished Training&#x27;)</span><br><span class="line"> # Okay, first step. Let us display an image from the test set to get familiar.</span><br><span class="line"></span><br><span class="line"> dataiter = iter(testloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # print images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment">diff --git a/training/cifar/cifar10_tutorial.py b/training/cifar/cifar10_tutorial.py</span></span><br><span class="line"><span class="comment">index 2154e36..114e8c5 100644</span></span><br><span class="line"><span class="comment">--- a/training/cifar/cifar10_tutorial.py</span></span><br><span class="line"><span class="comment">+++ b/training/cifar/cifar10_tutorial.py</span></span><br><span class="line"><span class="meta">@@ -110,7 +110,7 @@</span> def imshow(img):</span><br><span class="line"></span><br><span class="line"> # get some random training images</span><br><span class="line"> dataiter = iter(trainloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # show images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="meta">@@ -219,7 +219,7 @@</span> torch.save(net.state_dict(), PATH)</span><br><span class="line"> # Okay, first step. Let us display an image from the test set to get familiar.</span><br><span class="line"></span><br><span class="line"> dataiter = iter(testloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # print images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br></pre></td></tr></table></figure>
<h2 id="简单模型改写为deepspeed">简单模型改写为DeepSpeed</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">with_ds = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> with_ds:</span><br><span class="line">    <span class="keyword">import</span> deepspeed</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x_data = torch.tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]]).to(device)</span><br><span class="line">y_data = torch.tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]]).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LinearModel()</span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> with_ds:</span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ds_config = &#123;</span><br><span class="line">        <span class="string">&quot;train_micro_batch_size_per_gpu&quot;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">&quot;optimizer&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;SGD&quot;</span>,</span><br><span class="line">            <span class="string">&quot;params&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;lr&quot;</span>: <span class="number">1e-2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=ds_config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> with_ds:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.backward(loss)</span><br><span class="line">        model.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.tensor([[<span class="number">4.0</span>]]).to(device)</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>DeepSpeed基本执行流程</strong></p>
<p><em>这个简单的模型实际上还是完全调用的pytorch的函数，ds包的wapper其实啥都没干，完全透传</em></p>
<ol type="1">
<li>首先获取Accelerator（加速器），判断能不能import
intel_extension_for_deepspeed，如果能就用XPU，如果不能就用CUDA（Accelerator将设备管理，内存管理，Tensor等等进行了抽象，不同的后端设备继承实现），高版本这部分逻辑有改变，可以通过环境变量控制。<strong>切入点1，这里需要判断是否能够使用npu</strong></li>
<li>选择并行计算后端，如果Torch.distributed已经初始化，则使用直接使用，检查是否在Aure或者aws机器上，针对这些机器做环境变量配置，否则尝试寻找mpi，然后根据Accelerator的comm
backend类型初始化TorchBackend.<strong>切入点2，这里需要针对华为云机器做专门的环境变量配置，以及支持昇腾并行后端</strong></li>
<li>解析ds的配置文件</li>
<li>创建ds引擎
<ol type="1">
<li>检查环境变量，配置dist相关配置，包括rank，world size 等等</li>
<li>用dist分发模型参数，所有进程同步模型参数</li>
<li>根据配置创建optimizer，例如，上例中，就会生成torch自带的SGD优化器，也可以指定ds提供的Adam，lamb等优化器</li>
<li>配置checkpoint</li>
<li>编译Utils，就是flateen_unflateen.cpp</li>
</ol></li>
<li>执行forward，计算损失
<strong>切入点3，以下基本调用的都是pytorch的optimizer，这些需要pytorch有昇腾支持，另外，DS提供的优化器也需要昇腾支持</strong></li>
<li>反向传播
<ol type="1">
<li>梯度累加</li>
<li>根据是否使用zero优化，自动精度，混合精度等调用optimizer其他wapper的backward，并传入合适的参数</li>
<li>多进程计算梯度并收集结果</li>
</ol></li>
<li>更新参数
<ol type="1">
<li>如果到了梯度累加的预制，根据不同的配置，最终调用optimizer.step更新参数，并清空梯度信息</li>
</ol></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/06/%E5%89%8D%E5%90%91-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/06/%E5%89%8D%E5%90%91-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/" class="post-title-link" itemprop="url">前向&反向传播&链式法则</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-06 11:23:51 / 修改时间：13:06:25" itemprop="dateCreated datePublished" datetime="2023-06-06T11:23:51+08:00">2023-06-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="神经网络">神经网络</h2>
<p><strong>通俗理解</strong>，神经网络就是给一组输入，经过神经网络的运算后，得到一个在误差范围内的结果。模型的选择和对模型中各个神经元的系数的确定决定了模型的输出结果，为了得到期望的结果，需要有多组输入，和对应的正确结果，用这些数据来训练模型，确定合适的参数组合。参数确定后，即可预测新的输入对应的输出。</p>
<p>举一个简单的例子，两个神经元组成一个一层的神经网络，给定的两个输入，能够得到一个结果。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230601142530229.png"
alt="image-20230601142530229" />
<figcaption aria-hidden="true">image-20230601142530229</figcaption>
</figure>
<h2 id="前向传播">前向传播</h2>
<p><strong>通俗理解</strong>前向传播，就是根据输入计算输出的过程。按上述例子来说，就是
<span class="math display">\[
net_o = i1×w1+b1 + i2×w2+b2
\]</span> 假定输入数据为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i1 = 0.05, i2 = 0.10;</span><br><span class="line">w1 = 0.15, w2 = 0.20;</span><br><span class="line">b1 = 0.35, b2 = 0.60.</span><br></pre></td></tr></table></figure>
<p>那么： <span class="math display">\[
net_o = i1×w1+b1 + i2×w2+b2 = 0.05×0.15+0.35+0.10*0.20+0.60=0.9775
\]</span> 假定选择sigmoid函数作为激活函数，那么 <span
class="math display">\[
out_o =
\frac{1}{1+\epsilon^{net_o}}=\frac{1}{1+\epsilon^{0.9775}}=0.2734
\]</span></p>
<p>如果此时我们期望的输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">o=0.01</span><br></pre></td></tr></table></figure>
<p>误差为 <span class="math display">\[
o-out_o=0.01-0.2734=-0.2634
\]</span></p>
<h2 id="梯度下降法">梯度下降法</h2>
<p>考虑一个抛物线函数： <span class="math display">\[
y=f(x)^2
\]</span></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230601151845547.png"
alt="image-20230601151845547" />
<figcaption aria-hidden="true">image-20230601151845547</figcaption>
</figure>
<p>X轴为权值，Y轴为误差，那么我们能计算得到曲线斜率为0的位置为最低位置（x=0），如果是一个复杂的多维函数，不容易直接通过计算得出曲线或者平面的最低位置，所以可以使用梯度下降法来通过迭代计算最低位置。相当于放一个小球，小球会在重力的影响下，沿最陡的方向下降。</p>
<p>假设目前在x=10的点上(x0)，梯度为： <span class="math display">\[
\nabla f(x_0)=f&#39;(x0)=2x=20
\]</span>
那梯度的反方向就是下降率最快的方向，如果步长为η=0.2，那么新的位置x1为：
<span class="math display">\[
x_1 = x_0 - \eta×\nabla f(x_0)=10-0.2*20=6
\]</span> 然后继续迭代，直到梯度曲线趋近于0.</p>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>x0</th>
<th>x1</th>
<th>x2</th>
<th>x3</th>
<th>x4</th>
<th>x5</th>
<th>x6</th>
<th>X7</th>
<th>x8</th>
<th>x9</th>
<th>X10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>梯度</td>
<td>20</td>
<td>12</td>
<td>7,2</td>
<td>4.32</td>
<td>2.59</td>
<td>1.56</td>
<td>0.93</td>
<td>0.56</td>
<td>0.34</td>
<td>0.2</td>
<td>0.12</td>
</tr>
</tbody>
</table>
<p>注意步长的选择，如果步长太小，那么经过多次迭代仍然无法达到曲线或者平面的最低点，如果步长过长，那么或跳过最低点，从而不收敛。</p>
<p>同理，可以从二维（一元函数，曲线）推广到三维（二元函数，平面）甚至多维，目标就是找到最陡的反向，然后按步长走到下一个位置，然后一直迭代下去。</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/305638940">参考文档</a></p>
<h2 id="反向传播">反向传播</h2>
<p>假设使用均方误差（MSE）来作为误差评估方法，那么误差为(这个是MSE么???)：
<span class="math display">\[
E=\Sigma\frac{1}{2}(o-out_o)^2
\]</span> 由于这个例子中只有一个输出，那么误差为： <span
class="math display">\[
E=\frac{1}{2}(0.01-0.2734)^2=0.03468978
\]</span>
反向传播就是通过误差来计算当前参数的梯度，然后沿梯度下降的方向走一个步长η，迭代此过程。</p>
<p>根据正向传播的函数，能够得到： <span class="math display">\[
out_o = \frac{1}{1+\epsilon^{net_o}} = \frac{1}{1+\epsilon^{(i1×w1+b1 +
i2×w2+b2)}}
\]</span></p>
<p><span class="math display">\[
E=\Sigma\frac{1}{2}(o-out_o)^2=\Sigma\frac{1}{2}(o-\frac{1}{1+\epsilon^{(i1×w1+b1
+ i2×w2+b2)}})^2
\]</span></p>
<p>我们要计算梯度，也就是要计算总的误差E在w1和w2的偏导值。</p>
<p>这个公式比较复杂，所以要使用到求导的链式法则。</p>
<h2 id="链式法则">链式法则</h2>
<p><span class="math display">\[
\frac{\partial E}{\partial w1}=\frac{\partial E}{\partial
out_o}×\frac{\partial out_o}{\partial net_o}×\frac{\partial
net_o}{\partial w1}
\]</span></p>
<p>那么，上述复杂函数的求导就可以转换成几个简单函数求导的乘积。 <span
class="math display">\[
\frac{\partial E}{\partial
out_0}=2×\frac{1}{2}(o-out_o)^1*-1+0=(0.01-0.2734)*-1=0.2634
\]</span></p>
<p><span class="math display">\[
\frac{\partial out_o}{\partial
net_o}=out_o(1-out_o)=0.2734(1-0.2734)=0.19865244
\]</span></p>
<p><span class="math display">\[
\frac{\partial net_o}{\partial w1}=i1=0.05
\]</span></p>
<p>相乘可得： <span class="math display">\[
\frac{\partial E}{\partial w1}=0.2634×0.19865244×0.05=0.0002616
\]</span> 同理： <span class="math display">\[
\frac{\partial E}{\partial w2}=0.2634×0.19865244×0.10=0.0052325
\]</span> 如果步长η=0.2，那么： <span class="math display">\[
w1&#39; = w1 - \eta×\frac{\partial E}{\partial w1} =0.004994768
\]</span></p>
<p><span class="math display">\[
w2&#39; = w2 - \eta×\frac{\partial E}{\partial w2} =0.0989535
\]</span></p>
<p>然后，使用更新后的权值做下一步迭代。</p>
<p><a
target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html">参考文档</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/06/Python-C-API-pybind11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/06/Python-C-API-pybind11/" class="post-title-link" itemprop="url">Python C API && pybind11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-06 11:21:33 / 修改时间：11:22:43" itemprop="dateCreated datePublished" datetime="2023-06-06T11:21:33+08:00">2023-06-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="python-c-api">Python C API</h2>
<h3 id="什么是python的c语言扩展">什么是Python的C语言扩展</h3>
<p><strong>为什么要使用C API</strong> ？</p>
<ul>
<li>C语言实现的复杂计算效率高；</li>
<li>已经有成熟的C语言编写的函数库；</li>
<li>和操作系统相关的操作只能使用C语言实现。</li>
</ul>
<p><strong>为什么Python能够使用C模块</strong> ？</p>
<ul>
<li>cpython虚拟机是由C语言编写；</li>
<li>使用动态链接库的方式可以直接调用模块函数。</li>
</ul>
<p><strong>Python调用C模块都有哪些方法</strong> ？</p>
<ul>
<li>C api；</li>
<li>pybind11；</li>
<li>ctypes；</li>
<li>SWIG；</li>
<li>Cython。</li>
</ul>
<h3 id="我理解的c模块调用原理">我理解的C模块调用原理</h3>
<p><strong>模块加载</strong></p>
<p>python interpreter</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  Module</span><br><span class="line">    |</span><br><span class="line">   \/                       +-----------+</span><br><span class="line">Load lib(libsum.so) ------&gt; |PyModuleDef|</span><br><span class="line">                            +-----------+          +-----------+</span><br><span class="line">                            |PyMethodDef|  ------&gt; |PyMethodDef|</span><br><span class="line">                            +-----------+          +-----------+</span><br><span class="line">                                                   |C function | ------&gt; sum(c function)</span><br><span class="line">                                                   +-----------+</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>模块内函数调用</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">python --   call python function</span><br><span class="line">              |</span><br><span class="line">              \/</span><br><span class="line">       --  PyArg_ParseTuple</span><br><span class="line">       |      |</span><br><span class="line">       |      \/</span><br><span class="line"> C     |  call c module&#x27;s function</span><br><span class="line">       |      |</span><br><span class="line">       |      \/</span><br><span class="line">       --  Py_BuildValue</span><br><span class="line">              |</span><br><span class="line">              \/</span><br><span class="line"> python -- python function return</span><br><span class="line">       </span><br></pre></td></tr></table></figure>
<h3 id="简单示例">简单示例</h3>
<p><strong>libsum.cc</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Python.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 实际业务函数体</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *   业务函数的包装函数，该函数注册到PyMethodDef中，供Python虚拟机调用</span></span><br><span class="line"><span class="comment"> *   传入两个参数，self和args，args中的参数需要从对象中解析出来。</span></span><br><span class="line"><span class="comment"> *   PyArg_ParseTuple接收一个格式串，根据格式传将参数从PyObject中解析出来</span></span><br><span class="line"><span class="comment"> *   返回一个整数，需要包装成PyObject类型。</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject* <span class="title">sum_wrapper</span><span class="params">(PyObject* self, PyObject* args)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> a, b;</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">PyArg_ParseTuple</span>(args, <span class="string">&quot;ii&quot;</span>, &amp;a, &amp;b)) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> result = <span class="built_in">sum</span>(a, b);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;i&quot;</span>, result);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *   描述需要注册到模块中的函数，每个函数定义一行，以&#123;NULL, NULL, 0, NULL&#125;结束</span></span><br><span class="line"><span class="comment"> *   每行结构有4个参数，分别为：python调用的函数名，实际调用的包装函数；传入参数的类型；</span></span><br><span class="line"><span class="comment"> *                          函数的描述。</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="type">static</span> PyMethodDef SumMethods[] = &#123;</span><br><span class="line">    &#123;<span class="string">&quot;sum&quot;</span>, sum_wrapper, METH_VARARGS, <span class="string">&quot;Calculate the sum of two integers.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *   描述模块属性，参数分别为：固定参数；模块名，就是import后的名字；模块描述；？？？；</span></span><br><span class="line"><span class="comment"> *                         函数数组。</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">struct</span> <span class="title class_">PyModuleDef</span> summodule = &#123;</span><br><span class="line">    PyModuleDef_HEAD_INIT,</span><br><span class="line">    <span class="string">&quot;libsum&quot;</span>,</span><br><span class="line">    <span class="string">&quot;A module that adds two numbers&quot;</span>,</span><br><span class="line">    <span class="number">-1</span>,</span><br><span class="line">    SumMethods</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化模块，以PyInit_模块名 命名</span></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">PyInit_libsum</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">PyModule_Create</span>(&amp;summodule);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>py_sum.py</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> libsum</span><br><span class="line"></span><br><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">c = libsum.<span class="built_in">sum</span>(a, b)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;a&#125;</span> + <span class="subst">&#123;b&#125;</span> = <span class="subst">&#123;c&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>setup.py</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"></span><br><span class="line">libsum_module = Extension(<span class="string">&#x27;libsum&#x27;</span>,</span><br><span class="line">                    sources = [<span class="string">&#x27;libsum.cc&#x27;</span>],</span><br><span class="line">                    extra_compile_args=[<span class="string">&#x27;-g&#x27;</span>])</span><br><span class="line"></span><br><span class="line">setup (name = <span class="string">&#x27;libsum&#x27;</span>,</span><br><span class="line">       version = <span class="string">&#x27;1.0&#x27;</span>,</span><br><span class="line">       description = <span class="string">&#x27;This is a libsum module&#x27;</span>,</span><br><span class="line">       ext_modules = [libsum_module])</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python setup.py build_ext --inplace</span><br><span class="line"></span><br><span class="line">生成</span><br><span class="line">libsum.cpython-310-x86_64-linux-gnu.so</span><br><span class="line"></span><br><span class="line">可以直接被python import进来</span><br></pre></td></tr></table></figure>
<h3 id="数组和自定义类型">数组和自定义类型</h3>
<p><strong>数组</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> <span class="title">avg</span><span class="params">(<span class="type">double</span> *arr, <span class="type">size_t</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">        sum += arr[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (sum/len);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">avg_wrapper</span><span class="params">(PyObject *self, PyObject *args)</span> </span>&#123;</span><br><span class="line">  PyObject *bufobj;</span><br><span class="line">  Py_buffer view;</span><br><span class="line">  <span class="type">double</span> result;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">PyArg_ParseTuple</span>(args, <span class="string">&quot;O&quot;</span>, &amp;bufobj)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 数组内存是连续的</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">PyObject_GetBuffer</span>(bufobj, &amp;view,</span><br><span class="line">      PyBUF_ANY_CONTIGUOUS | PyBUF_FORMAT) == <span class="number">-1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 判断是一维数组</span></span><br><span class="line">  <span class="keyword">if</span> (view.ndim != <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="built_in">PyErr_SetString</span>(PyExc_TypeError, <span class="string">&quot;Expected a 1-dimensional array&quot;</span>);</span><br><span class="line">    <span class="built_in">PyBuffer_Release</span>(&amp;view);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 检查是否是double类型的数组</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">strcmp</span>(view.format,<span class="string">&quot;d&quot;</span>) != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">PyErr_SetString</span>(PyExc_TypeError, <span class="string">&quot;Expected an array of doubles&quot;</span>);</span><br><span class="line">    <span class="built_in">PyBuffer_Release</span>(&amp;view);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// View.buf是double数组的首指针，view.shape[0]是这一维度的长度</span></span><br><span class="line">  result = <span class="built_in">avg</span>((<span class="type">double</span>*)view.buf, view.shape[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 需要显式释放Py_buffer</span></span><br><span class="line">  <span class="built_in">PyBuffer_Release</span>(&amp;view);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;d&quot;</span>, result);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>自定义类型</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义C++类型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubleWarpper</span> &#123;</span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="type">double</span> m_value;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">DoubleWarpper</span>(<span class="type">double</span> value) : <span class="built_in">m_value</span>(value) &#123;&#125;</span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">DoubleWarpper</span>() &#123; m_value = <span class="number">0</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">double</span> <span class="title">get</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> m_value; &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">set</span><span class="params">(<span class="type">double</span> value)</span> </span>&#123; m_value = value; &#125;</span><br><span class="line"></span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>+(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value + other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>-(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value - other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>*(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value * other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>/(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value / other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 包装成Python C API需要的结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">  PyObject_HEAD;</span><br><span class="line">  DoubleWarpper *warpper = <span class="literal">nullptr</span>;</span><br><span class="line">&#125; PyDoubleWarpper;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Python对象构造时调用，需要返回一个PyDoubleWarpper类型的对象。 kwds？？？</span></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">pycal_new</span><span class="params">(PyTypeObject *type, PyObject *args, PyObject *kwds)</span> </span>&#123;</span><br><span class="line">  PyDoubleWarpper *self;</span><br><span class="line">  self = (PyDoubleWarpper *)type-&gt;<span class="built_in">tp_alloc</span>(type, <span class="number">0</span>);</span><br><span class="line">  <span class="type">char</span> *kwlist[] = &#123;<span class="string">&quot;value&quot;</span>, <span class="number">0</span>&#125;;</span><br><span class="line">  <span class="type">double</span> value = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">PyArg_ParseTupleAndKeywords</span>(args, kwds, <span class="string">&quot;d&quot;</span>, kwlist, &amp;value)) &#123;</span><br><span class="line">    <span class="built_in">Py_DECREF</span>(self);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  self-&gt;warpper = <span class="keyword">new</span> <span class="built_in">DoubleWarpper</span>(value);</span><br><span class="line">  <span class="keyword">return</span> (PyObject *)self;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Python对象销毁时调用</span></span><br><span class="line"><span class="function"><span class="type">static</span> <span class="type">void</span> *<span class="title">pycal_dealloc</span><span class="params">(PyObject *py_cal)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">delete</span> ((PyDoubleWarpper *)py_cal)-&gt;warpper;</span><br><span class="line">  <span class="built_in">Py_TYPE</span>(py_cal)-&gt;<span class="built_in">tp_free</span>(py_cal);</span><br><span class="line">  <span class="keyword">return</span> (<span class="type">void</span> *)<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从PyObject中获取Calculator指针</span></span><br><span class="line"><span class="function"><span class="type">static</span> DoubleWarpper *<span class="title">get_cal</span><span class="params">(PyObject *obj)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> ((PyDoubleWarpper *)obj)-&gt;warpper;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过Calculator指针构造PyObject</span></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">return_cal</span><span class="params">(DoubleWarpper *cal, PyTypeObject *type)</span> </span>&#123;</span><br><span class="line">  PyDoubleWarpper *obj = <span class="built_in">PyObject_NEW</span>(PyDoubleWarpper, type);</span><br><span class="line">  obj-&gt;warpper = cal;</span><br><span class="line">  <span class="keyword">return</span> (PyObject *)obj;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Calculator类提供的函数的包装</span></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">pycal_set</span><span class="params">(PyObject *self, PyObject *args)</span> </span>&#123;</span><br><span class="line">  DoubleWarpper *cal = <span class="built_in">get_cal</span>(self);</span><br><span class="line">  <span class="type">double</span> value = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">PyArg_ParseTuple</span>(args, <span class="string">&quot;d&quot;</span>, &amp;value)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  cal-&gt;<span class="built_in">set</span>(value);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;i&quot;</span>, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印对象</span></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">pycal_str</span><span class="params">(PyObject *self)</span> </span>&#123;</span><br><span class="line">  DoubleWarpper *cal = <span class="built_in">get_cal</span>(self);</span><br><span class="line">  std::stringstream ss;</span><br><span class="line">  ss&lt;&lt;cal-&gt;<span class="built_in">get</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;s&quot;</span>, ss.<span class="built_in">str</span>().<span class="built_in">c_str</span>());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现加减乘除</span></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">pycal_add</span><span class="params">(PyObject *a, PyObject *b)</span> </span>&#123;</span><br><span class="line">  DoubleWarpper *cal_a = <span class="built_in">get_cal</span>(a);</span><br><span class="line">  DoubleWarpper *cal_b = <span class="built_in">get_cal</span>(b);</span><br><span class="line">  DoubleWarpper *ret = <span class="keyword">new</span> <span class="built_in">DoubleWarpper</span>(*cal_a + *cal_b);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">return_cal</span>(ret, a-&gt;ob_type);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">pycal_minus</span><span class="params">(PyObject *a, PyObject *b)</span> </span>&#123;</span><br><span class="line">  DoubleWarpper *cal_a = <span class="built_in">get_cal</span>(a);</span><br><span class="line">  DoubleWarpper *cal_b = <span class="built_in">get_cal</span>(b);</span><br><span class="line">  DoubleWarpper *ret = <span class="keyword">new</span> <span class="built_in">DoubleWarpper</span>(*cal_a - *cal_b);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">return_cal</span>(ret, a-&gt;ob_type);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">pycal_multipy</span><span class="params">(PyObject *a, PyObject *b)</span> </span>&#123;</span><br><span class="line">  DoubleWarpper *cal_a = <span class="built_in">get_cal</span>(a);</span><br><span class="line">  DoubleWarpper *cal_b = <span class="built_in">get_cal</span>(b);</span><br><span class="line">  DoubleWarpper *ret = <span class="keyword">new</span> <span class="built_in">DoubleWarpper</span>(*cal_a * *cal_b);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">return_cal</span>(ret, a-&gt;ob_type);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">static</span> PyObject *<span class="title">pycal_divide</span><span class="params">(PyObject *a, PyObject *b)</span> </span>&#123;</span><br><span class="line">  DoubleWarpper *cal_a = <span class="built_in">get_cal</span>(a);</span><br><span class="line">  DoubleWarpper *cal_b = <span class="built_in">get_cal</span>(b);</span><br><span class="line">  DoubleWarpper *ret = <span class="keyword">new</span> <span class="built_in">DoubleWarpper</span>(*cal_a / *cal_b);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">return_cal</span>(ret, a-&gt;ob_type);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对象的数字属性，这里仅实现了加减乘除</span></span><br><span class="line"><span class="type">static</span> PyNumberMethods numberMethods = &#123;</span><br><span class="line">    pycal_add,      <span class="comment">// nb_add</span></span><br><span class="line">    pycal_minus,    <span class="comment">// nb_subtract;</span></span><br><span class="line">    pycal_multipy,  <span class="comment">// nb_multiply</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_remainder;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_divmod;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_power;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_negative;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_positive;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_absolute;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_bool;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_invert;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_lshift;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_rshift;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_and;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_xor;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_or;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_int;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_reserved;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_float;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_add;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_subtract;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_multiply;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_remainder;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_power;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_lshift;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_rshift;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_and;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_xor;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_or;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_floor_divide;</span></span><br><span class="line">    pycal_divide,   <span class="comment">// nb_true_divide;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_floor_divide;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_inplace_true_divide;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_index;</span></span><br><span class="line">    <span class="literal">nullptr</span>,        <span class="comment">// nb_matrix_multiply;</span></span><br><span class="line">    <span class="literal">nullptr</span>         <span class="comment">// nb_inplace_matrix_multiply;</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PyCalculator对象的成员函数</span></span><br><span class="line"><span class="type">static</span> PyMethodDef pycal_methods[] = &#123;</span><br><span class="line">    &#123;<span class="string">&quot;set&quot;</span>, (PyCFunction)pycal_set, METH_VARARGS, <span class="string">&quot;set DoubleWarpper value.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="literal">nullptr</span>&#125;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PyCalculator对象内容</span></span><br><span class="line"><span class="type">static</span> PyTypeObject DoubleWarpperType = &#123;</span><br><span class="line">    <span class="built_in">PyVarObject_HEAD_INIT</span>(<span class="literal">nullptr</span>, <span class="number">0</span>) <span class="string">&quot;libsum.DoubleWarpper&quot;</span>, <span class="comment">/* tp_name */</span></span><br><span class="line">    <span class="built_in">sizeof</span>(PyDoubleWarpper),                                  <span class="comment">/* tp_basicsize */</span></span><br><span class="line">    <span class="number">0</span>,                                                        <span class="comment">/* tp_itemsize */</span></span><br><span class="line">    (destructor)pycal_dealloc,                                <span class="comment">/* tp_dealloc */</span></span><br><span class="line">    <span class="number">0</span>,                                        <span class="comment">/* tp_vectorcall_offset */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_getattr */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_setattr */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_reserved */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_repr */</span></span><br><span class="line">    &amp;numberMethods,                           <span class="comment">/* tp_as_number */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_as_sequence */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_as_mapping */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_hash  */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_call */</span></span><br><span class="line">    pycal_str,                                <span class="comment">/* tp_str */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_getattro */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_setattro */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_as_buffer */</span></span><br><span class="line">    Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, <span class="comment">/* tp_flags */</span></span><br><span class="line">    <span class="string">&quot;Coustom DoubleWarpper class.&quot;</span>,           <span class="comment">/* tp_doc */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_traverse */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_clear */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_richcompare */</span></span><br><span class="line">    <span class="number">0</span>,                                        <span class="comment">/* tp_weaklistoffset */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_iter */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_iternext */</span></span><br><span class="line">    pycal_methods,                            <span class="comment">/* tp_methods */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_members */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_getset */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_base */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_dict */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_descr_get */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_descr_set */</span></span><br><span class="line">    <span class="number">0</span>,                                        <span class="comment">/* tp_dictoffset */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_init */</span></span><br><span class="line">    <span class="literal">nullptr</span>,                                  <span class="comment">/* tp_alloc */</span></span><br><span class="line">    pycal_new                                 <span class="comment">/* tp_new */</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">PyInit_libsum</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">PyType_Ready</span>(&amp;DoubleWarpperType) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  PyObject *<span class="keyword">module</span> = <span class="built_in">PyModule_Create</span>(&amp;summodule);</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">module</span> == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 注册对象，这里为啥要引用+1？？</span></span><br><span class="line">  <span class="built_in">Py_INCREF</span>(&amp;DoubleWarpperType);</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">PyModule_AddObject</span>(<span class="keyword">module</span>, <span class="string">&quot;DoubleWarpper&quot;</span>,</span><br><span class="line">                         (PyObject *)&amp;DoubleWarpperType) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">Py_DECREF</span>(&amp;DoubleWarpperType);</span><br><span class="line">    <span class="built_in">Py_DECREF</span>(<span class="keyword">module</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">module</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>执行结果</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> array</span><br><span class="line"><span class="keyword">import</span> libsum</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Simple case:&quot;</span>)</span><br><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">c = libsum.<span class="built_in">sum</span>(a, b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;a&#125;</span> + <span class="subst">&#123;b&#125;</span> = <span class="subst">&#123;c&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Pass array:&quot;</span>)</span><br><span class="line">d = libsum.avg(array.array(<span class="string">&#x27;d&#x27;</span>,[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;avg = <span class="subst">&#123;d&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Define new struct:&quot;</span>)</span><br><span class="line">e = libsum.DoubleWarpper(<span class="number">1</span>);</span><br><span class="line">f = libsum.DoubleWarpper(<span class="number">2</span>);</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;e = <span class="subst">&#123;e&#125;</span>, f = <span class="subst">&#123;f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">e.<span class="built_in">set</span>(<span class="number">20</span>);</span><br><span class="line">f.<span class="built_in">set</span>(<span class="number">10</span>);</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;after set: e = <span class="subst">&#123;e&#125;</span>, f = <span class="subst">&#123;f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">g = e+f;</span><br><span class="line">h = e-f;</span><br><span class="line">i = e*f;</span><br><span class="line">j = e/f;</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;<span class="subst">&#123;e&#125;</span> + <span class="subst">&#123;f&#125;</span> = <span class="subst">&#123;g&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;<span class="subst">&#123;e&#125;</span> - <span class="subst">&#123;f&#125;</span> = <span class="subst">&#123;h&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;<span class="subst">&#123;e&#125;</span> * <span class="subst">&#123;f&#125;</span> = <span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;<span class="subst">&#123;e&#125;</span> / <span class="subst">&#123;f&#125;</span> = <span class="subst">&#123;j&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">(base) hua@hfc-ascend:~/code/share$ python py_sum.py</span><br><span class="line">Simple <span class="keyword">case</span>:</span><br><span class="line"><span class="number">1</span> + <span class="number">2</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">Pass array:</span><br><span class="line">avg = <span class="number">3.0</span></span><br><span class="line"></span><br><span class="line">Define new struct:</span><br><span class="line">e = <span class="number">1</span>, f = <span class="number">2</span></span><br><span class="line">after <span class="built_in">set</span>: e = <span class="number">20</span>, f = <span class="number">10</span></span><br><span class="line"><span class="number">20</span> + <span class="number">10</span> = <span class="number">30</span></span><br><span class="line"><span class="number">20</span> - <span class="number">10</span> = <span class="number">10</span></span><br><span class="line"><span class="number">20</span> * <span class="number">10</span> = <span class="number">200</span></span><br><span class="line"><span class="number">20</span> / <span class="number">10</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="pybind-11">pybind 11</h2>
<h3 id="简单例子">简单例子</h3>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Python.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pybind11/pybind11.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pybind11/stl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> py = pybind11;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123; <span class="keyword">return</span> a + b; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">avg</span><span class="params">(std::vector&lt;<span class="type">double</span>&gt; &amp;arr)</span> </span>&#123;</span><br><span class="line">  <span class="type">double</span> sum = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> it = arr.<span class="built_in">begin</span>(); it != arr.<span class="built_in">end</span>(); it++) &#123;</span><br><span class="line">    sum += (*it);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> (sum / arr.<span class="built_in">size</span>());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(libsum, m) &#123;</span><br><span class="line">    m.<span class="built_in">doc</span>() = <span class="string">&quot;Py module example.&quot;</span>;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;sum&quot;</span>, &amp;sum, <span class="string">&quot;Calculate the sum of two integers.&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;avg&quot;</span>, &amp;avg, <span class="string">&quot;alculate the avg of a double array.&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="自定义类型">自定义类型</h3>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DoubleWarpper</span> &#123;</span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="type">double</span> m_value;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">DoubleWarpper</span>(<span class="type">double</span> value) : <span class="built_in">m_value</span>(value) &#123;&#125;</span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">DoubleWarpper</span>() &#123; m_value = <span class="number">0</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">double</span> <span class="title">get</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> m_value; &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">set</span><span class="params">(<span class="type">double</span> value)</span> </span>&#123; m_value = value; &#125;</span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>+(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value + other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>-(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value - other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>*(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value * other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line">  DoubleWarpper <span class="keyword">operator</span>/(<span class="type">const</span> DoubleWarpper &amp;other) <span class="type">const</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">DoubleWarpper</span>(m_value / other.m_value);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(libsum, m) &#123;</span><br><span class="line">    m.<span class="built_in">doc</span>() = <span class="string">&quot;Py module example.&quot;</span>;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;sum&quot;</span>, &amp;sum, <span class="string">&quot;Calculate the sum of two integers.&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;avg&quot;</span>, &amp;avg, <span class="string">&quot;alculate the avg of a double array.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    py::<span class="built_in">class_</span>&lt;DoubleWarpper&gt;(m, <span class="string">&quot;DoubleWarpper&quot;</span>)</span><br><span class="line">      .<span class="built_in">def</span>(py::<span class="built_in">init</span>&lt;<span class="type">double</span>&gt;())</span><br><span class="line">      .<span class="built_in">def</span>(<span class="string">&quot;set&quot;</span>, &amp;DoubleWarpper::set)</span><br><span class="line">      .<span class="built_in">def</span>(<span class="string">&quot;get&quot;</span>, &amp;DoubleWarpper::get)</span><br><span class="line">      <span class="comment">//运算符重载</span></span><br><span class="line">      .<span class="built_in">def</span>(py::self + py::self)</span><br><span class="line">      .<span class="built_in">def</span>(py::self - py::self)</span><br><span class="line">      .<span class="built_in">def</span>(py::self * py::self)</span><br><span class="line">      .<span class="built_in">def</span>(py::self / py::self)</span><br><span class="line">      .<span class="built_in">def</span>(<span class="string">&quot;__repr__&quot;</span>,</span><br><span class="line">          [](DoubleWarpper &amp;warpper) &#123;</span><br><span class="line">            std::stringstream ss;</span><br><span class="line">            ss&lt;&lt;warpper.<span class="built_in">get</span>();</span><br><span class="line">            <span class="keyword">return</span> ss.<span class="built_in">str</span>().<span class="built_in">c_str</span>();</span><br><span class="line">          &#125;</span><br><span class="line">      );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="toml">toml</h2>
<p><strong>pyproject.toml</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pyproject.toml</span></span><br><span class="line">[build-system]</span><br><span class="line">requires = [<span class="string">&quot;setuptools&gt;=61.0&quot;</span>, <span class="string">&quot;cython&quot;</span>]</span><br><span class="line">build-backend = <span class="string">&quot;setuptools.build_meta&quot;</span></span><br><span class="line"></span><br><span class="line">[project]</span><br><span class="line">name        = <span class="string">&quot;libsum&quot;</span></span><br><span class="line">description = <span class="string">&quot;This is a libsum module&quot;</span></span><br><span class="line">version     = <span class="string">&quot;0.0.1&quot;</span></span><br><span class="line">readme      = <span class="string">&quot;README.md&quot;</span></span><br><span class="line">requires-python = <span class="string">&quot;&gt;=3.10&quot;</span></span><br><span class="line">authors = [</span><br><span class="line">  &#123; name=<span class="string">&quot;huafengchun&quot;</span>, email=<span class="string">&quot;huafengchun@huawei.com&quot;</span> &#125;,</span><br><span class="line">]</span><br><span class="line">classifiers = [</span><br><span class="line">    <span class="string">&quot;Programming Language :: Python :: 3&quot;</span>,</span><br><span class="line">    <span class="string">&quot;License :: OSI Approved :: MIT License&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Operating System :: OS Independent&quot;</span>,</span><br><span class="line">]</span><br><span class="line">keywords = [<span class="string">&quot;sum&quot;</span>, <span class="string">&quot;avg&quot;</span>]</span><br><span class="line"></span><br><span class="line">[project.urls]</span><br><span class="line"><span class="string">&quot;Homepage&quot;</span> = <span class="string">&quot;w3.huawei.com&quot;</span></span><br><span class="line"></span><br><span class="line">[tool.setuptools]</span><br><span class="line">py-modules = [<span class="string">&quot;_custom_build&quot;</span>]</span><br><span class="line"></span><br><span class="line">[tool.setuptools.cmdclass]</span><br><span class="line">build_py = <span class="string">&quot;_custom_build.build_py&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _custom_build.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> Extension</span><br><span class="line"><span class="keyword">from</span> setuptools.command.build_py <span class="keyword">import</span> build_py <span class="keyword">as</span> _build_py</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">build_py</span>(<span class="title class_ inherited__">_build_py</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        self.run_command(<span class="string">&quot;build_ext&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().run()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initialize_options</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().initialize_options()</span><br><span class="line">        <span class="keyword">if</span> self.distribution.ext_modules == <span class="literal">None</span>:</span><br><span class="line">            self.distribution.ext_modules = []</span><br><span class="line"></span><br><span class="line">        self.distribution.ext_modules.append(</span><br><span class="line"></span><br><span class="line">            Extension(</span><br><span class="line">                <span class="string">&quot;libsum&quot;</span>,</span><br><span class="line">                sources=[<span class="string">&quot;libsum.c&quot;</span>],</span><br><span class="line">                extra_compile_args=[<span class="string">&#x27;-g&#x27;</span>, <span class="string">&#x27;-I/home/hua/anaconda3/include/python3.10&#x27;</span>, <span class="string">&#x27;-I/home/hua/anaconda3/lib/python3.10/site-packages/pybind11/include&#x27;</span>],</span><br><span class="line">            )</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">python -m build</span><br><span class="line"></span><br><span class="line">base) hua@hfc-ascend:~/code/share$ ll</span><br><span class="line">total 2488</span><br><span class="line">drwxrwxr-x 7 hua hua    4096 Jun  5 09:20 ./</span><br><span class="line">drwxrwxr-x 5 hua hua    4096 May 30 11:08 ../</span><br><span class="line">drwxrwxr-x 4 hua hua    4096 Jun  5 09:17 build/</span><br><span class="line">-rw-rw-r-- 1 hua hua     716 Jun  5 09:16 _custom_build.py</span><br><span class="line">drwxrwxr-x 2 hua hua    4096 Jun  5 09:16 dist/</span><br><span class="line">-rw-rw-r-- 1 hua hua    9015 May 29 19:14 libsum.cc</span><br><span class="line">-rwxrwxr-x 1 hua hua 2485776 Jun  5 09:17 libsum.cpython-310-x86_64-linux-gnu.so*</span><br><span class="line">drwxrwxr-x 2 hua hua    4096 Jun  5 09:16 libsum.egg-info/</span><br><span class="line">-rw-rw-r-- 1 hua hua    1715 May 29 19:33 libsum_pybind11.cc</span><br><span class="line">drwxrwxr-x 2 hua hua    4096 Jun  5 09:16 __pycache__/</span><br><span class="line">-rw-rw-r-- 1 hua hua     660 Jun  5 09:16 pyproject.toml</span><br><span class="line">-rw-rw-r-- 1 hua hua     535 May 29 19:17 py_sum.py</span><br><span class="line">-rw-rw-r-- 1 hua hua     477 May 29 19:16 setup.py</span><br><span class="line">drwxrwxr-x 2 hua hua    4096 May 30 11:32 .vscode/</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/06/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/06/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-06 09:09:56" itemprop="dateCreated datePublished" datetime="2023-06-06T09:09:56+08:00">2023-06-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hipudding"
      src="https://dogecoin.com/assets/images/doge.svg">
  <p class="site-author-name" itemprop="name">hipudding</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hipudding" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hipudding" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:huafengchun@gmail.com" title="E-Mail → mailto:huafengchun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hipudding</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
