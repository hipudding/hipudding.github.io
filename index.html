<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="16x16" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="mask-icon" href="https://dogecoin.com/assets/images/doge.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>



<link rel="canonical" href="http://example.com/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>hipudding's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">hipudding's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hipudding"
      src="https://dogecoin.com/assets/images/doge.svg">
  <p class="site-author-name" itemprop="name">hipudding</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hipudding" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hipudding" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:huafengchun@gmail.com" title="E-Mail → mailto:huafengchun@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/31/OpenCV%E7%A4%BE%E5%8C%BA%E6%B4%9E%E5%AF%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/31/OpenCV%E7%A4%BE%E5%8C%BA%E6%B4%9E%E5%AF%9F/" class="post-title-link" itemprop="url">OpenCV社区洞察</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-31 15:37:43" itemprop="dateCreated datePublished" datetime="2025-08-31T15:37:43+08:00">2025-08-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:38:49" itemprop="dateModified" datetime="2025-01-23T15:38:49+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="opencv背景">OpenCV背景</h2>
<p>OpenCV是1998年在Intel公司内的CVL(计算机视觉库)项目，由Gary
Bradski发起，并由<a target="_blank" rel="noopener" href="https://github.com/vpisarev">Vadim
Pisarevsky</a>担任技术主管，于1999年开源，2000年首次公开发布。2008年OpenCV的核心成员加入<a
target="_blank" rel="noopener" href="http://www.willowgarage.com/">Willow Garage</a>和<a
target="_blank" rel="noopener" href="https://itseez3d.com/">Itseez</a>公司继续开发。Itseez公司在2016年被Intel收购，核心开发团队重回Intel。</p>
<p>目前主要由Intel公司赞助OpenCV核心开发团队，并且很多OpenCV的开发者是Intel的雇员。这是一个由Intel公司主导，OpenCV.org非盈利基金会运营的开源项目。2019年以来，核心开发团队由Intel，OpenCV中国团队和<a
target="_blank" rel="noopener" href="https://www.xperience.ai/">xperience.ai</a>公司组成。</p>
<h2 id="版本发布策略">版本发布策略</h2>
<p>目前每6个月发布一次版本，社区没有明确说明每个版本的生命周期。</p>
<p><strong>4.x版本</strong></p>
<table>
<thead>
<tr>
<th>版本号</th>
<th>发布时间</th>
<th>时间间隔</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.8.0</td>
<td>2023.6.29</td>
<td>\</td>
</tr>
<tr>
<td>4.7.0</td>
<td>2022.12.28</td>
<td>6个月</td>
</tr>
<tr>
<td>4.6.0</td>
<td>2022.6.7</td>
<td>6个月</td>
</tr>
<tr>
<td>4.5.5</td>
<td>2021.12.25</td>
<td>6个月</td>
</tr>
<tr>
<td>4.5.4</td>
<td>2021.10.10</td>
<td>2个月</td>
</tr>
<tr>
<td>4.5.3</td>
<td>2021.7.6</td>
<td>3个月</td>
</tr>
<tr>
<td>4.5.2</td>
<td>2021.4.3</td>
<td>3个月</td>
</tr>
<tr>
<td>4.5.1</td>
<td>2020.12.22</td>
<td>4个月</td>
</tr>
<tr>
<td>4.5.0</td>
<td>2020.10.12</td>
<td>2个月</td>
</tr>
</tbody>
</table>
<p>4.0.0版本发布于2018.11.18。</p>
<p><strong>3.x版本</strong></p>
<table>
<thead>
<tr>
<th>版本号</th>
<th>发布时间</th>
<th>时间间隔</th>
</tr>
</thead>
<tbody>
<tr>
<td>3.4.20</td>
<td>2023.6.27（仅打Tag）</td>
<td>\</td>
</tr>
<tr>
<td>3.4.19</td>
<td>2022.12.27（仅打Tag）</td>
<td>6个月</td>
</tr>
<tr>
<td>3.4.18</td>
<td>2022.6.5（仅打Tag）</td>
<td>6个月</td>
</tr>
<tr>
<td>3.4.17</td>
<td>2021.12.25（仅打Tag）</td>
<td>6个月</td>
</tr>
<tr>
<td>3.4.16</td>
<td>2021.10.10</td>
<td>2个月</td>
</tr>
<tr>
<td>3.4.15</td>
<td>2021.7.6</td>
<td>3个月</td>
</tr>
<tr>
<td>3.4.14</td>
<td>2021.4.2</td>
<td>3个月</td>
</tr>
<tr>
<td>3.4.13</td>
<td>2020.12.22</td>
<td>4个月</td>
</tr>
<tr>
<td>3.4.12</td>
<td>2020.10.12</td>
<td>2个月</td>
</tr>
</tbody>
</table>
<p>3.x版本最新的一个release版本3.4.16发布时间为2021.10，最后一个tag版本3.4.20发布时间为2021.6。</p>
<p>3.0发布于2015.6.24。</p>
<p><strong>2.x版本</strong></p>
<p>2.x版本的最新一个release版本是2.4.13.6，发布于2018.2.26，从发布时间上看，已经不再维护。</p>
<p><strong>社区没有明确说明每个大版本的支持周期</strong></p>
<h2 id="opencv基金会">OpenCV基金会</h2>
<h3 id="领导团队">领导团队</h3>
<ul>
<li><p>Gary Bradski (Itseez, Intel）</p></li>
<li><p>Anna Petrovicheva (Intel)</p></li>
<li><p>Vladimir Dudnik (Intel)</p></li>
<li><p>Stefano Fabri (Deeper)</p></li>
<li><p>Tatiana Khanova (Xperience.ai)</p></li>
<li><p>Satya Mallick (OpenCV CEO)</p></li>
<li><p>Vadim Pisarevsky (Huawei)</p></li>
<li><p>Vincent Rabaud (Google)</p></li>
<li><p>Edgar Riba (farm-ng)</p></li>
<li><p>Aleksandr Voron (N/A)</p></li>
</ul>
<p>领导团队（leadership meeting)，每周三 8:00 am, 太平洋时间，通过<a
target="_blank" rel="noopener" href="https://hangouts.google.com/">Hangouts</a>沟通，会议纪要记录在<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/Meeting_notes">github的wiki上</a>。</p>
<h3 id="开发团队和贡献者社区">开发团队和贡献者社区</h3>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/opencv/">github</a>社区活跃开发者</strong></p>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 31%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>姓名</th>
<th>社区职位</th>
<th>就职公司</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/asmorkalov">Alexander Smorkalov</a></td>
<td>活跃commitor，合入PR数量众多</td>
<td>Xperience.AI</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/vpisarev">Vadim Pisarevsky</a></td>
<td>OpenCV技术负责人</td>
<td>华为</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/alalek">Alexander Alekhin</a></td>
<td>活跃开发者，reviewer，commitor</td>
<td>Intel</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ilya-lavrenov">Ilya Lavrenov</a></td>
<td>活跃开发者</td>
<td>Itseez CTO</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/dkurt">Dmitry Kurtaev</a></td>
<td>活跃开发者，reviewer，commitor</td>
<td>YADRO</td>
</tr>
<tr>
<td>... ...</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>小粒度特性和bug fix可以用<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/issues">issue</a>跟踪，大粒度特性需要有<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/Evolution-Proposals">进化提案</a>跟踪。贡献社区需要参考OpenCV社区的<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/How_to_contribute">贡献指导</a>，所提交的代码需要符合社区编码<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/Coding_Style_Guide">规范</a>。</p>
<p><strong>领域主席</strong></p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 27%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr>
<th>领域</th>
<th>姓名</th>
<th>单位</th>
</tr>
</thead>
<tbody>
<tr>
<td>RISC-V</td>
<td>Mingjie Xing</td>
<td>中国科学院软件研究所</td>
</tr>
<tr>
<td>人脸识别与分析</td>
<td>Weihong Deng</td>
<td>北京邮电大学</td>
</tr>
<tr>
<td>人体检测</td>
<td>Andrea Pennisi</td>
<td>University of Antwerp</td>
</tr>
<tr>
<td>图像增强</td>
<td>Zhangyang "Atlas" Wang</td>
<td>The University of Texas at Austin</td>
</tr>
<tr>
<td>形状检测</td>
<td>Qi Jia</td>
<td>大连理工大学</td>
</tr>
<tr>
<td>文档</td>
<td>Dr. Vikas Ramachandra</td>
<td>Columbia University in the City of New York</td>
</tr>
<tr>
<td>辅助技术</td>
<td>Jagadish Mahendran</td>
<td>Bovi.ag</td>
</tr>
</tbody>
</table>
<p><strong>官方<a target="_blank" rel="noopener" href="https://forum.opencv.org/">论坛</a></strong></p>
<p>社区交流可以在官方论坛上互动。</p>
<h3 id="opencv合作伙伴">OpenCV合作伙伴</h3>
<ul>
<li>Intell, OpenCV 白金会员</li>
<li>黄金会员：Microsoft Azure, Google summer of Code, FUTUREWEI,
华为（成为黄金会员方式：捐献十万美金，开发者或者其他资源）</li>
<li>发展合作伙伴：KHADAS, ORBBEC, RunPod</li>
</ul>
<p>合作联系方式：contact@opencv.ai（美国），admin@opencv.org.cn
（中国）</p>
<h3 id="opencv中国团队">OpenCV中国团队</h3>
<p>OpenCV中国团队于2019年9月成立，
非营利目的，致力于OpenCV的开发、维护和推广工作。OpenCV中国团队由OpenCV项目发起人Gary
Bradski担任团队顾问，OpenCV技术负责人<a
target="_blank" rel="noopener" href="https://github.com/vpisarev">Vadim
Pisarevsky</a>领导技术开发，OpenCV中文社区创始人于仕琪博士担任团队负责人。</p>
<p>国内负责人和核心开发成员均为<a
target="_blank" rel="noopener" href="https://faculty.sustech.edu.cn/?cat=2&amp;tagid=yusq&amp;orderby=date&amp;iscss=1&amp;snapid=1&amp;go=1">于仕琪博士团队</a>，并且是于仕琪的研究助理，主要社区提交为DNN相关内容。</p>
<table>
<colgroup>
<col style="width: 3%" />
<col style="width: 13%" />
<col style="width: 28%" />
<col style="width: 37%" />
<col style="width: 5%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr>
<th>姓名</th>
<th>职位</th>
<th>github id</th>
<th>OpenCV贡献</th>
<th>社区职位</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>吴佳</td>
<td>研究助理</td>
<td><a target="_blank" rel="noopener" href="https://github.com/kaingwade">kaingwade</a></td>
<td><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pull/23531">1PR</a>
<font color=green>38 ++</font> <font color=red>0--</font></td>
<td>无</td>
<td></td>
</tr>
<tr>
<td>母自豪</td>
<td>研究助理，2018级研究生</td>
<td><a target="_blank" rel="noopener" href="https://github.com/zihaomu">zihaomu</a></td>
<td><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pulls?q=is%3Apr+is%3Aclosed+author%3Azihaomu">79PR</a>
<font color=green>42,381 ++</font> <font color=red>22,986 --</font></td>
<td>reviewer</td>
<td></td>
</tr>
<tr>
<td>冯远滔</td>
<td>研究助理，2018级研究生</td>
<td><a target="_blank" rel="noopener" href="https://github.com/fengyuentau">fengyuentau</a></td>
<td><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pulls?q=is%3Apr+is%3Aclosed+author%3Afengyuentau">51PR</a>
<font color=green>7,863 ++</font> <font color=red>2,053 --</font></td>
<td>reviewer</td>
<td>DNN支持CANN后端作者</td>
</tr>
<tr>
<td>钟万里</td>
<td>研究助理</td>
<td><a target="_blank" rel="noopener" href="https://github.com/WanliZhong">WanliZhong</a></td>
<td><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/pulls?q=is%3Apr+is%3Aclosed+author%3AWanliZhong">18PR</a>
<font color=green>561 ++</font> <font color=red>147 --</font></td>
<td>reviewer</td>
<td></td>
</tr>
</tbody>
</table>
<p>OpenCV欢迎外部公司<a
target="_blank" rel="noopener" href="https://www.opencv.org.cn/?page_id=56">合作</a>，可由OpenCV中国团队指导，外部公司软件工程师开发，提交patch到OpenCV项目。可联系中国团队（admin@opencv.org.cn）洽谈。其中提到了在不同硬件平台上的OpenCV的加速，契合昇腾使能的诉求。</p>
<h3 id="社区运作方式">社区运作方式</h3>
<ul>
<li>代码仓库和版本控制：OpenCV代码托管在<a
target="_blank" rel="noopener" href="https://github.com/opencv">github</a>上，使用git做版本管理，社区成员在这些仓库中提交代码和PR，有reviewer进行代码review，并最终由commitor合入代码。OpenCV有三个主要代码库：
<ul>
<li>opencv：opencv主要代码库，包含关键数据结构和成熟算法，HAL方式在此库以3rd_party的方式贡献</li>
<li>opencv_contrib：opencv扩展模块库，依赖opencv_core，社区提交要求先进入此库，成熟后合入主库。cuda等算法均在此库，命名空间方式以独立模块方式在此库贡献</li>
<li>opencv_extra：opencv扩展数据库，存放测试数据，供测试使用</li>
</ul></li>
<li>讨论和沟通：OpenCV社区使用（邮件列表？未找到）<a
target="_blank" rel="noopener" href="https://github.com/opencv">github</a>，<a
target="_blank" rel="noopener" href="https://forum.opencv.org/">论坛</a>进行技术讨论和沟通</li>
<li>问题追踪和解决：OpenCV使用github issue进行问题追踪：
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv/issues">opencv</a> -
库和稳定模块的一般问题，与构建相关的问题</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv_contrib/issues">opencv_contrib</a>
-实验模块及其依赖项的问题</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/opencv-infrastructure/opencv.org/issues">opencv.org</a>
- 官方网站的问题</li>
</ul></li>
<li>文档和教程：最新的版本使用<a
target="_blank" rel="noopener" href="https://docs.opencv.org/">Doxygen</a>来展示文档，其中包含了<a
target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d9/df8/tutorial_root.html">使用教程</a></li>
<li>社区活动和会议：开发者可以参与<a
target="_blank" rel="noopener" href="https://github.com/opencv/opencv/wiki/GSoC_2023">谷歌代码之夏</a>活动来提交新的想法和代码，社区领导团队每周三太平洋时间8:00
am在环聊上开展会议，并归档到<a
href="(https://github.com/opencv/opencv/wiki/Meeting_notes)">wiki</a>上。</li>
<li>教育和培训：社区联合<a
target="_blank" rel="noopener" href="https://bigvision.ai/">Bigision</a>提供了许多在线<a
target="_blank" rel="noopener" href="https://opencv.org/university/">课程</a>（收费），完成学习后可以获得毕业证书以及优秀证书，针对企业和组织也提供了培训计划。</li>
</ul>
<h2 id="昇腾接入方式">昇腾接入方式</h2>
<h3 id="贡献开源方式">贡献开源方式</h3>
<ol type="1">
<li>HAL：OpenCV在core，imgproc，feature2d这三个模块都提供了HAL机制，通过include
<code>hal_replacement.hpp</code>替换相关宏定义，决定真正的执行后端。</li>
<li>命名空间：使用独立模块，实现与cv相同的函数接口。使用命名空间的方式进行调用。</li>
</ol>
<p><strong>两种接入方式比较</strong></p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 45%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr>
<th>比较项</th>
<th>HAL</th>
<th>命名空间</th>
</tr>
</thead>
<tbody>
<tr>
<td>实现成本</td>
<td>低，进需要实现定义好的函数接口即可，不涉及Matrix对象，入参出参均为常见数据类型。</td>
<td>较高，需要自定义实现NPU上的Mat结构，代码生成等相关工作。</td>
</tr>
<tr>
<td>用户使用成本</td>
<td>对用户完全透明，无缝替换。</td>
<td>用户需要替换函数调用接口，有可能涉及Mat对象转换NPU
Mat对象操作。</td>
</tr>
<tr>
<td>性能</td>
<td>每个算子执行前后均涉及内存数据搬迁，无法异步执行，效率低。</td>
<td>计算过程中算子无需搬迁，可使用异步执行，效率高。</td>
</tr>
<tr>
<td>现有实现</td>
<td>carotene，Nvdia实现的SIMD加速库。</td>
<td>CUDA加速。</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/23/Ray%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8-%E7%BC%96%E8%AF%91%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BB%BB%E5%8A%A1%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/23/Ray%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8-%E7%BC%96%E8%AF%91%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BB%BB%E5%8A%A1%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/" class="post-title-link" itemprop="url">Ray技术入门-编译部署和任务生命周期</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-01-23 11:39:15 / 修改时间：15:17:18" itemprop="dateCreated datePublished" datetime="2025-01-23T11:39:15+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="背景知识">背景知识</h2>
<p>Ray是一个使用Bazel构建的，基于gRPC上层打造的开源分布式计算框架，旨在简化分布式应用的开发和运行。它支持无缝地将
Python 代码扩展到多核、多节点环境，适合构建高性能的分布式系统。Ray
提供灵活的任务调度和状态管理，支持多种编程模型，包括任务并行和 actor
模式，并通过自动化的资源管理和容错机制简化复杂分布式工作的部署。它还拥有丰富的生态系统，包含机器学习库（如
Ray Tune、Ray Serve 和
RLlib），适用于模型训练、超参数调优、在线服务等场景，是云原生应用和大规模计算的理想选择。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/ray-project/ray">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://www.ray.io/">主页</a></strong></p>
<h3 id="bazel">Bazel</h3>
<p>Bazel是一种高效、可扩展的构建工具，最初由Google开发，专为管理大型代码库和复杂项目而设计。它支持多语言和多平台构建，包括C++,
Java, Python,
Go等，并能够跨操作系统（如Linux、macOS和Windows）执行构建任务。Bazel通过声明式的构建规则（BUILD文件）和依赖管理，实现了高性能的增量构建，避免了不必要的重复编译。其特点包括分布式构建、沙盒化执行和强大的缓存机制，可以加快构建速度并提高构建的稳定性。此外，Bazel还提供高度可配置的扩展机制，方便开发者为特定需求编写自定义规则，适合从小型项目到超大规模工程的使用场景。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/bazelbuild/bazel">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://bazel.build">主页</a></strong></p>
<h3 id="grpc">gRPC</h3>
<p>gRPC 是由 Google 开发的高性能开源 RPC 框架，基于 HTTP/2
协议，支持多语言和跨平台通信。它使用 Protocol Buffers
定义接口和序列化数据，简化了服务间的集成开发。gRPC
提供高效的请求-响应模型、流式传输、负载均衡和内置 TLS
安全特性，非常适合云原生应用、微服务架构和实时通信场景，广泛应用于分布式系统和高性能应用开发中。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/grpc/grpc">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://grpc.io/">主页</a></strong></p>
<h3 id="cython">cython</h3>
<p>Cython 是一种优化的 Python 扩展语言，结合了 Python 的易用性和 C
的高性能，旨在提升 Python 代码的运行速度。通过将 Python 代码转译为 C 或
C++ 并进行编译，Cython 可以显著减少运行时的性能开销，同时支持调用 C/C++
库，从而实现与底层代码的高效交互。Cython 保留了大部分 Python
的语法，同时允许使用 C
类型声明进行性能优化，非常适合计算密集型任务或对性能要求较高的场景，如科学计算、机器学习和数据处理。</p>
<p><strong><a
target="_blank" rel="noopener" href="https://github.com/cython/cython">社区</a></strong>，<strong><a
target="_blank" rel="noopener" href="https://cython.org/">主页</a></strong></p>
<h2 id="角色">角色</h2>
<p>Ray集群的整体架构如下图所示</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/06_Ray架构图-20250121160411114.png"
alt="Ray架构图" />
<figcaption aria-hidden="true">Ray架构图</figcaption>
</figure>
<p>一个Ray集群包括多个Node节点，其中每个Node节点包含Actor，Worker，共享内存，本地调度器。其中Head
Node还有GCS服务，包含各类元数据存储，WebUI，全局调度等功能。</p>
<h3 id="node">Node</h3>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/v2-cf09ec3ccc183b816fa303164f4144b3_1440w.jpg"
alt="Ray Nodes" />
<figcaption aria-hidden="true">Ray Nodes</figcaption>
</figure>
<p>Node中包含一个Raylet进程，负责本地调度以及共享内存。Raylet会根据任务情况启动一个或者多个worker或者Actor。Head
node是一个特殊的Node，除了普通Node的功能之外，还有一些外的进程，包括gcs_server服务，dashboard等。并且每个Node节点还会启动monitor进程，log_monitor进程，agent进程等。</p>
<p>Raylet和gcs_server是非Python进程，其他辅助进程，包括Driver，worker以及Actor均是python进程（针对Python语言而言）。</p>
<h3 id="gcs_server">gcs_server</h3>
<p>Ray 的 GCS Server 是 Ray
框架的核心组件，负责元数据存储、任务调度、资源管理和集群状态维护。它通过存储模块（Redis
或 内存）管理节点和任务的生命周期，使用 Pub-Sub
系统进行状态广播，并通过高效的调度协调与其他组件（如 Scheduler, Worker,
Actor 或
Object）协作，确保系统的高性能、高可用性和灵活扩展性。gcs_server是一个非Python进程，二进制文件路径在
<code>ray/python/ray/core/src/ray/gcs/gcs_server</code>。</p>
<h3 id="raylet">raylet</h3>
<p>Raylet 是 Ray
集群中每个节点的核心运行时组件，负责任务执行、资源管理和数据依赖协调。它接收
GCS Server 分配的任务，管理本地资源，启动 Worker 进程执行任务，并通过
Plasma Store 处理数据存储与传输。同时，Raylet 定期向 GCS Server
汇报节点状态，协作实现任务调度、资源分配和故障恢复，是 Ray
分布式运行的关键执行单元。raylet是一个非Python进程，二进制文件路径在
<code>ray/python/ray/core/src/ray/raylet/raylet</code>。</p>
<h3 id="worker">worker</h3>
<p>Worker 是 Ray 中的核心计算单元，由 Raylet 启动，负责具体任务的执行和
Actor 的运行。它与 Plasma Store 协作进行数据存取，并通过与 Raylet
的通信完成任务调度和资源管理。Worker 支持多语言运行环境（如
Python、Java），能够高效并行处理任务，是 Ray
框架实现分布式计算的基础组件。</p>
<h3 id="actor">actor</h3>
<p>Actor 是 Ray
框架中的一种状态管理单元，允许用户在分布式系统中创建带有持久状态的计算对象。每个
Actor 由一个独立的 Worker
进程运行，支持并行调用方法并维护自身状态。Actor
可以通过远程调用接口与其他组件交互，实现任务分解和动态扩展，是 Ray
中用于构建有状态应用和分布式服务的重要抽象。</p>
<h3 id="driver">driver</h3>
<p>Driver就是用户程序（例如，用@ray.remote修饰的用户python代码），Driver负责Task的定义和提交，需要运行在Ray的Head或者Node节点上。</p>
<h2 id="编译">编译</h2>
<p>安装Ray有多种方法，包括wheel包，pip，conda，容器镜像等。这些内容可以参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/ray-overview/installation.html">社区手册</a>。这里介绍从<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/ray-contribute/development.html#building-ray">源码安装</a>。</p>
<h3 id="使用conda环境">使用Conda环境</h3>
<p>官方推荐conda或者venv两种虚拟环境安装ray，建议选择conda。无虚拟环境将无法编译ray，并且在实测中发现了venv的未知错误。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -c conda-forge python=3.9 -n myenv</span><br><span class="line">conda activate myenv</span><br></pre></td></tr></table></figure>
<h3 id="安装依赖">安装依赖</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y build-essential curl clang pkg-config psmisc unzip</span><br></pre></td></tr></table></figure>
<p><strong>安装bazel</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ci/env/install-bazel.sh</span><br></pre></td></tr></table></figure>
<p><strong>安装npm</strong></p>
<p>用于dashboard</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">(curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh)</span></span><br><span class="line">nvm install 14</span><br><span class="line">nvm use 14</span><br></pre></td></tr></table></figure>
<h3 id="构建">构建</h3>
<p><strong>构建dashboard</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ray/python/ray/dashboard/client</span><br><span class="line">npm ci</span><br><span class="line">npm run build</span><br></pre></td></tr></table></figure>
<p><strong>构建ray</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd ../../..</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#如果构建机器的内存小于32G，需要限制内存使用，避免oom</span></span></span><br><span class="line">export BAZEL_ARGS=&quot;--local_ram_resources=8&quot;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#debug编译，保留符号表供调试</span></span></span><br><span class="line">export RAY_DEBUG_BUILD=debug</span><br><span class="line">pip install -e . --verbose</span><br></pre></td></tr></table></figure>
<h3 id="可选的编译环境变量">可选的编译环境变量</h3>
<ul>
<li><code>RAY_INSTALL_JAVA</code>: If set and equal to <code>1</code>,
extra build steps will be executed to build java portions of the
codebase</li>
<li><code>RAY_INSTALL_CPP</code>: If set and equal to <code>1</code>,
<code>ray-cpp</code> will be installed</li>
<li><code>RAY_DISABLE_EXTRA_CPP</code>: If set and equal to
<code>1</code>, a regular (non - <code>cpp</code>) build will not
provide some <code>cpp</code> interfaces</li>
<li><code>SKIP_BAZEL_BUILD</code>: If set and equal to <code>1</code>,
no Bazel build steps will be executed</li>
<li><code>SKIP_THIRDPARTY_INSTALL</code>: If set will skip installation
of third-party python packages</li>
<li><code>RAY_DEBUG_BUILD</code>: Can be set to <code>debug</code>,
<code>asan</code>, or <code>tsan</code>. Any other value will be
ignored</li>
<li><code>BAZEL_ARGS</code>: If set, pass a space-separated set of
arguments to Bazel. This can be useful for restricting resource usage
during builds, for example. See https://bazel.build/docs/user-manual for
more information about valid arguments.</li>
<li><code>IS_AUTOMATED_BUILD</code>: Used in CI to tweak the build for
the CI machines</li>
<li><code>SRC_DIR</code>: Can be set to the root of the source checkout,
defaults to <code>None</code> which is <code>cwd()</code></li>
<li><code>BAZEL_SH</code>: used on Windows to find a
<code>bash.exe</code>, see below</li>
<li><code>BAZEL_PATH</code>: used on Windows to find
<code>bazel.exe</code>, see below</li>
<li><code>MINGW_DIR</code>: used on Windows to find
<code>bazel.exe</code> if not found in <code>BAZEL_PATH</code></li>
</ul>
<h2 id="启动一个ray集群">启动一个Ray集群</h2>
<p>接下来使用一个简单的例子来使用Ray，这是一个使用概率计算圆周率π的程序（蒙特卡洛法）。蒙特卡洛方法在计算圆周率时设一个正方形内部相切一个圆，这时圆和正方形的面积之比是π/4。在这个正方形内部，随机产生n个点（这些点服从均匀分布），计算它们与中心点的距离是否大于圆的半径，以此判断是否落在圆的内部。统计圆内的点数，与n的比值乘以4，就是π的值。理论上，n越大，计算的π值越精确。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/3d5d171ad6df43c2babfe981a2ee91e8.jpeg"
alt="蒙特卡洛法" />
<figcaption aria-hidden="true">蒙特卡洛法</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/4f050b95b2c1439baa33a66de817e659.jpeg"
alt="蒙特卡洛法" />
<figcaption aria-hidden="true">蒙特卡洛法</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ray</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Ray</span></span><br><span class="line">ray.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个任务：生成点并统计圆内点数</span></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_points_in_circle</span>(<span class="params">num_samples: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">        x, y = random.uniform(<span class="number">0</span>, <span class="number">1</span>), random.uniform(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> x**<span class="number">2</span> + y**<span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_pi</span>(<span class="params">num_samples: <span class="built_in">int</span>, num_workers: <span class="built_in">int</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="comment"># 每个 worker 分配的样本数</span></span><br><span class="line">    samples_per_worker = num_samples // num_workers</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建并运行任务</span></span><br><span class="line">    futures = [</span><br><span class="line">        count_points_in_circle.remote(samples_per_worker) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_workers)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 收集结果</span></span><br><span class="line">    total_in_circle = <span class="built_in">sum</span>(ray.get(futures))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算圆周率</span></span><br><span class="line">    pi_estimate = <span class="number">4</span> * total_in_circle / num_samples</span><br><span class="line">    <span class="keyword">return</span> pi_estimate</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 总样本数和并行任务数</span></span><br><span class="line">    total_samples = <span class="number">100_000_000</span></span><br><span class="line">    num_workers = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 π</span></span><br><span class="line">    pi = calculate_pi(total_samples, num_workers)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Estimated π: <span class="subst">&#123;pi&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭 Ray</span></span><br><span class="line">    ray.shutdown()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="临时启动">临时启动</h3>
<p>如果不启动Ray集群，直接执行该Python程序，那会在当前节点上默认拉起一个ray集群供计算。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python pi.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 09:35:24,180 INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Estimated π: 3.12</span></span></span><br></pre></td></tr></table></figure>
<h3 id="启动集群">启动集群</h3>
<p><strong>启动Head节点</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">ray start --head</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Local node IP: 192.168.64.8</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Ray runtime started.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Next steps</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To add another node to this Ray cluster, run</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray start --address=&#x27;192.168.64.8:6379&#x27;</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To connect to this Ray cluster:</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    import ray</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray.init()</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To submit a Ray job using the Ray Jobs CLI:</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    RAY_ADDRESS=&#x27;http://127.0.0.1:8265&#x27; ray job submit --working-dir . -- python my_script.py</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  for more information on submitting Ray jobs to the Ray cluster.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To terminate the Ray runtime, run</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray stop</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To view the status of the cluster, use</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    ray status</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  To monitor and debug Ray, view the dashboard at</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#    127.0.0.1:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  If connection to the dashboard fails, check your firewall settings and network configuration.</span></span></span><br></pre></td></tr></table></figure>
<p>可以在<code>http://127.0.0.1:8265</code>查看Ray控制台。</p>
<p><strong>启动Node节点</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ray start --address=&#x27;192.168.64.8:6379&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Local node IP: 192.168.64.9</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#[2025-01-21 10:49:34,903 W 1882 1882] global_state_accessor.cc:463: Retrying to get node with node ID ba8aafea9f23f6f29ff6cd174e31aaac37cddb0e832c4e3170ddcf63</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Ray runtime started.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#--------------------</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#To terminate the Ray runtime, run</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  ray stop</span></span></span><br></pre></td></tr></table></figure>
<p><strong>集群状态</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">======== Autoscaler status: 2025-01-21 10:54:25.170247 ========</span><br><span class="line">Node status</span><br><span class="line">---------------------------------------------------------------</span><br><span class="line">Active:</span><br><span class="line"> 1 node_948847515a43d4fba13c1bdb6a5e5611c2580ecb60f60183ef033771</span><br><span class="line"> 1 node_ba8aafea9f23f6f29ff6cd174e31aaac37cddb0e832c4e3170ddcf63</span><br><span class="line">Pending:</span><br><span class="line"> (no pending nodes)</span><br><span class="line">Recent failures:</span><br><span class="line"> (no failures)</span><br><span class="line"></span><br><span class="line">Resources</span><br><span class="line">---------------------------------------------------------------</span><br><span class="line">Usage:</span><br><span class="line"> 0.0/16.0 CPU</span><br><span class="line"> 0B/8.40GiB memory</span><br><span class="line"> 0B/3.93GiB object_store_memory</span><br><span class="line"></span><br><span class="line">Demands:</span><br><span class="line"> (no resource demands)</span><br></pre></td></tr></table></figure>
<h3 id="启用监控">启用监控</h3>
<p><strong>Prometheus</strong></p>
<p>ray提供了一个命令来下载和部署普罗米修斯，ray提供了数据采集接口，可以让普罗米修斯通过这个接口来收集集群数据，注意，简易命令拉起的普罗米修斯不能用于生产环境，完整部署可参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/cluster/metrics.html#optional-manual-running-prometheus-locally">官方手册</a>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ray metrics launch-prometheus</span><br></pre></td></tr></table></figure>
<p>可以在这个地址上查看普罗米修斯状态：http://localhost:9090，可查看其采集的信息<code>ray_dashboard_api_requests_count_requests_total</code>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121190550385.png"
alt="普罗米修斯dashboard" />
<figcaption aria-hidden="true">普罗米修斯dashboard</figcaption>
</figure>
<p><strong>grafana</strong></p>
<p>普罗米修斯采集的数据，通过grafana的方式进行可视化显示，并且ray
dashboard中的metric页面的信息也是来自于grafana。可以通过启动新的grafana服务来完成配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/share/grafana</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#启动grafana需要创建data目录，需要sudo执行</span></span></span><br><span class="line">sudo ./bin/grafana-server --config /tmp/ray/session_latest/metrics/grafana/grafana.ini web</span><br></pre></td></tr></table></figure>
<p>将grafana dashboard加入到已有的grafana server可以参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/cluster/metrics.html#simplest-setting-up-grafana-with-ray-provided-configurations">官方手册</a>。</p>
<p>可以在这个地址上查看grafana的dashboard：http://localhost:3000</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191218231.png"
alt="grafana dashboard" />
<figcaption aria-hidden="true">grafana dashboard</figcaption>
</figure>
<p><strong>Ray dashboard</strong></p>
<p>完成上述两个步骤后，Ray
dashboard中的metric即可正常显示，如果不是本机部署，你可能需要配置允许所有ip访问：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RAY_GRAFANA_HOST=http://192.168.64.8:3000 ray start --head --dashboard-host=0.0.0.0</span><br></pre></td></tr></table></figure>
<p><code>RAY_GRAFANA_HOST</code>的作用是让ray的dashboard能够访问到grafana服务；</p>
<p><code>--dashboard-host=0.0.0.0</code>允许所有ip访问。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191317452.png"
alt="Ray dashboard" />
<figcaption aria-hidden="true">Ray dashboard</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191504505.png"
alt="Ray dashboard" />
<figcaption aria-hidden="true">Ray dashboard</figcaption>
</figure>
<h3 id="提交一个任务">提交一个任务</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python pi.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:17:11,760 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 192.168.64.8:6379...</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:17:11,775 INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at 192.168.64.8:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Estimated π: 3.36</span></span></span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191829305.png"
alt="Ray 任务列表" />
<figcaption aria-hidden="true">Ray 任务列表</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191902742.png"
alt="Ray 任务详情" />
<figcaption aria-hidden="true">Ray 任务详情</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121191917649.png"
alt="Ray 任务详情" />
<figcaption aria-hidden="true">Ray 任务详情</figcaption>
</figure>
<h3 id="部署一个服务">部署一个服务</h3>
<p>Ray除了提供基础的分布式计算能力之外，还提供了一系列的AI
libs，其中可以在其上部署服务，Ray自动提供proxy和负载均衡能力。这里使用一个翻译的服务举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> starlette.requests <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ray</span><br><span class="line"><span class="keyword">from</span> ray <span class="keyword">import</span> serve</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@serve.deployment(<span class="params">num_replicas=<span class="number">2</span>, ray_actor_options=&#123;<span class="string">&quot;num_cpus&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;num_gpus&quot;</span>: <span class="number">0</span>&#125;</span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Translator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Load model</span></span><br><span class="line">        self.model = pipeline(<span class="string">&quot;translation_en_to_fr&quot;</span>, model=<span class="string">&quot;t5-small&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">translate</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># Run inference</span></span><br><span class="line">        model_output = self.model(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Post-process output to return only the translation text</span></span><br><span class="line">        translation = model_output[<span class="number">0</span>][<span class="string">&quot;translation_text&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> translation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, http_request: Request</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        english_text: <span class="built_in">str</span> = <span class="keyword">await</span> http_request.json()</span><br><span class="line">        <span class="keyword">return</span> self.translate(english_text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">translator_app = Translator.bind()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    ray.init()</span><br><span class="line">    serve.start(http_options=&#123;<span class="string">&quot;host&quot;</span>: <span class="string">&quot;0.0.0.0&quot;</span>&#125;)  <span class="comment"># 设置监听地址为 0.0.0.0</span></span><br><span class="line">    serve.run(translator_app)</span><br></pre></td></tr></table></figure>
<p>具体修改方法，可以参考<a
target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/serve/getting_started.html">官方手册</a></p>
<p>直接运行这个python程序即可完成服务的部署：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">python translate.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:23:45,794 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 192.168.64.8:6379...</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#2025-01-21 11:23:45,810 INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at 192.168.64.8:8265</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:46,673 serve 8302 -- Started Serve in namespace &quot;serve&quot;.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:46,675 serve 8302 -- Connecting to existing Serve app in namespace &quot;serve&quot;. New http options will not be applied.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#WARNING 2025-01-21 11:23:46,675 serve 8302 -- The new client HTTP config differs from the existing one in the following fields: [&#x27;host&#x27;, &#x27;location&#x27;]. The new HTTP config is ignored.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeController pid=6931) INFO 2025-01-21 11:23:46,687 controller 6931 -- Deploying new version of Deployment(name=&#x27;Translator&#x27;, app=&#x27;default&#x27;) (initial target replicas: 2).</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ProxyActor pid=8045) INFO 2025-01-21 11:23:46,644 proxy 192.168.64.8 -- Proxy starting on node 8e8707766c1fc9b7d838c24446c99440be5881c04ea44b6e4e83a7aa (HTTP port: 8000).</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ProxyActor pid=8045) INFO 2025-01-21 11:23:46,660 proxy 192.168.64.8 -- Got updated endpoints: &#123;&#125;.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ProxyActor pid=8045) INFO 2025-01-21 11:23:46,690 proxy 192.168.64.8 -- Got updated endpoints: &#123;Deployment(name=&#x27;Translator&#x27;, app=&#x27;default&#x27;): EndpointInfo(route=&#x27;/&#x27;, app_is_cross_language=False)&#125;.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeController pid=6931) INFO 2025-01-21 11:23:46,792 controller 6931 -- Adding 2 replicas to Deployment(name=&#x27;Translator&#x27;, app=&#x27;default&#x27;).</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeReplica:default:Translator pid=8044) Device set to use cpu</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:50,711 serve 8302 -- Application &#x27;default&#x27; is ready at http://0.0.0.0:8000/.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#INFO 2025-01-21 11:23:50,711 serve 8302 -- Deployed app &#x27;default&#x27; successfully.</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#(ServeReplica:default:Translator pid=2371, ip=192.168.64.9) Device set to use cpu</span></span></span><br></pre></td></tr></table></figure>
<p>在dashboard上可以看到服务的详情：</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121192836367.png"
alt="Ray Serve详情" />
<figcaption aria-hidden="true">Ray Serve详情</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250121192851056.png"
alt="Ray Serve metrics" />
<figcaption aria-hidden="true">Ray Serve metrics</figcaption>
</figure>
<p>通过curl命令可以验证服务运行情况：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://127.0.0.1:8000/ -H &quot;Content-Type: application/json&quot; -d &#x27;&quot;Hello world!&quot;&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#Bonjour monde!</span></span></span><br></pre></td></tr></table></figure>
<h2 id="调试">调试</h2>
<p>Ray是一个多进程，Python和C++混合调用的程序（以Python语言为例），调试上需要掌握一定的技巧。调试Python，Driver，以及自动拉起的gcs_server，raylet以及worker，actor的方法都不同。下面以VsCode为例。</p>
<h3 id="调试python">调试python</h3>
<p>Python调试与普通程序调试相同，直接点debug
python文件，或者配置launch.json即可。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Python: pi.py&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;debugpy&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/pi.py&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;console&quot;</span><span class="punctuation">:</span> <span class="string">&quot;integratedTerminal&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;justMyCode&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="调试driver">调试Driver</h3>
<p>Driver就是用户python程序，调试Driver的Python部分参考上一节，如果调试Driver的C++部分，需要调试python进程，前提是Ray是debug编译的，否则没有符号表无法调试。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Ray C++&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/home/hua/miniconda3/envs/myenv/bin/python3.9&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;$&#123;workspaceFolder&#125;/pi.py&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stopAtEntry&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;externalConsole&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;MIMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;setupCommands&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;为 gdb 启用整齐打印&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-enable-pretty-printing&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;将反汇编风格设置为 Intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-gdb-set disassembly-flavor intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="调试worker或者其他进程">调试Worker或者其他进程</h3>
<p>gcs_server，raylet，worker以及actor都是自动拉起的进程，调试的时候需要attach到这些进程上进行调试。</p>
<blockquote>
<p>注意，需要接触gdb attach的限制，永久接触方法如下：</p>
<p>sudo vi /etc/sysctl.d/10-ptrace.conf</p>
<p>kernel.yama.ptrace_scope = 0</p>
<p>sudo sysctl --system</p>
</blockquote>
<p>调试上述pi.py，在一个worker的情况下大概需要8G内存，否则会导致Ray
kill掉worker或者gdb异常退出。在调试worker过程中，为了方便，可以限制仅启动一个worker，在本地拉起的情况下，配置<code>ray.init(num_cpus=1)</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attach to worker&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;attach&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;processId&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;command:pickProcess&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/home/hua/miniconda3/envs/myenv/bin/python3.9&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;sourceFileMap&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;/proc/self/cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/home/hua/code/ray&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;setupCommands&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;为 gdb 启用整齐打印&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-enable-pretty-printing&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;将反汇编风格设置为 Intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-gdb-set disassembly-flavor intel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>worker和actor是python进程，gcs_server和reylet是非python进程，二进制在
ray/python/ray/core/src/ray/下。</p>
<h2 id="grpc流程">gRPC流程</h2>
<h3 id="grpc是什么">gRPC是什么</h3>
<p>简单来说，RPC框架就是像调用本地函数一样调用远程函数。gRPC使用protobuf来定义服务和传输的对象，在客户端中，有一个存根（Stub），与服务有相同的函数签名，通过调用这个存根，即可完成一次RPC调用。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/landing-2.svg"
alt="gRPC原理" />
<figcaption aria-hidden="true">gRPC原理</figcaption>
</figure>
<p>Ray是基于gRPC构建的分布式计算系统，有关gRPC的代码存放在
ray/src/ray/rpc目录下，下面，我们通过worker进程的gRPC服务来分析。</p>
<h3 id="grpc-client">gRPC client</h3>
<p>涉及到gRPC client的几个文件：<code>grpc_client.h</code>,
<code>client_call.h</code></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/20250122145957615.png"
alt="gRPC client类图" />
<figcaption aria-hidden="true">gRPC client类图</figcaption>
</figure>
<p><code>ClientCall</code>是对RPC调用的一个封装，主要包括需要调用的stub函数指针，以及相关的状态和结果获取，当gRPC调用返回时，需要回调<code>ClientCall</code>中注册的回调函数。<code>ClientCallManager</code>是对gRPC调用发起的管理，包括结果队列，监听线程等，
<code>GrpcClient</code>保存的是gRPC的连接句柄，可以通过该对象发起一个gRPC请求。</p>
<p>对于CoreWorker来说，在此之上还有一层封装（<code>worker/core_worker_client.h</code>,
<code>worker/core_worker_client_pool.h/cc</code>），<code>CoreWorkerClient</code>，该类封装了CoreWorkerService可用的所用调用，直接调用提供的函数接口即可完成RPC调用。与之匹配的还有一个<code>CoreWorkerClientPool</code>，用于<code>CoreWorkerClient</code>的缓存。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250122142401278.png"
alt="CoreWorkerClient 类图" />
<figcaption aria-hidden="true">CoreWorkerClient 类图</figcaption>
</figure>
<p><code>CoreWorkerClientPool</code>维护一个map&lt;WorkerId,
CoreWorkerClient&gt;，当已经存在对应的<code>CoreWorkerClient</code>时直接取出使用。如果不存在，则调用<code>CoreWorkerClientFactoryFn</code>工厂方法创建一个gRPC的client连接。该工厂方法在<code>CoreWorker</code>的构造函数中定义，通过一个<code>rpc::Address</code>创建对应的<code>CoreWorkerClient</code>对象。</p>
<p>每个<code>CoreWorkerClient</code>对象构造过程中，会创建gRPC连接，并且通过<code>ClientCallManager</code>来发起RPC请求，并通过监听CompletionQueue来响应RPC的处理结果。</p>
<h3 id="grpc-server">gRPC server</h3>
<p>涉及到gRPC server的几个文件：<code>grpc_server.h/cc</code>,
<code>client_server.h/cc</code></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/20250122145823773.png"
alt="gPRC Server类图" />
<figcaption aria-hidden="true">gPRC Server类图</figcaption>
</figure>
<p><code>GrpcServer</code>是gRPC的服务端，其中定义了初始化，关闭，注册服务，运行等操作。它会根据其中注册的Service来向gRPC服务中注册服务和对应的处理方法。<code>GrpcService</code>是一个虚拟类，其本身没有实现，需要不同的组件来继承实现，例如，CoreWorker就会用<code>CoreWorkerGrpcService</code>来实现一个Worker对应的Service。Service中需要提供一组<code>ServiceCallFactory</code>，这些Factory记录了服务，gRPC的stub，回调函数，本地异步IO组件等信息，供<code>GrpcServer</code>来注册对应的服务。<code>ServiceCall</code>即服务端服务的本身，包括一系列回调函数处理对应的事件，这个call对象会以Tag的方式放入gRPC请求中，处理时取出call对象对相应的处理。</p>
<p>对于CoreWorker来说，需要基于<code>GrpcService</code>实现<code>GrpcCoreWorkerGrpcService</code>（<code>work/core_worker_server.h</code>）。实际上的工作就是将CoreWorkerService中的服务全部注册到ServiceCallFactory中。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20250122152145818.png"
alt="CoreWorkerService 类图" />
<figcaption aria-hidden="true">CoreWorkerService 类图</figcaption>
</figure>
<p><code>CoreWorkerServiceHandler</code>是一组Handle方法的集合，包含CoreWorkerService中的所有服务的处理方法，<code>CoreWorkerGrpcService</code>中会通过一组宏来构造protobuf中的注册，响应等必要信息的对象集合（ServiceCallFactory）。</p>
<p>注册完成后，<code>GrpcServer</code>在运行之前，会将所有的事件和响应注册到队列中，这样，队列中进入事件时，就可以调用对应的处理函数进行处理。</p>
<h3 id="本地异步调用">本地异步调用</h3>
<p>Ray使用了大量的异步处理，例如gRPC框架中的请求和响应，以及本地的异步处理框架。Ray的Worker等进程中，除了gRPC的异步框架之外，还有一个<code>boost::asio::io_context</code>框架，所有gRPC的响应并不是在pull_threads中处理，而是把事件转交给本地的异步处理框架，然后在该异步处理框架中处理。并且该框架中还内置了一个EventTracker，来记录所有时间的处理信息。</p>
<p>结果的处理交给本地异步处理来运行，猜测是为了加快gRPC队列中的数据消费。</p>
<h2 id="driver提交流程">Driver提交流程</h2>
<p>以无状态任务为例，描述任务的提交流程。</p>
<h3 id="python部分">Python部分</h3>
<p><strong><span class="citation"
data-cites="ray.remote">@ray.remote</span></strong></p>
<p>被<code>@ray.remote</code>装饰的函数会被ray分布式处理。该装饰器会将函数（或者对象，后续的描述均为函数的装饰）封装成<code>RemoteFunction</code>对象。该对象保存了被装饰函数的function对象，并且提供<code>remote</code>方法。</p>
<p>当<code>remote</code>方法被调用时，会将python函数包装成<code>PythonFunctionDescriptor</code>,记录了module/function/class
name，以及分配的uuid。随后使用pickle_dump将函数序列化，交给worker处理。worker会将序列化后的函数存储到gcs服务的function
table中，并记录该函数的uuid，以便于通过函数描述找到函数体。</p>
<p>以上工作完成后，remote方法会调用worker的submit_task方法提交任务，该任务即可通过gRPC发送到集群中处理。submit_task返回一个object_ref。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">invocation (/home/hua/code/ray/python/ray/remote_function.py:485)</span><br><span class="line">_remote (/home/hua/code/ray/python/ray/remote_function.py:504)</span><br><span class="line">_invocation_remote_span (/home/hua/code/ray/python/ray/util/tracing/tracing_helper.py:310)</span><br><span class="line">auto_init_wrapper (/home/hua/code/ray/python/ray/_private/auto_init_hook.py:21)</span><br><span class="line">_remote_proxy (/home/hua/code/ray/python/ray/remote_function.py:156)</span><br><span class="line">&lt;listcomp&gt; (/home/hua/code/ray/pi.py:23)</span><br><span class="line">calculate_pi (/home/hua/code/ray/pi.py:22)</span><br><span class="line">&lt;module&gt; (/home/hua/code/ray/pi.py:39)</span><br></pre></td></tr></table></figure>
<h3 id="driver部分">Driver部分</h3>
<p><strong>任务提交到本地</strong></p>
<p>Driver的python代码调用submit_task后，会通过cython调用到C++
extention中。对应的函数是<code>CoreWorker::SubmitTask</code>，这里会将相关的任务信息打包成<code>TaskSpec</code>，然后提交到本地异步IO中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::core::CoreWorker::SubmitTask() (/home/hua/code/ray/src/ray/core_worker/core_worker.cc:2467)</span><br><span class="line"></span><br><span class="line">cython ...</span><br><span class="line">Python ...</span><br></pre></td></tr></table></figure>
<p><strong>解决依赖</strong></p>
<p>从异步IO调度到该任务后(<code>NormalTaskSubmitter::SubmitTask</code>)，会先等待依赖的资源处理结束，这里使用了回调的方式异步等待依赖的任务结束。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::SubmitTask() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:23)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()(const struct &#123;...&#125; * const __closure) (/home/hua/code/ray/src/ray/core_worker/core_worker.cc:2469)</span><br><span class="line"></span><br><span class="line">_raylet.so!EventTracker::RecordExecution() (/home/hua/code/ray/src/ray/common/event_stats.cc:113)</span><br><span class="line"></span><br><span class="line">_raylet.so!std::_Function_handler&lt;void(), instrumented_io_context::post() (/home/hua/code/ray/src/ray/common/asio/instrumented_io_context.cc:97)</span><br><span class="line"></span><br><span class="line">从异步IO调度</span><br></pre></td></tr></table></figure>
<p><strong>请求资源</strong></p>
<p>依赖的任务执行结束后，准备执行当前任务，但是对当前SchedulingKey来说目前没有空闲的Worker，需要先向reylet请求Worker，<code>NormalTaskSubmitter::RequestNewWorkerIfNeeded</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::RequestNewWorkerIfNeeded() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:347)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()(struct &#123;...&#125; * const __closure, ray::Status status) (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:80)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::LocalDependencyResolver::ResolveDependencies() (/home/hua/code/ray/src/ray/core_worker/transport/dependency_resolver.cc:84)</span><br><span class="line"></span><br><span class="line">异步回调</span><br></pre></td></tr></table></figure>
<p><strong>任务提交到集群</strong></p>
<p>Worker资源异步请求会返回空闲Worker的Address，然后可以通过RPC将任务直接提交(<code>PushTask</code>)给这个Worker。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::rpc::CoreWorkerClient::PushNormalTask() (/home/hua/code/ray/src/ray/rpc/worker/core_worker_client.h:399)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::PushNormalTask() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:561)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::NormalTaskSubmitter::OnWorkerIdle() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:181)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()() (/home/hua/code/ray/src/ray/core_worker/transport/normal_task_submitter.cc:436)</span><br><span class="line"></span><br><span class="line">RPC回调</span><br></pre></td></tr></table></figure>
<h2 id="worker执行流程">Worker执行流程</h2>
<p><strong>调试技巧</strong></p>
<ol type="1">
<li>首先启动Driver，在<code>INVOKE_RPC_CALL</code>执行之前打断点，阻塞任务提交到集群。</li>
<li>attach到Worker进程上，并且在<code>CoreWorker::HandlePushTask</code>打断点，这里是处理RPC请求的入口。</li>
<li>让Driver继续执行，Worker就会命中断点，可以继续调试Worker。</li>
</ol>
<p><strong>gRPC将任务提交到本地</strong></p>
<p><code>GrpcServer::PollEventsFromCompletionQueue</code>会等待gRPC请求，当收到请求后，就会调用从Tag中取出ServerCall对象，该对象中保存着该请求的所有处理的必要信息。然后将该任务提交给异步IO。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!ray::rpc::ServerCallImpl&lt;ray::rpc::CoreWorkerServiceHandler, ray::rpc::GetCoreWorkerStatsRequest, ray::rpc::GetCoreWorkerStatsReply, (ray::rpc::AuthType)0&gt;::HandleRequest() (/home/hua/code/ray/bazel-out/aarch64-dbg/bin/_virtual_includes/grpc_common_lib/ray/rpc/server_call.h:237)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::rpc::GrpcServer::PollEventsFromCompletionQueue() (/home/hua/code/ray/src/ray/rpc/grpc_server.cc:199)</span><br><span class="line"></span><br><span class="line">gRPC pulling thread</span><br></pre></td></tr></table></figure>
<p><strong>调用gRPC注册的Handler方法</strong></p>
<p>异步IO会回调注册的方法<code>CoreWorker::HandlePushTask</code>，配置定义<code>send_reply_callback</code>回调函数，最后将任务通过异步IO提交给<code>task_execution_service</code>（就是另外一个异步IO队列）。</p>
<p><strong>执行函数</strong></p>
<p>当执行调度到PushTask任务时，就会回调上一步配置的<code>send_reply_callback</code>回调，远程函数的执行就在这个回调中运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">_raylet.so!operator()(const struct &#123;...&#125; * const __closure, const ray::TaskSpecification &amp; task_spec, ray::rpc::SendReplyCallback send_reply_callback) (/home/hua/code/ray/src/ray/core_worker/transport/task_receiver.cc:100)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::InboundRequest::Accept(ray::core::InboundRequest * const this) (/home/hua/code/ray/src/ray/core_worker/transport/actor_scheduling_util.cc:36)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::NormalSchedulingQueue::ScheduleRequests(ray::core::NormalSchedulingQueue * const this) (/home/hua/code/ray/src/ray/core_worker/transport/normal_scheduling_queue.cc:87)</span><br><span class="line"></span><br><span class="line">_raylet.so!ray::core::TaskReceiver::RunNormalTasksFromQueue(ray::core::TaskReceiver * const this) (/home/hua/code/ray/src/ray/core_worker/transport/task_receiver.cc:294)</span><br><span class="line"></span><br><span class="line">_raylet.so!operator()(const struct &#123;...&#125; * const __closure) (/home/hua/code/ray/src/ray/core_worker/core_worker.cc:3777)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>回调中的<code>task_handler_</code>
就是注册进去的<code>CoreWorker::ExecuteTask</code>，将这个对象封装成了一个lamda函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> execute_task = std::<span class="built_in">bind</span>(&amp;CoreWorker::ExecuteTask,</span><br><span class="line">                                  <span class="keyword">this</span>,</span><br><span class="line">                                  std::placeholders::_1,</span><br><span class="line">                                  std::placeholders::_2,</span><br><span class="line">                                  std::placeholders::_3,</span><br><span class="line">                                  std::placeholders::_4,</span><br><span class="line">                                  std::placeholders::_5,</span><br><span class="line">                                  std::placeholders::_6,</span><br><span class="line">                                  std::placeholders::_7,</span><br><span class="line">                                  std::placeholders::_8);</span><br></pre></td></tr></table></figure>
<p>最终调用到了<code>options_.task_execution_callback</code>，这个callback会根据语言的不同而不同，以Python为例，这个callback调用的是注册进来的一个Python方法，将Python的远程函数交还给Python解释器来执行。这部分代码在ray/python/ray/_raylet.pyx中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cdef void execute_task: return function(actor, *arguments, **kwarguments)</span><br><span class="line">cdef execute_task_with_cancellation_handler: execute_task</span><br><span class="line">cdef CRayStatus task_execution_handler: execute_task_with_cancellation_handler</span><br><span class="line">CoreWorker::__cinit__: options.task_execution_callback = task_execution_handler</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/12/llama-cpp%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/11/12/llama-cpp%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/" class="post-title-link" itemprop="url">llama.cpp昇腾原生支持</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-12 15:33:29" itemprop="dateCreated datePublished" datetime="2024-11-12T15:33:29+08:00">2024-11-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:34:11" itemprop="dateModified" datetime="2025-01-23T15:34:11+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="项目背景">1. 项目背景</h2>
<p>llama.cpp
是一个开源项目，旨在将大模型高效地部署在低资源环境中，例如个人电脑或移动设备。这个项目由
Georgi Gerganov 创建，目标是通过优化和精简，使得 LLaMA 模型能够在不依赖
GPU 的情况下高效运行。llama.cpp 支持多平台和多后端，且兼容大部分
Transformer 模型和部分 CLIP
模型，便于在各种环境中部署。其模块化设计包括模型分片、KV
缓存、推理引擎和输出处理，适合边缘计算、隐私保护和低成本推理场景，帮助用户在普通设备上实现大模型推理。</p>
<h3 id="目标">1.1 目标</h3>
<p>开发基于昇腾的llama.cpp后端，实现昇腾runtime和核心算子。后端使用CANN和昇腾算子库的能力来加速大模型的推理。使得常见的模型能够在llama.cpp中使用昇腾推理，加速推理效率。</p>
<h3 id="项目概述">1.2 项目概述</h3>
<p><strong>昇腾后端和Runtime接入</strong></p>
<p>在 llama.cpp 中，为Ascend加速器提供接口适配层，使 llama.cpp
的模型推理请求能通过接口层传递至 Ascend Runtime。</p>
<p>涉及：</p>
<ul>
<li>设备接入，支持多卡接入；</li>
<li>内存管理和Tensor管理；</li>
<li>Stream，Event管理；</li>
</ul>
<p><strong>昇腾算子</strong></p>
<p>为了支持大部分的模型推理，需要实现43个算子。这些算子可以利用aclnn的算子能力构建，如果aclnn的算子不足以满足llama.cpp的算子，</p>
<p>则：</p>
<ul>
<li>优先使用aclnn算子组合的方式实现功能；</li>
<li>使用AscendC编写自定义算子。</li>
</ul>
<p>对算子的需求，可用性大于性能，为了减少开发工作量并快速完成支持，不考虑acl
op算子。能使用aclnn组合的算子，优先使用算子组合实现。</p>
<p><strong>精度和性能</strong></p>
<ul>
<li><p>实现的算子需要通过llama.cpp的精度对比测试，以及内存越界检查，确保实现的算子实现正确。</p></li>
<li><p>910B算子性能需要超过Intel CPU水平（以Intel(R) Xeon(R) Gold 6348
CPU @ 2.60GHz为例）。</p></li>
<li><p>910B模型推理（llama3
8B）性能延迟不高于100ms，吞吐率不低于300token/s。</p></li>
</ul>
<p><strong>多芯片支持</strong></p>
<ul>
<li>首先支持910B系列芯片，包括主要的模型端到端推理，q4_0，q8_0量化格式；</li>
<li>然后支持310P（910A）系列芯片，除了q4_0外(310P不支持4bit量化)，其他功能应当与910B芯片能力持平；</li>
<li>最后尝试支持310B系列芯片，310B的支持程度以aclnn和AscendC库的支持情况而定。</li>
</ul>
<p><strong>文档和用户指南</strong></p>
<ul>
<li><p>用户指南，介绍文档结构和使用说明，帮助用户理解如何在 llama.cpp
中配置和使用 Ascend 后端；</p></li>
<li><p>安装配置步骤，详细说明 Ascend
后端的安装流程，包括环境依赖、编译步骤及配置方法，以确保用户可以顺利完成安装；</p></li>
<li><p>常见问题和解决方法，总结用户在使用 Ascend
后端时可能遇到的问题，并提供解决方案，如内存溢出、兼容性问题和性能调优建议等。</p></li>
</ul>
<h2 id="设计思路">2. 设计思路</h2>
<h3 id="llama.cpp项目架构">2.1 llama.cpp项目架构</h3>
<figure>
<img
src="https://cdn-0.plantuml.com/plantuml/png/TPJFRjim3CRlVWgYzxn03qE3D1s29LZMRBiCnS1PT2fKfZo9pluKU_T9egkq1f9B1FBJxoCfmZTHCCZOkoPGAyX7Ht2rtU9k2Jjlo5q1HkZp2PuRIBzlMuz6SmyQkFFX5mO3Uunn2dqQaSN-HR6Ufr2v0OV1MH7BnuVcN_FQyiDNM67xI9ayEYgsJwVlRObDpYhOi53eoLWKWdkAevCNstioOqkG_zWW2wnyFuoYPSmCDznH84xoDHyjgwETWjNoa1npFMz8chgbaqt27J8UgIUMkSF7KT8Ls0VVKeofvsBXDVfSPzUZW4edi19pQuFdI77ETGvxN4GA9me5gSSNv7A_IIsPmLj-Ium9-NEaA4hKHrqitftdV0rVajyvK-UHKni--QUKhUgv83LwiYRPtA9WKpD5frqdqNjY2YY9CrKzfno8JOJwEhNcHX55FrszJaaP0yVph9g6lH04UtmKy9rkRXb2hKwN6zcKyEU073iVUWgwrB64DzowCQjdQziG6yWMCwCwGxtT3ycDukkjWNNNsTOIjtykGUeSbB8AFiR5tg7a-WehaNxOLCfbX4vfwfW631Hphw2BfitTMKOtxTn5aCwuwDU_XKA-abThsUPjXivhJgTPd-kCkqcd_5dv3m00"
alt="llama.cpp架构图" />
<figcaption aria-hidden="true">llama.cpp架构图</figcaption>
</figure>
<p>llama.cpp的核心功能主要涉及以上几个部分：</p>
<p><strong>模型管理</strong></p>
<p>llama.cpp不仅支持llama，而且支持多种大语言模型和一些clip模型。llama.cpp使用模型管理模块来搭建模型结构，包括算子，量化等并且加载gguf模型的信息和模型权重。由于llama.cpp支持模型拆分的功能，以便于支持多卡推理和GPU/CPU混合推理，所以模型结构会进行合适的拆分，并且管理子图之间的数据拷贝。</p>
<p><strong>kv-cache</strong></p>
<p>kv-cache有助于加速attention的计算速度，将历史的kv信息做缓存。kv-cache会直接当做算子融合到模型中，kv-cache模块本身负责cache的管理，包括cache写入，更新和替换。</p>
<p><strong>server和api接口</strong></p>
<p>llama.cpp提供了一个简单的服务端，提供api接口。server支持并发推理。</p>
<p><strong>推理引擎</strong></p>
<p>llama.cpp对推理引擎进行了抽象，以便于支持不同的后端。推理引擎负责管理设备的内存，流，事件，多卡以及GPU/CPU数据拷贝。并且计算由模型管理模块构建的模型图。</p>
<h3 id="昇腾后端接入方法">2.2 昇腾后端接入方法</h3>
<p>llama.cpp提供了一系列抽象接口来接入后端加速器：</p>
<ol type="1">
<li><strong><code>ggml_backend_cann_device_interface</code></strong>：用于描述设备接口的模块，定义了设备的基本功能。</li>
<li><strong><code>ggml_backend_cann_interface</code></strong>：用于管理后端通用接口的模块，包含常见的张量异步处理和图计算功能。</li>
<li><strong><code>ggml_backend_cann_buffer_type_host</code></strong>：负责分配主机缓冲区，确保与后端设备内存的接口兼容。</li>
<li><strong><code>ggml_cann_compute_forward</code></strong>：主计算模块，负责分派和执行各个算子操作。</li>
</ol>
<p>在 <code>ggml_cann_compute_forward</code> 中，所有的算子都作为 case
分支进行注册，表示算子名称对应具体操作，例如
<code>GGML_OP_ADD</code>、<code>GGML_OP_MUL</code> 等等。</p>
<p>昇腾接入需要实现llama.cpp的runtime接口，并且实现推理所必须的算子。</p>
<h2 id="实现原理">3. 实现原理</h2>
<h3 id="运行时">3.1 运行时</h3>
<p>runtime提供了多个抽象接口，第一阶段主要目标是基本功能支持，所以仅需要支持必要的接口。其中split
tensor功能和图推理功能暂时不实现。llama.cpp的后端接入主要是通过注册三组接口实现，分别是设备访问接口，资源管理接口，内存管理接口。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_device_i ggml_backend_cann_device_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name                = */</span> ggml_backend_cann_device_get_name,</span><br><span class="line">    <span class="comment">/* .get_description         = */</span> ggml_backend_cann_device_get_description,</span><br><span class="line">    <span class="comment">/* .get_memory              = */</span> ggml_backend_cann_device_get_memory,</span><br><span class="line">    <span class="comment">/* .get_type                = */</span> ggml_backend_cann_device_get_type,</span><br><span class="line">    <span class="comment">/* .get_props               = */</span> ggml_backend_cann_device_get_props,</span><br><span class="line">    <span class="comment">/* .init_backend            = */</span> ggml_backend_cann_device_init,</span><br><span class="line">    <span class="comment">/* .get_buffer_type         = */</span> ggml_backend_cann_device_get_buffer_type,</span><br><span class="line">    <span class="comment">/* .get_host_buffer_type    = */</span> ggml_backend_cann_device_get_host_buffer_type,</span><br><span class="line">    <span class="comment">/* .buffer_from_host_ptr    = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .supports_op             = */</span> ggml_backend_cann_supports_op,</span><br><span class="line">    <span class="comment">/* .supports_buft           = */</span> ggml_backend_cann_supports_buft,</span><br><span class="line">    <span class="comment">/* .offload_op              = */</span> ggml_backend_cann_offload_op,</span><br><span class="line">    <span class="comment">/* .event_new               = */</span> ggml_backend_cann_device_event_new,</span><br><span class="line">    <span class="comment">/* .event_free              = */</span> ggml_backend_cann_device_event_free,</span><br><span class="line">    <span class="comment">/* .event_synchronize       = */</span> ggml_backend_cann_device_event_synchronize,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>此接口 <code>ggml_backend_cann_device_interface</code> 为 CANN 后端在
llama.cpp 中提供了一个通用的设备访问与操作抽象层，便于整合并统一管理
CANN 设备资源。通过实现接口中的各个函数，用户可以控制 CANN
设备的初始化、资源分配、操作支持检测等关键功能，从而确保 llama.cpp
中的模型计算能够顺利利用 CANN 的加速能力。</p>
<p>以下是接口中各函数的功能描述：</p>
<ul>
<li><p><strong>ggml_backend_cann_device_get_name</strong>：返回设备的名称，用于识别不同的设备类型。例如可以返回
"CANN 设备" 或者具体的设备型号。</p></li>
<li><p><strong>ggml_backend_cann_device_get_description</strong>：返回设备的详细描述信息，通常包含设备的硬件特性以及版本信息等，帮助用户理解设备特性。</p></li>
<li><p><strong>ggml_backend_cann_device_get_memory</strong>：获取设备的内存信息，包括总内存大小和当前可用内存，以便
llama.cpp 优化内存分配策略。</p></li>
<li><p><strong>ggml_backend_cann_device_get_type</strong>：返回设备类型，用于区分不同种类的设备（如
CPU、GPU、NPU 等），便于进行不同类型设备的适配。</p></li>
<li><p><strong>ggml_backend_cann_device_get_props</strong>：获取设备的属性信息，包括计算能力、内存带宽等。这些属性信息可用于优化计算分配和选择适合的算子。</p></li>
<li><p><strong>ggml_backend_cann_device_init</strong>：初始化后端设备，确保设备的资源和状态准备就绪。这一步通常在加载模型或开始计算之前调用。</p></li>
<li><p><strong>ggml_backend_cann_device_get_buffer_type</strong>：返回设备内存缓冲区的类型信息，帮助
llama.cpp 决定如何在设备端管理数据缓冲。</p></li>
<li><p><strong>ggml_backend_cann_device_get_host_buffer_type</strong>：返回主机端缓冲区类型，用于在主机和设备之间进行高效的数据交换。</p></li>
<li><p><strong>buffer_from_host_ptr</strong>：该接口可用于将主机端内存直接映射或转换为设备端缓冲区，若未来需求可扩展。</p></li>
<li><p><strong>ggml_backend_cann_supports_op</strong>： 检查 CANN
设备是否支持指定的操作（op），确保模型中的特定操作能够得到设备的加速支持。</p></li>
<li><p><strong>ggml_backend_cann_supports_buft</strong>：检查设备是否支持指定的缓冲区类型，确保数据在缓冲区类型上的一致性和兼容性。</p></li>
<li><p><strong>ggml_backend_cann_offload_op</strong>：将计算操作卸载到设备端执行，提升操作效率和加速模型推理过程。</p></li>
<li><p><strong>ggml_backend_cann_device_event_new</strong>：创建新的事件对象，用于异步操作的状态跟踪，如操作完成的通知。</p></li>
<li><p><strong>ggml_backend_cann_device_event_free</strong>：释放事件对象，清理事件资源，确保内存不被泄漏。</p></li>
<li><p><strong>ggml_backend_cann_device_event_synchronize</strong>：同步事件，确保指定的异步操作完成。这通常用于确保操作的执行顺序。</p></li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_i ggml_backend_cann_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name                = */</span> ggml_backend_cann_name,</span><br><span class="line">    <span class="comment">/* .free                    = */</span> ggml_backend_cann_free,</span><br><span class="line">    <span class="comment">/* .set_tensor_async        = */</span> ggml_backend_cann_set_tensor_async,</span><br><span class="line">    <span class="comment">/* .get_tensor_async        = */</span> ggml_backend_cann_get_tensor_async,</span><br><span class="line">    <span class="comment">/* .cpy_tensor_async        = */</span> ggml_backend_cann_cpy_tensor_async,</span><br><span class="line">    <span class="comment">/* .synchronize             = */</span> ggml_backend_cann_synchronize,</span><br><span class="line">    <span class="comment">/* .graph_plan_create       = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_free         = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_update       = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_compute      = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_compute           = */</span> ggml_backend_cann_graph_compute,</span><br><span class="line">    <span class="comment">/* .event_record            = */</span> ggml_backend_cann_event_record,</span><br><span class="line">    <span class="comment">/* .event_wait              = */</span> ggml_backend_cann_event_wait,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>ggml_backend_cann_interface</code> 接口提供了 CANN 后端在
llama.cpp 中的资源管理、异步数据传输、计算图执行等功能接口，实现了与
CANN 后端的深度集成。通过该接口，llama.cpp
可以高效地管理张量的异步操作、事件记录、同步及图计算，确保计算任务能够顺畅运行在
CANN 设备上。</p>
<p>以下是接口中各函数的功能描述：</p>
<ul>
<li><p><strong>ggml_backend_cann_name</strong>：
返回后端名称，通常用于标识该后端为 CANN 后端。</p></li>
<li><p><strong>ggml_backend_cann_free</strong>：释放后端资源，确保内存和其他资源在后端不再使用时被正确回收。</p></li>
<li><p><strong>ggml_backend_cann_set_tensor_async</strong>：异步设置张量数据到设备端，为后续计算提供数据准备。异步设置可提高数据传输的效率。</p></li>
<li><p><strong>ggml_backend_cann_get_tensor_async</strong>
：异步获取张量数据，方便在计算完成后从设备端提取数据，避免阻塞主线程。</p></li>
<li><p><strong>ggml_backend_cann_cpy_tensor_async</strong>：异步复制张量数据，支持设备端和主机端之间的数据交互或设备内部的数据拷贝，以便于多任务并行处理。</p></li>
<li><p><strong>ggml_backend_cann_synchronize</strong>
：同步操作，确保所有异步任务完成，通常用于确保张量操作和事件顺序执行。</p></li>
<li><p><strong>graph_plan_create</strong>：该接口目前未实现。将来可用于创建计算图执行计划，优化计算图的操作顺序和资源分配。</p></li>
<li><p><strong>graph_plan_free</strong>：该接口目前未实现。可以释放计算图计划的资源，确保内存使用的高效管理。</p></li>
<li><p><strong>graph_plan_update</strong>：该接口目前未实现。可用于在图执行过程中动态更新计算计划，以适应运行时的资源情况。</p></li>
<li><p><strong>graph_plan_compute</strong>：该接口目前未实现。未来可能用于执行图计划中的所有操作，便于更复杂的任务调度。</p></li>
<li><p><strong>ggml_backend_cann_graph_compute</strong>：执行计算图中的所有节点操作，是核心计算接口之一。该函数负责协调图中的计算任务，使之并行或顺序执行。</p></li>
<li><p><strong>ggml_backend_cann_event_record</strong>：记录事件，用于标记特定操作的时间点，便于在异步计算中追踪进度和执行状态。</p></li>
<li><p><strong>ggml_backend_cann_event_wait</strong>：等待特定事件完成，通常用于确保在后续操作开始前当前任务已完成，以保持计算图的执行正确性。</p></li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_buffer_type_i ggml_backend_cann_buffer_type_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name         = */</span> ggml_backend_cann_buffer_type_name,</span><br><span class="line">    <span class="comment">/* .alloc_buffer     = */</span> ggml_backend_cann_buffer_type_alloc_buffer,</span><br><span class="line">    <span class="comment">/* .get_alignment    = */</span> ggml_backend_cann_buffer_type_get_alignment,</span><br><span class="line">    <span class="comment">/* .get_max_size     = */</span> <span class="literal">NULL</span>,  <span class="comment">// defaults to SIZE_MAX</span></span><br><span class="line">    <span class="comment">/* .get_alloc_size   = */</span> ggml_backend_cann_buffer_type_get_alloc_size,</span><br><span class="line">    <span class="comment">/* .is_host          = */</span> ggml_backend_cann_buffer_type_is_host,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>ggml_backend_cann_buffer_type_interface</code> 结构体定义了
CANN
后端缓冲区类型的接口，它提供了一组操作缓冲区属性和行为的函数接口。这个接口使得
CANN 后端的缓冲区能够在 llama.cpp
中被正确管理和使用，确保内存分配、对齐、大小等操作的一致性和高效性。</p>
<p>以下是 <code>ggml_backend_cann_buffer_type_interface</code>
结构体中各字段的功能描述：</p>
<ul>
<li><strong>ggml_backend_cann_buffer_type_name</strong>：返回缓冲区类型的名称。该函数用于标识当前缓冲区类型，主要用于调试和日志记录。</li>
<li><strong>ggml_backend_cann_buffer_type_alloc_buffer</strong>：用于分配缓冲区的内存。通过该函数，llama.cpp
可以请求 CANN 后端分配指定大小的内存块，用于存储数据和张量。</li>
<li><strong>ggml_backend_cann_buffer_type_get_alignment</strong>：返回缓冲区的对齐方式。内存对齐对于性能至关重要，因为不适当的对齐可能导致
CPU 或 GPU
在访问数据时的效率降低。该函数可以确保数据在内存中的对齐符合硬件的要求。</li>
<li><strong>get_max_size</strong> ：该字段指示缓冲区的最大尺寸，若设置为
<code>NULL</code>，则默认最大值为
<code>SIZE_MAX</code>，即没有固定的尺寸限制。此函数适用于不希望为缓冲区大小设定上限的场景。</li>
<li><strong>ggml_backend_cann_buffer_type_get_alloc_size</strong>
：获取缓冲区实际分配的内存大小。该函数确保返回正确的分配大小，便于用户跟踪内存使用情况。</li>
<li><strong>ggml_backend_cann_buffer_type_is_host</strong>：判断缓冲区是否为主机缓冲区。该函数用于区分主机内存和设备内存，以便进行适当的内存管理和数据传输。</li>
</ul>
<p><strong>Host buffer</strong></p>
<p>Host buffer是一种特殊的buffer
type，用于在CPU上申请内存，用于一些中间数据的临时存储，为后端设备提供了以快速访问的内存区域。</p>
<p>Pin
memory，又称“锁页内存”或“固定内存”，是指将主机内存中的一部分内存固定在物理内存上，以便快速传输至计算设备（如GPU或NPU）。通常情况下，操作系统会将不经常使用的内存页移至虚拟内存中，这可能导致数据传输时出现额外的内存访问延迟。而使用Pin
memory则可以避免这种情况，因为锁页内存不会被系统交换出物理内存，从而大大加速数据传输过程。</p>
<p>Host buffer使用Pin
memory实现，用户加速Host和Device之间的内存拷贝速度。Host
buffer与buffer_type的结构相同，以接口注册的方式提供Host
buffer的能力。</p>
<p><strong>Split Tensor</strong></p>
<p>Split
Tensor用于在做复杂计算的时候充分利用多卡能力，llama.cpp中，对矩阵乘法，使用到了Split
Tensor，计算时会相乘的矩阵其中一个进行拆分，使用多卡进行并行计算，计算完成后做结果的合并。</p>
<p>Split
Tensor实现复杂，并且无法利用已有的aclnn算子，在本次设计中不考虑，待后续性能提升中考虑实现。</p>
<h3 id="算子">3.2 算子</h3>
<p>llama.cpp主要的推理是单算子推理功能，图推理功能在本次设计中暂不考虑实现。昇腾的单算子支持aclop以及aclnn两种调用方式。经过简单的demo进行性能对比，aclop编译执行的方式执行效率较低，主要算子均通过aclnn实现，aclnn不支持的算子使用aclnn基本算子组合的方式实现，后续需要使用AscendC将组合算子进行融合以提高性能。</p>
<h4 id="tensor转换">3.2.1 Tensor转换</h4>
<p>llama.cpp和昇腾算子对Tensor的定义有一定的差异，为了能够使用昇腾算子，需要在调用的时候对Tensor结构做转换。</p>
<p><strong>结构差异</strong></p>
<p>两者的Tensor基本上都是数据和dims，nelements，nstride，dtype的属性集合，但是有一些差异：</p>
<ul>
<li>llama.cpp的ne和nb的顺序是从内到外，也就是与传统意义的维度顺序相反，序号小的是最内的维度。</li>
<li>llama.cpp的stride的单位是字节，而aclnn的stride单位是元素。</li>
</ul>
<p><strong>广义broadcast</strong></p>
<p>当两个计算的tensor维度不同时，会尝试做broadcast，aclnn接口支持的是传统broadcast方式，而llama.cpp支持的是广义的broadcast：</p>
<ul>
<li>aclnn接口的broadcast仅会在Tensor的某个维度不同，但是其中一个Tensor的维度为1的时候发生；</li>
<li>llama.cpp的broadcast会在Tensor的某个维度不同，但一个Tensor的维度大小是另外一个的整数倍的时候发生。</li>
</ul>
<p>为了减少显示broadcast带来的性能和内存的开销，需要进行维度的调整，以便于利用算子的broadcast特性：</p>
<p>例如，Tensor A(9,5,2,7)， Tensor
B(9,10,2,7)，这两个Tensor对aclnn接口来说不可自动broadcast，但是对llama.cpp来说允许自动broadcast。当数据内容连续时，可以通过添加一个维度来兼容aclnn的broadcast规则。
通过将
A(9,5,2,7)转换成A'(9,5,1,2,7)，B(9,10,2,7)转换成B'(9,5,2,2,7)。仅通过调整dims信息，即可利用aclnn算子的自动broadcast能力。</p>
<h4 id="aclnn算子">3.2.1 aclnn算子</h4>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 43%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr>
<th>算子名称</th>
<th>描述</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>Elementwise Add</td>
<td>对两个张量进行逐元素加法，并将结果存储在目标张量中。</td>
<td><span class="math inline">\(dst(i)=src0(i)+src1(i)\)</span></td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td>对输入张量应用 Leaky ReLU 激活函数，并将结果存储在目标张量中。</td>
<td><span class="math display">\[\text{dst(i)}
=\begin{cases}\text{src(i)} &amp; \text{src(i)} \geq \text{0}
\\\text{negative-slope} \times \text{src(i)}&amp; \text{src(i)} &lt;
\text{0}\end{cases}\]</span></td>
</tr>
<tr>
<td>Arange</td>
<td>创建一个从 <code>start</code> 开始，到 <code>stop</code>
结束，每次增长 <code>step</code> 的 Tensor。</td>
<td><span class="math inline">\(\text {out }_{i+1}=\text {out }_i+\text
{step}\)</span></td>
</tr>
<tr>
<td>Clamp</td>
<td>将 input 张量的每个元素夹紧到区间 [min, max]
中，并将结果返回到新的张量中。</td>
<td><span class="math display">\[  \text{dst}(i)
=  \begin{cases}  \text{min} &amp; \text{src(i)} &lt; \text{min}
\\  \text{src(i)} &amp; \text{min} \leq \text{src(i)} \leq \text{min}
\\  \text{max} &amp; \text{src(i)} &gt;
\text{max}  \end{cases}  \]</span></td>
</tr>
<tr>
<td>Scale</td>
<td>使用 <code>scale</code> 缩放一个 Tensor
的所有元素，将结果返回到新的张量中。</td>
<td><span class="math inline">\(dst(i) = src(i) \times
scale\)</span></td>
</tr>
<tr>
<td>Argsort</td>
<td>将输入 Tensor 中的元素根据某个维度进行升序 / 降序排序，返回对应的
index 值。</td>
<td>-</td>
</tr>
<tr>
<td>Layer Norm</td>
<td>对指定层进行均值为 0、标准差为 1
的归一化计算，并将结果写入到新的张量中。</td>
<td><span class="math inline">\(out = \frac{x - E[x]}{\sqrt{Var[x] +
\epsilon}} \times w + b\)</span></td>
</tr>
<tr>
<td>Group Norm</td>
<td>计算输入的组归一化结果返回到新的张量中。</td>
<td>$ out &amp;= + \$</td>
</tr>
<tr>
<td>Acc</td>
<td>将 src 张量的数据累加到 dst 中。</td>
<td><span class="math inline">\(dst(i) = src(i) + dst(i)\)</span></td>
</tr>
<tr>
<td>Sum Rows</td>
<td>返回给定维度中输入张量每行的和。</td>
<td>-</td>
</tr>
<tr>
<td>Upsample Nearest2d</td>
<td>对由多个输入通道组成的输入信号应用最近邻插值算法进行上采样。</td>
<td>-</td>
</tr>
<tr>
<td>Pad</td>
<td>将 Tensor 填充到与目标 Tensor 相同的尺寸。</td>
<td>-</td>
</tr>
<tr>
<td>avg pool2d</td>
<td>对输入 Tensor 进行窗口为 kH×kW、步长为 sH×sW
的二维平均池化操作。</td>
<td><span class="math display">\[\text{out}\left(N_{i}, C_{i}, h,
w\right) = \frac{1}{k H \cdot k W} \sum_{m=0}^{k H-1} \sum_{n=0}^{k W-1}
\text{input}\left(N_{i}, C_{i}, \text{stride}[0] \times h + m,
\text{stride}[1] \times w + n\right)\]</span></td>
</tr>
<tr>
<td>max pooling</td>
<td>对于 dim=3 或 4 维的输入张量，进行最大池化操作。</td>
<td><span class="math inline">\(\text{out}\left(N_{i}, C_{i}, h,
w\right) = \max_{m=0}^{k H-1} \max_{n=0}^{k W-1}
\text{input}\left(N_{i}, C_{i}, \text{stride}[0] \times h + m,
\text{stride}[1] \times w + n\right)\)</span></td>
</tr>
<tr>
<td>rms norm</td>
<td>计算给定 Tensor 的均方根归一化函数，并将结果写入到输出 Tensor
中。</td>
<td><span
class="math inline">\(\text{RmsNorm}\left(x_i\right)=\frac{x_i}{\text{Rms}(\mathbf{x})}
g_i,&lt;br/&gt;   *\quad \text { where }
\text{Rms}(\mathbf{x})=\sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2+e p
s}\)</span></td>
</tr>
<tr>
<td>diag mask</td>
<td>将 Tensor 进行三角形掩码运算，将下三角部分保留，上三角部分置
1。</td>
<td>-</td>
</tr>
<tr>
<td>img2col</td>
<td>用于将二维 Tensor
数据转换成矩阵形式，以便于高效地进行卷积运算。</td>
<td>-</td>
</tr>
<tr>
<td>timestep_embedding</td>
<td>用于生成时间步嵌入。</td>
<td><span class="math inline">\(\text{dst}(t) =
[\sin(\frac{t}{10000^{2i/d}}),
\cos(\frac{t}{10000^{2i/d}})]\)</span></td>
</tr>
<tr>
<td>softmax</td>
<td>将输入的张量转化为概率分布，其值范围在 [0, 1] 之间，总和为 1。</td>
<td>$ (x_i) = $</td>
</tr>
<tr>
<td>matmul</td>
<td>计算两个 Tensor 的矩阵乘法，结果返回到新的 Tensor 中。</td>
<td>$ C_{ij} = <em>{k=1}^{n} A</em>{ik} B_{kj}$</td>
</tr>
<tr>
<td>Rope</td>
<td>算子是一种位置编码方法，通过旋转操作为输入序列引入位置信息，增强模型对位置关系的感知能力。</td>
<td><span class="math inline">\(\text{ROPE}(q, k) = \left[
q_{\text{even}} \cos(\theta) - q_{\text{odd}} \sin(\theta), ;
q_{\text{odd}} \cos(\theta) + q_{\text{even}} \sin(\theta)
\right]\)</span></td>
</tr>
<tr>
<td>repeat</td>
<td>对输入张量的元素沿特定维度重复，扩展原始数据的维度或增加相同数据的次数。</td>
<td><span class="math inline">\(\text{repeat}(x) = [x, x, \dots, x]
\quad (\text{repeated along specified dimension})\)</span></td>
</tr>
<tr>
<td>concat</td>
<td>将两个或多个张量在指定维度上拼接。</td>
<td><span class="math inline">\(\text{concat}(x_1, x_2, \dots, x_n) =
[x_1, x_2, \dots, x_n] \quad (\text{along specified
dimension})\)</span></td>
</tr>
<tr>
<td>Cast</td>
<td>将张量的数据类型从一种类型转换为另一种类型。</td>
<td>-</td>
</tr>
<tr>
<td>permute</td>
<td>重新排列张量的维度顺序。</td>
<td>-</td>
</tr>
<tr>
<td>exp</td>
<td>对 Tensor 的每个元素执行 exp 指数运算。</td>
<td><span class="math inline">\(\text{dst}_i =
e^{\text{src}_i}\)</span></td>
</tr>
<tr>
<td>Elementwise Mul</td>
<td>对两个张量对应元素进行乘法运算。</td>
<td><span class="math inline">\(z = x \times y\)</span></td>
</tr>
<tr>
<td>Cos</td>
<td>对张量的每个元素计算余弦值。</td>
<td><span class="math inline">\(y = \cos(x)\)</span></td>
</tr>
<tr>
<td>Sin</td>
<td>对张量的每个元素计算正弦值。</td>
<td><span class="math inline">\(y = \sin(x)\)</span></td>
</tr>
<tr>
<td>fill scalar</td>
<td>将张量的所有元素填充为指定的标量值。</td>
<td><span class="math inline">\(x[:] = \text{scalar}\)</span></td>
</tr>
<tr>
<td>pow tensor</td>
<td>将一个张量的每个元素提升到对应的指数幂。</td>
<td><span class="math inline">\(y = x^{\text{power}}\)</span></td>
</tr>
<tr>
<td>Alibi</td>
<td>一种相对位置嵌入策略，在注意力分数中加入线性偏置，帮助捕获相对位置信息。</td>
<td>$(i, j) = -m </td>
</tr>
<tr>
<td>repeat interleave</td>
<td>对张量的每个元素按指定次数重复，以在张量中插入更多的副本。</td>
<td><span class="math inline">\(\text{dst}(x, \text{repeats}) = [x_1,
x_1, \dots, x_1, x_2, x_2, \dots, x_2, \dots]\)</span></td>
</tr>
<tr>
<td>roll</td>
<td>将张量元素沿指定维度循环移动，即滚动。</td>
<td><span class="math inline">\(\text{roll}(x, \text{shift}) =
x_{\text{shifted along axis}}\)</span></td>
</tr>
<tr>
<td>index fill tensor</td>
<td>在张量的特定索引位置填充指定值。</td>
<td><span class="math inline">\(dst[\text{index}] =
\text{src}\)</span></td>
</tr>
</tbody>
</table>
<h4 id="ascendc算子">3.2.2 AscendC算子</h4>
<p>以下算子没有aclnn接口可调用，也无法使用基础算子组合，需要通过AscendC编程语言实现。为了简化算子的调用流程，采用kernel
call的方式进行调用。AscendC算子独立编译，以<code>.a</code>的方式链接到llama.cpp中。</p>
<p><strong>dup</strong></p>
<p>dup和copy语义相同，均为Tensor之间的拷贝，需要支持：</p>
<ul>
<li>量化Tensor和非量化Tensor之间的拷贝，拷贝过程中涉及到量化和反量化的计算过程。需要支持Q4_0和Q8_0两种量化格式。非量化格式需要支持fp32和fp16两种格式。</li>
<li>连续Tensor和非连续Tensor之间的拷贝（量化格式Tensor不涉及非连续场景）。</li>
</ul>
<p><strong>get rows</strong></p>
<p>从Tensor中按照index获取每行内容。</p>
<p>需要支持多种数据格式，包括fp32，fp16，Q4_0和Q8_0。获取后的数据均为fp32格式。</p>
<p>AscendC算子通过kernel launch的方式调用，调用时需要判断AI
core的数量，来配置合适的数量以提升执行效率。</p>
<p>为了兼容多种芯片，CMake时需要检测或根据提供的芯片类型进行编译和链接。</p>
<h3 id="内存管理">3.3 内存管理</h3>
<p>aclnn执行时，有些需要申请临时的NPU上内存做临时数据存储，频繁的内存分配和释放效率很低，需要内存池来提高内存分配性能。</p>
<p>在llama.cpp中，需要实现2中内存池：</p>
<p><strong>legacy pool</strong></p>
<p>使用N（256）个buffer做内存缓存，所有的内存释放必须放回内存池（防止异步执行访问到已释放内存），内存申请首先选择内存池中大小最合适的缓存，<strong>内存池为空则去申请内存</strong>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/47dd1352ecaef88457cf9dc7adba3f6e-image-20240806193006017-6f2692.png"
alt="legacy pool流程图" />
<figcaption aria-hidden="true">legacy pool流程图</figcaption>
</figure>
<p>会占用额外的内存，并且存在内存块查找的开销，并且，如果free的内存块超过N（256），则会出现assert失败问题。</p>
<p><strong>vmm pool</strong></p>
<p>使用虚拟内存，业务代码看到的是一段连续的内存，方便使用。实际上申请的物理内存是非连续的，当内存不足时申请一段物理内存映射到虚拟内存中。避免内存碎片和占用额外内存。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/14e4c332447ad345329843f1478b39a3-image-20240806193622691-6c7fee.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p>在虚拟内存中，申请的数据紧密排列，申请和销毁的顺序是相反的。比如，buffer1早于buffer2申请，那么buffer2必须要早于buffer1释放。在内存管理中，仅维护一个free指针，指示下一个buffer申请的起始地址。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/70ebc785a616889aa3601a77a8d74fe0-image-20240806194704808-3b08a3.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p><strong>异步计算的内存延迟释放</strong></p>
<p>由于所有的算子计算都是异步的，但是内存的申请和释放并不是异步的，所以，需要保证在异步计算完成之前，申请的内存是有效的。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/73704ff1cfbe476e085afc7fbfdf0c6a-image-20240806195347967-5ab4b4.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p>如图所示，当算子提交完成后，buffer3就会释放，free指针指向buffer3的起始地址。接着，下个算子开始执行，会从free指针开始申请内存，此时buffer3和buffer4是重叠的，但是由于stream中的算子计算有序，所以buffer3内的数据在完成计算之前，是不会被buffer4修改的。</p>
<h3 id="量化格式">3.4 量化格式</h3>
<p>以4bit量化为例：</p>
<p><strong>量化分组格式</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> QK4_0 32 <span class="comment">// 每组32个f32数据</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">    ggml_half d; <span class="comment">// 公共系数</span></span><br><span class="line">    <span class="type">uint8_t</span> qs[QK4_0 / <span class="number">2</span>]; <span class="comment">// 4bit存储的数据</span></span><br><span class="line">&#125; block_q4_0;</span><br></pre></td></tr></table></figure>
<p><strong>量化算法描述</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quantize_row_q4_0_reference</span><span class="params">(<span class="type">const</span> <span class="type">float</span> * restrict x, block_q4_0 * restrict y, <span class="type">int64_t</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="type">static</span> <span class="type">const</span> <span class="type">int</span> qk = QK4_0;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">assert</span>(k % qk == <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> nb = k / qk;</span><br><span class="line">		</span><br><span class="line">  	<span class="comment">// 1. 找到绝对值最大的数的值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nb; i++) &#123;</span><br><span class="line">        <span class="type">float</span> amax = <span class="number">0.0f</span>; <span class="comment">// absolute max</span></span><br><span class="line">        <span class="type">float</span> max  = <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; qk; j++) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> v = x[i*qk + j];</span><br><span class="line">            <span class="keyword">if</span> (amax &lt; <span class="built_in">fabsf</span>(v)) &#123;</span><br><span class="line">                amax = <span class="built_in">fabsf</span>(v);</span><br><span class="line">                max  = v;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 2. 公共系数是第一步的值除以 -8</span></span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> d  = max / <span class="number">-8</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> id = d ? <span class="number">1.0f</span>/d : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        y[i].d = <span class="built_in">GGML_FP32_TO_FP16</span>(d);</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 3. 对组内的所有数据，除以公共系数，然后按以下顺序存储</span></span><br><span class="line">        <span class="comment">// 量化前： 1,2,3,4,5,6,7,8 ...... 30,31</span></span><br><span class="line">      	<span class="comment">// 量化后： 1,17,2,18,3,19 ...... 16,32</span></span><br><span class="line">        <span class="comment">// 也就是数据按顺序先填充量化后组的低4位，然后再填充高4位。</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; qk/<span class="number">2</span>; ++j) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x0 = x[i*qk + <span class="number">0</span>    + j]*id;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x1 = x[i*qk + qk/<span class="number">2</span> + j]*id;</span><br><span class="line">						<span class="comment">// 量化的值+8.5，向上去整并转无符号数。</span></span><br><span class="line">            <span class="type">const</span> <span class="type">uint8_t</span> xi0 = <span class="built_in">MIN</span>(<span class="number">15</span>, (<span class="type">int8_t</span>)(x0 + <span class="number">8.5f</span>));</span><br><span class="line">            <span class="type">const</span> <span class="type">uint8_t</span> xi1 = <span class="built_in">MIN</span>(<span class="number">15</span>, (<span class="type">int8_t</span>)(x1 + <span class="number">8.5f</span>));</span><br><span class="line"></span><br><span class="line">            y[i].qs[j]  = xi0;</span><br><span class="line">            y[i].qs[j] |= xi1 &lt;&lt; <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>NPU善于做向量和矩阵计算，按字节的计算，以及位计算性能不佳。
所以需要调整数据存储格式。</p>
<p>在set
tensor过程中，使用cpu做tensor的内存调整，让后续的计算能够充分利用NPU能力。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/3580ce93f92d2075f1ee71f434f9d54a-image-20240806191922718-496986.png"
alt="量化内存格式调整示意图" />
<figcaption aria-hidden="true">量化内存格式调整示意图</figcaption>
</figure>
<p>如上图所示，输入的Tensor是按组存放的，每组存放了该组的公共系数，以及32个数据的量化后的值，int4类型量化值是先填充高4位，再填充低4位。为了昇腾的计算效率，在做这类伪量化算法时，将原始Tensor拆解成2个Tensor，一个按顺序记录所有的值，另外一个记录每一组的公共系数，并且值和公共系数按32:1的方式对应。然后昇腾算子按照weight和group
scale的方式输入进算子，能够提高量化后Tensor的执行效率。</p>
<p>所有的内存布局修改的时机是set tensor和get
tensor过程中，对于整个程序来说，对内存布局的修改是不感知的，拷贝到NPU上时进行布局修改，从NPU下载时再进行布局复原，这样，及时设计CPU，NPU混合运算，也不会影响数据的正确性。</p>
<p>对于更加复杂的量化方式，例如q5_0，需要设计到位运算，此类量化由于性能问题尚未支持。</p>
<h3 id="代码风格和注释">3.5 代码风格和注释</h3>
<p>llama.cpp社区对代码风格没有详细的要求，社区仅要求”清除所有尾随空格，使用
4 个空格缩进，括号在同一行<code>void * ptr</code>，int &amp;
a`”。并且，对注释也没有明确的要求。为了能够保持一致的风格，以及方便社区开发者了解昇腾后端的业务逻辑，需要在编写代码时遵循一致的编码规范和详尽的注释。</p>
<ul>
<li>除了社区要求的内容之外，其他代码规范需要遵循google编码规范；</li>
<li>注释需要包含函数和变量的介绍，参数和返回值说明，算子相关代码需要注释算法的数学公式。其他的复杂逻辑按需求添加注释；</li>
<li>注释需要符合doxygen风格，以便于生成方便阅读的手册。</li>
</ul>
<h2 id="测试和验证">4. 测试和验证</h2>
<p>本设计文档主要是昇腾的后端支持，llama.cpp已经做了后端抽象，所有测试用例可以复用社区的内容。针对社区用例没有看护到的部分，添加必要的用例来看护。</p>
<h3 id="runtime测试">4.1 Runtime测试</h3>
<p>Runtime测试主要是验证设备注册，内存分配，stream和event管理相关功能。</p>
<p><strong>设备注册和卸载（单卡，多卡）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证昇腾设备可以正常注册到llama.cpp中，支持单卡和多卡注册。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>调用设备注册接口，注册单卡以及多卡；</li>
<li>查看设备信息是否正常获取。</li>
</ol>
<p><strong>预期结果</strong></p>
<p>无报错信息，并根据<code>ASCEND_VISIBLE_DEVICES</code>的设置情况，能够正常获取到对应的设备信息。</p>
<p><strong>buffer和Tensor创建</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证昇腾后端可以正常的创建buffer以及llama.cpp的Tensor结构；</li>
</ul>
<p><strong>测试步骤</strong></p>
<ul>
<li>构造若干个Tensor结构，并给这些Tensor分配内存buffer；</li>
</ul>
<p><strong>预期结果</strong></p>
<ul>
<li>内存完成分配，无错误信息。</li>
</ul>
<p><strong>Tensor的上传和下载（同步，异步）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证数据可以正确的上传和下载。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机数据；</li>
<li>将数据拷贝到创建好的Tensor中；</li>
<li>将Tensor中的数据下载；</li>
<li>与原始随机数据进行对比。</li>
<li>分别使用同步拷贝和异步拷贝，重复以上过程。</li>
</ol>
<p><strong>预期结果</strong></p>
<ul>
<li>数据比对与原始数据相同。</li>
</ul>
<p><strong>Tensor卡间拷贝（包括event同步）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证卡间拷贝以及事件同步的正确性。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机数据；</li>
<li>将数据拷贝到卡1的Tensor中；</li>
<li>开启卡1和卡2的卡间拷贝开关；</li>
<li>在卡1的stream提交卡1Tensor向卡2Tensor拷贝的任务；</li>
<li>在拷贝流中插入卡2的event事件；</li>
<li>在卡2的流中等待event事件；</li>
<li>从卡2中下载Tensor数据；</li>
<li>与原始数据做比对。</li>
</ol>
<p><strong>预期结果</strong></p>
<ol type="1">
<li>数据比对与原始数据相同；</li>
<li>卡2event同步正确，在卡1stream中构造耗时操作，确保event能够等待拷贝动作结束。</li>
</ol>
<p><strong>量化拷贝验证</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>量化Tensor拷贝需要调整内存布局，验证量化Tensor的拷贝结果正确。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机的量化Tensor；</li>
<li>将量化Tensor上传到设备上；</li>
<li>使用aclrtmemcpy直接拷贝数据；</li>
<li>从设备上将Tensor下载下来；</li>
<li>与原始数据作对比。</li>
</ol>
<p><strong>预期结果</strong></p>
<ol type="1">
<li>步骤3的memcpy的结果与原始数据不同，因为上传过程做了内存布局调整；</li>
<li>步骤5数据对比与原始数据相同。</li>
</ol>
<h3 id="算子单元测试">4.2 算子单元测试</h3>
<p>单元测试复用社区的单元测试用例(test-backend-ops)，包含1500多个用例。其覆盖的场景有：</p>
<ul>
<li>算子多shape多dtype验证，保证该算子所有的输出输出的shape和dtype类型都能够覆盖；</li>
<li>算子的精度验证，用例会构造随机数据，分别在设备上和CPU上运行，最后对比精度，两个Tensor的归一化方差需要小于
1e-6。</li>
<li>计算结果越界检查，由于推理过程中，Tensor是紧密排列，所以每个tensor的计算结果不能越界，否则会损坏其他tensor的数据，用例会在每个输入和输出tensor前后分别放置一个随机tensor，通过对比随机tensor的计算前后的结果，来检查是否存在越界行为。</li>
</ul>
<p>单元测试用例会判断后端的算子支持情况，理论上，所有支持的算子（包括shape和dtype）都需要通过该测试用例集的验证。</p>
<h3 id="性能测试">4.3 性能测试</h3>
<p>算子的性能测试用例与单元测试用例相同，区别是性能测试用例不会验证精度，也不会创建随机tensor用作越界检查。性能测试会构造一个特殊的图，包含最多8192个计算节点，然后交给后端进行推理，并计算平均每次的执行时间，以及数据吞吐率。</p>
<ul>
<li>910B对于简单算子（包括直接调用aclnn接口的，或者做了简单的参数调整的）性能要超过Intel主流CPU的性能。</li>
<li>对于复杂算子（包括构造多个临时tensor，以及需要多个算子组合的）暂不做算子的性能要求。</li>
<li>非910B芯片，不做性能要求。</li>
</ul>
<p>910B模型推理（llama3
8B）整体性能，token延迟需要小于100ms（人类的阅读速度大致是10个token/s，延迟小于100ms，可以满足人类的阅读需求），吞吐需要超过300token/s（0.6
* A100 vllm llama3 8B的推理性能）。</p>
<p>以下为 Qwen 2.5 全系列模型在昇腾 910B 上的推理性能表现汇总数据，包括
Qwen2.5 0.5B、1.5B、3B 的 Q8_0 和Q4_0
量化的推理性能数据作为对比参考：</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 27%" />
<col style="width: 16%" />
<col style="width: 15%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><strong>Model</strong></th>
<th style="text-align: left;"><strong>Tokens</strong> <strong>/</strong>
<strong>Second</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Util</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Mem</strong></th>
<th style="text-align: left;"><strong>NPU Card（64G/Card）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B FP16</td>
<td style="text-align: left;">42 tokens/second</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B FP16</td>
<td style="text-align: left;">35 tokens/second</td>
<td style="text-align: left;">Util 11~13%</td>
<td style="text-align: left;">Mem 10%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B FP16</td>
<td style="text-align: left;">29 tokens/second</td>
<td style="text-align: left;">Util 15~16%</td>
<td style="text-align: left;">Mem 15%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 7B FP16</td>
<td style="text-align: left;">32 tokens/second</td>
<td style="text-align: left;">Util 16~21%</td>
<td style="text-align: left;">Mem 16%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 14B FP16</td>
<td style="text-align: left;">19 tokens/second</td>
<td style="text-align: left;">Util 19~22%</td>
<td style="text-align: left;">Mem 28%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 32B FP16</td>
<td style="text-align: left;">10.5 tokens/second</td>
<td style="text-align: left;">Util 10~45%</td>
<td style="text-align: left;">Mem 54%</td>
<td style="text-align: left;">双卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 72B FP16</td>
<td style="text-align: left;">6 tokens/second</td>
<td style="text-align: left;">Util 10~60%</td>
<td style="text-align: left;">Mem 78%</td>
<td style="text-align: left;">三卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B Q8_0</td>
<td style="text-align: left;">6.5 tokens/second</td>
<td style="text-align: left;">Util 2~5%</td>
<td style="text-align: left;">Mem 6%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B Q4_0</td>
<td style="text-align: left;">6 tokens/second</td>
<td style="text-align: left;">Util 4~5%</td>
<td style="text-align: left;">Mem 6%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B Q8_0</td>
<td style="text-align: left;">3.5 tokens/second</td>
<td style="text-align: left;">Util 4~11%</td>
<td style="text-align: left;">Mem 8%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B Q4_0</td>
<td style="text-align: left;">17~18 tokens/second</td>
<td style="text-align: left;">Util 9~12%</td>
<td style="text-align: left;">Mem 7%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B Q8_0</td>
<td style="text-align: left;">3.2 tokens/second</td>
<td style="text-align: left;">Util 10~15%</td>
<td style="text-align: left;">Mem 10%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B Q4_0</td>
<td style="text-align: left;">14.5 tokens/second</td>
<td style="text-align: left;">Util 8~15%</td>
<td style="text-align: left;">Mem 8%</td>
<td style="text-align: left;">单卡</td>
</tr>
</tbody>
</table>
<p>对其中的 Qwen 2.5 0.5B FP16 模型进行并发测试的性能表现如下：</p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 31%" />
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><strong>Concurrency</strong></th>
<th style="text-align: left;"><strong>Tokens</strong> <strong>/</strong>
<strong>Second</strong></th>
<th style="text-align: left;"><strong>Throughput</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Util</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Mem</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">39 tokens/second</td>
<td style="text-align: left;">39</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">38 tokens/second</td>
<td style="text-align: left;">76</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">37.66 tokens/second</td>
<td style="text-align: left;">113</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">34.25 tokens/second</td>
<td style="text-align: left;">137</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">31 tokens/second</td>
<td style="text-align: left;">155</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">28.16 tokens/second</td>
<td style="text-align: left;">169</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">27.57 tokens/second</td>
<td style="text-align: left;">193</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">26.87 tokens/second</td>
<td style="text-align: left;">215</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">26 tokens/second</td>
<td style="text-align: left;">234</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">10</td>
<td style="text-align: left;">26.9 tokens/second</td>
<td style="text-align: left;">269</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">20</td>
<td style="text-align: left;">20.3 tokens/second</td>
<td style="text-align: left;">406</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 8%</td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">10.34 tokens/second</td>
<td style="text-align: left;">517</td>
<td style="text-align: left;">Util 3~5%</td>
<td style="text-align: left;">Mem 8%</td>
</tr>
<tr>
<td style="text-align: left;">100</td>
<td style="text-align: left;">4.17 tokens/second</td>
<td style="text-align: left;">417</td>
<td style="text-align: left;">Util 2~5%</td>
<td style="text-align: left;">Mem 9%</td>
</tr>
</tbody>
</table>
<h3 id="模型精度验证">4.4 模型精度验证</h3>
<p>除了算子的精度验证以外，对模型需要做整体的精度验证，以避免在数据加载拷贝，kv_cache操作等过程中出现错误。</p>
<p><strong>eval-callback</strong></p>
<p>llama.cpp社区提供了一个精度对比工具：eval-callback，这个工具会执行一次推理过程，并将推理过程中所有涉及的算子的计算结果进行打印。通过对比相同seed情况下的NPU和CPU的推理结果，判断整个推理过程是否存在异常。</p>
<p>需要注意的是，tensor的内容在会存在微小的差异，这不属于精度异常。</p>
<p><strong>CPU推理对比</strong></p>
<p>使用llama3模型，使用相同的seed，分别在NPU和CPU上进行相同的推理内容，理论上前数百token应该完全一致。由于存在精度的微小差异，推理累计的过程中，在长回复的后段，可能会出现细微差异。</p>
<h3 id="模型支持验证">4.5 模型支持验证</h3>
<p>目前，llama.cpp支持以下模型以及多种量化格式，我们仅关注fp16，Q8_0和Q4_0三种dtype。</p>
<p>模型支持的原则是不存在不支持的算子，检查方式是查看切图的情况，如果出现了大量子图（超过100），说明存在算子不支持，已经fallback到CPU进行推理，此类模型虽然能够完成推理，但是推理性能较低。</p>
<table>
<thead>
<tr>
<th style="text-align: center;">模型</th>
<th style="text-align: center;">FP16</th>
<th style="text-align: center;">Q8_0</th>
<th style="text-align: center;">Q4_0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AquilaChat2-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Baichuan-7b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Baichuan2-7B-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bitnet_b1_58-large</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bloom-560m</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bloomz-alpaca-560m</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">c4ai-command-r-35B-v01</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">chatglm3-6B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">chinese-alpaca-2-1.3b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">CodeShell-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td
style="text-align: center;">deepseek-ai_deepseek-coder-1.3B-base</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">deepseek-ai_DeepSeek-V2-Lite</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">deepseek-coder-6.7B-instruct</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-V2-Lite-64x1.5B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">falcon-7b-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">flan-t5-large</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">gemma-2-9b-it</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">glm-4-9B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">gpt2</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Gpt2-163M</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">granite-3B-code-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">GritLM-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">internlm2_5-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">koala-7B-HF</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7b-chat-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-Smaug-8B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-Chinese-7b-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama3-8B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama3-8b-chinese</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">mamba-130m-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">mpt-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">OLMo-1B-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">OpenELM-3B-Instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Orion-14b-base</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">phi1</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">phi2</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">Phi-3-mini-4k-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">plamo-13b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">pythia-70M</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-1.5B-Instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Refact-1_6B-fim</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">SmolLM-135M</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">stablelm-zephyr</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">stablelm-2-zephyr-1_6b</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">starcoderbase-1b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">starcoder2-3b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">vigogne-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">xverse-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Yi-6b-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
</tbody>
</table>
<h3 id="社区ci">4.6 社区CI</h3>
<p>目前由于资源限制，暂时无法向社区提供开发机和CI机器，但是需要保证编译通过，防止社区的重构导致的昇腾后端被破坏的问题。编译不需要昇腾硬件，可以使用社区的CI机器。</p>
<ul>
<li>提供昇腾构建的容器镜像，避免配置复杂的环境。</li>
<li>提供github workflow的job，添加昇腾的CI验证，并作为门禁。</li>
</ul>
<h2 id="ollama支持">5. Ollama支持</h2>
<p>Ollama
是一个旨在提升本地大型语言模型（LLM）运行效率和灵活性的开源平台，快速在本地部署启动大模型的应用。Ollama
的设计初衷是通过优化硬件加速和支持更高效的推理计算，帮助开发者和研究人员更方便地在本地部署和运行
LLM，从而不依赖云计算资源或其他昂贵的基础设施。，Ollama使用llama.cpp作为推理引擎。一条命令可以完成安装和模型拉起。</p>
<p><strong>安装</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>
<p><strong>运行</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3</span><br></pre></td></tr></table></figure>
<p>除此之外，Ollama还有有一个模型的仓库，保存有海量的gguf模型，其兼容openAI
API，有着众多的前端应用。</p>
<p>为了能够充分的利用llama.cpp的昇腾后端能力，简化昇腾使用门槛，同时需要完成Ollama的昇腾适配。简单来说，OIlama需要适配一下几个关键部分：</p>
<p><strong>构建</strong></p>
<p>Ollama会构建llama.cpp工程，并将二进制打包到ollama的二进制文件中，在构建ollama的过程中，需要完成llama.cpp的昇腾版本的构建。</p>
<p><strong>NPU检测</strong></p>
<p>Ollama在运行时会检测NPU硬件，显存容量等，来判断模型是否能够运行，以及合理的模型拆分方式，所以需要在ollama中实现必要的昇腾硬件检测接口。</p>
<p><strong>拉起</strong></p>
<p>Ollama运行模型时，会拉起对应后端的llama.cpp服务器，这里需要根据硬件检测的结果来拉起NPU版本的llama.cpp服务器。</p>
<p>这里仅做Ollama兼容昇腾后端的简单洞察，不做详细设计，社区方案已完成，PR提交中。</p>
<h2 id="社区跟进">6. 社区跟进</h2>
<p>llama.cpp是一个非常活跃的社区，平均每天有十几个提交的合入，包括大量的重构和大粒度特性的合入。昇腾后端需要紧跟社区的发展路线，根据社区的重构和特性进行适配。</p>
<p>同时，在社区也存在对昇腾后端的需求，以及问题反馈，需要及时完成解决。</p>
<p>社区没有要求SLA，原则上，简单问题修复和重构适配应当在5个工作日内完成，特性需求根据实际情况灵活处理。</p>
<h2 id="文档和说明">7. 文档和说明</h2>
<p>为了帮助llama.cpp的昇腾用户，需要编写详尽的文档，包括环境搭建，构建，运行，模型和数据类型支持情况以及贡献指导等。</p>
<h3 id="社区doc">7.1 社区doc</h3>
<ul>
<li>在社区README添加Ascend的支持描述，并且提供跳转链接。</li>
<li>提供环境搭建步骤，包括操作系统版本，昇腾驱动和CANN的版本要求和安装方法。</li>
<li>提供Dockerfile，包含llama.cpp所需的环境配置，能够避免复杂的环境部署。</li>
<li>提供构建，运行的命令。</li>
<li>提供模型和数据类型支持情况。</li>
<li>提供issue和PR提交规范。</li>
</ul>
<h3 id="昇腾开源手册">7.2 昇腾开源手册</h3>
<p>为了方便中文用户，以及昇腾社区入口的用户，还需要在昇腾开源文档中提供中文版的step
by step构建和推理手册。</p>
<table>
<colgroup>
<col style="width: 85%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th>PR</th>
<th>代码量</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/6035">[CANN]
Add Ascend NPU backend #6035</a></td>
<td>+10,756 −8</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/8867">[CANN]
Add doc and docker image #8867</a></td>
<td>+329 −0</td>
</tr>
</tbody>
</table>
<p>其他参与review的PR和issue见<a
target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/issues?q=+label%3A%22Ascend+NPU%22+">链接</a>。</p>
<p><a
target="_blank" rel="noopener" href="https://ascend.github.io/docs/sources/llama_cpp/index.html">llama.cpp昇腾开源使用手册</a>。</p>
<h2 id="项目引用">8. 项目引用</h2>
<p><strong>ollama</strong></p>
<p><a
target="_blank" rel="noopener" href="https://github.com/ollama/ollama">Ollama</a>是一款专注于在本地运行大型语言模型的工具，旨在简化模型的部署和使用，提供高性能且无需云端依赖的AI推理体验，使用llama.cpp作为推理引擎，以git
submodule的方式引用llama.cpp代码。目前已与2012同事一同完成设计并提交<a
target="_blank" rel="noopener" href="https://github.com/ollama/ollama/pull/5872">PR</a>。</p>
<p><strong>llama edge</strong></p>
<p><a target="_blank" rel="noopener" href="https://github.com/LlamaEdge/LlamaEdge">Llama
Edge</a>是一个为边缘设备优化的轻量级大语言模型框架，旨在支持本地化、高效的推理，以满足低延迟和有限资源的计算需求，使用llama.cpp作为其推理后端。llama
edge官方发表了一篇知乎的<a
target="_blank" rel="noopener" href="https://www.zhihu.com/question/624955377/answer/13849002583?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_psn=1833491376626614272&amp;utm_source=wechat_session&amp;utm_id=0">回复</a>以及一篇<a
target="_blank" rel="noopener" href="https://www.secondstate.io/articles/llm-agents-on-ascend/">官方文档</a>。</p>
<p><strong>llamabox&amp;gpu stack</strong></p>
<p><a
target="_blank" rel="noopener" href="https://github.com/gpustack/llama-box">Llamabox</a>是一个便捷的平台，提供开箱即用的大语言模型部署方案，使用户能够轻松运行和管理AI模型；而<a
target="_blank" rel="noopener" href="https://github.com/gpustack/gpustack">gpustack</a>是一项云服务，专为高性能计算和AI模型训练优化，提供灵活的GPU资源共享和管理功能，其使用了llama.cpp作为其推理后端之一。有一篇使用gpustack使用昇腾推理的实践<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/KZ-kuNUx03BPo9vlK1OPxg">文章</a>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/25/%E6%8E%A8%E7%90%86%E5%85%A8%E5%9C%BA%E6%99%AF%E6%B4%9E%E5%AF%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/25/%E6%8E%A8%E7%90%86%E5%85%A8%E5%9C%BA%E6%99%AF%E6%B4%9E%E5%AF%9F/" class="post-title-link" itemprop="url">推理全场景洞察</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-12-25 14:20:26 / 修改时间：14:25:58" itemprop="dateCreated datePublished" datetime="2023-12-25T14:20:26+08:00">2023-12-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="推理场景">推理场景</h2>
<ul>
<li><p><strong>计算机视觉：</strong>图像分类，目标检测，人脸识别，图像生成。</p></li>
<li><p><strong>自然语言处理（NLP)：</strong>翻译，语音识别，文本分类，内容审核。</p></li>
<li><p><strong>自动驾驶</strong></p></li>
<li><p><strong>生物学：</strong>医学影响分析，基因分析，蛋白质预测。</p></li>
<li><p><strong>金融：</strong>信用，风控。</p></li>
<li><p><strong>制造业：</strong>质量控制，成本控制。</p></li>
<li><p><strong>零售业：</strong>推荐广告系统，库存管理。</p></li>
<li><p><strong>能源：</strong>智能电网。</p></li>
<li><p><strong>大模型推理：</strong>LLM，多模态模型推理。</p></li>
</ul>
<h2 id="推理的一般流程">推理的一般流程</h2>
<p>以目标检测为例：</p>
<ol type="1">
<li><p><strong>预训练模型获取</strong>：从
Huggingface或者其他预训练模型平台上下载模型；</p></li>
<li><p><strong>模型转换：</strong>根据使用的推理框架，将下载的模型转换成对应的格式；</p></li>
<li><p><strong>数据预处理：</strong>视频解码，并从流中抓取图像，将图像进行裁剪，旋转，翻转，调整色彩空间，标准化等操作；</p></li>
<li><p><strong>模型推理：</strong>将处理完成的图像数据输入模型进行推理，得到推理结果；</p></li>
<li><p><strong>后处理：</strong>将模型推理结果进行处理，获取目标框，标签等信息；</p></li>
<li><p><strong>结果展示：</strong>将原始图像和推理结果进行融合，给检测的目标加上框和目标类型标签。</p></li>
</ol>
<h2 id="推理技术栈">推理技术栈</h2>
<p><strong>Nvidia</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116101903233.png"
alt="image-20231116101903233" />
<figcaption aria-hidden="true">image-20231116101903233</figcaption>
</figure>
<p><strong>昇腾</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116101849666.png"
alt="image-20231116101849666" />
<figcaption aria-hidden="true">image-20231116101849666</figcaption>
</figure>
<p>推理技术栈自下而上一般为：</p>
<ul>
<li>硬件：参与推理的硬件，例如CPU，GPU和NPU等，Nvidia目前常用的GPU为Volta-&gt;Turing-&gt;Ampere-&gt;Hopper等架构，Ascend为310系列和910系列芯片，
采用Davinci架构。</li>
<li>驱动和基础软件：此类软件包括加速卡的驱动程序，异构计算运行时（CUDA
RT, CANN
RT)，kernel开发调试工具等。除此之外，Ascend还提供了常用算子库。</li>
<li>推理引擎：推理引擎一般提供模型转换，模型优化，以及模型推理功能，并且提供运行的性能指标供性能分析和自动负载均衡。
大部分推理引擎都原生支持CUDA，对昇腾的原生支持较弱。</li>
<li>推理服务化：推理服务化工具一般提供restful和rpc接口，模型服务化部署。另外可以配合容器技术，调度技术和负载均衡等实现自动扩缩容，提高推理速度，提高资源利用率。基本上大多深度学习框架均提供了服务化部署能力，其中Triton支持多种后端，并提供了友好的接入接口。</li>
<li>行业应用：针对特定行业的预处理，pipeline或者相关的SDK用于简化行业应用的开发复杂度，甚至通过配置可以直接在行业内应用。</li>
<li>其他配套：其他配套例如预训练模型的仓库，模型调优工具，算法加速库以及边缘计算平台等。</li>
</ul>
<h2 id="推理流程中涉及的软件">推理流程中涉及的软件</h2>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116102732237.png"
alt="image-20231116102732237" />
<figcaption aria-hidden="true">image-20231116102732237</figcaption>
</figure>
<ul>
<li>模型预处理hub：hub不仅能够保存预训练模型，并且能够通过代码api的方式直接下载并加载模型。昇腾的Model
Zoo需要手动下载模型，Huggingface的预训练模型可以通过python
api下载和使用。</li>
<li>模型转换：ONNX是一种开发的模型格式，可以与常见的深度学习框架进行转换。除此之外，其他的框架一般提供有限的模型转换能力，大多是同一个框架内的不同模型格式的转换。</li>
<li>预处理：计算机视觉中，Nvidia自研了部分适配GPU的加速库，并且常见的OpenCV，torchvision也原生支持GPU。目前对昇腾的支持较弱，目前仅有mxBase和OpenCV实现了少量的常用接口。昇腾推理的CV预处理，还需要依赖CPU处理。自然语言处理库由于算法的特殊性，无法充分利用并行计算能力，上述库基本上都仅在CPU上运行。Nvidia提供了推荐系统大量数据并能处理能力的库NVTabular，昇腾在此方面可以使用mxRec提供的加速能力。</li>
<li>模型分析优化：推理框架一般分为优化和运行两部分，其中优化部分对传入的模型根据底层架构进行优化。并且在执行过程中可以通过组件监控模型的运行情况，以用来模型调优，或者提供弹性扩缩容能力。</li>
<li>推理框架：推理框架是推理业务的重点，不同的推理框架能力各有优劣：
<ul>
<li><strong>TensorRT (TensorRT by NVIDIA):</strong>
<ul>
<li>优点：
<ul>
<li>面向 NVIDIA GPU 的深度学习推理优化库。</li>
<li>针对高性能、低延迟的推理任务进行了优化。</li>
</ul></li>
<li>缺点：
<ul>
<li>仅适用于 NVIDIA GPU，不具备跨平台性。</li>
</ul></li>
</ul></li>
<li><strong>ONNX Runtime:</strong>
<ul>
<li>优点：
<ul>
<li>开放的模型表示格式，允许在不同框架之间共享和部署模型。</li>
<li>支持多种深度学习框架，如TensorFlow、PyTorch、Caffe等。</li>
</ul></li>
<li>缺点：
<ul>
<li>部分框架的支持可能不如原生框架的性能优越。</li>
</ul></li>
</ul></li>
<li><strong>OpenVINO</strong>
<ul>
<li>优点：
<ul>
<li>多平台支持，支持多深度学习框架。</li>
<li>针对各设备硬件进行优化，能在多种设备上高性能推理。</li>
</ul></li>
<li>缺点：
<ul>
<li>大量优化针对Intel硬件，对其他硬件厂商的优化有限。</li>
<li>开源版本功能限制，有些特性需要商业版支持。</li>
</ul></li>
</ul></li>
<li><strong>ncnn， TNN， MNN，ARMNN</strong>
<ul>
<li>优点：
<ul>
<li>面向移动端和嵌入式CPU或GPU，轻量级，弱依赖。</li>
<li>支持多种模型类型，有模型转换能力。</li>
</ul></li>
<li>缺点：
<ul>
<li>非嵌入式平台（ARMNN在非ARM平台）支持较弱。</li>
</ul></li>
</ul></li>
</ul></li>
<li>推理服务化：主流的深度学习框架基本上都提供了服务化能力，可以通过restful或者rpc接口进行模型推理。其中Triton设计更为灵活，能够方便的集成不同的后端，目前已经支持主流深度学习框架的推理服务。目前还没有支持昇腾推理框架，但是可以通过pytorch插件或者ONNX
runtime进行推理。</li>
<li>后处理：后处理将图例结果进行加工处理，并展示推理结果，所需软件与预处理大致相同。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20231116102541095.png"
alt="image-20231116102541095" />
<figcaption aria-hidden="true">image-20231116102541095</figcaption>
</figure>
<p>除了推理流程中的软件之外，还有行业应用，边缘计算，以及算法加速库。Nvidia和昇腾在这些领域均有涉及。</p>
<h2 id="总结">总结</h2>
<ol type="1">
<li>昇腾在主流深度学习框架，推理框架以及推理服务化软件中，原生支持较弱，大部分框架在设计之初均考虑GPU支持，目前已经支持昇腾的框架多为后期开发。如果框架在后端支持上设计不够友好，接入难度较高。</li>
<li>计算机视觉预处理能力与GPU能力差距较大，包括OpenCV，torchvision等开源CV软件均原生支持GPU，并且Nvidia还有自研的图像预处理库，昇腾仅支持少量高频使用的接口，并且性能还存在差距。</li>
<li>Nvidia和开源在框架，应用软件和加速库的使用上较为容易，社区活跃，文档完整规范，学习成本低。昇腾相关软件使用门槛较高，使用上相较而言较为繁琐。</li>
<li>建议在推理全流程中选择一个技术路线，做昇腾支持，在功能，性能上追平或超过友商，然后再考虑自研更适合昇腾场景的自研软件。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/12/AscendC-vs-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/12/AscendC-vs-CUDA/" class="post-title-link" itemprop="url">AscendC vs CUDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-10-12 09:35:14 / 修改时间：09:38:04" itemprop="dateCreated datePublished" datetime="2023-10-12T09:35:14+08:00">2023-10-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="什么是ascendccuda编程">什么是AscendC/CUDA编程</h2>
<blockquote>
<p>面向算子开发场景的编程语言Ascend
C，原生支持C和C++标准规范，最大化匹配用户开发习惯；通过多层接口抽象、自动并行计算、孪生调试等关键技术，极大提高算子开发效率，助力AI开发者低成本完成算子开发和模型调优部署。</p>
</blockquote>
<blockquote>
<p><strong>CUDA</strong>（<strong>C</strong>ompute
<strong>U</strong>nified <strong>D</strong>evice
<strong>A</strong>rchitecture，<strong>统一计算架构</strong>[<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/CUDA#cite_note-1">1]</a>）是由英伟达<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/NVIDIA">NVIDIA</a>所推出的一种<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/軟體">软</a><a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/计算机硬件">硬件</a>集成技术，是该公司对于<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GPGPU">GPGPU</a>的正式名称。透过这个技术，用户可利用NVIDIA的<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GPU">GPU</a>进行<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/图像处理">图像处理</a>之外的运算，亦是首次可以利用GPU作为C-编译器的开发环境。</p>
</blockquote>
<p><strong>一句话概括：AscendC/CUDA就是使用昇腾设备/GPU设备的编程接口。</strong></p>
<h2 id="与我们熟悉的编程有什么区别">与我们熟悉的编程有什么区别</h2>
<p><strong>内存</strong></p>
<p>Host编程仅考虑主存，所有的内存操作对象均为主存，不需要考虑CPU缓存，寄存器等，这些对程序开发完全透明。</p>
<p>Device编程需要了解每个运行单元能访问的内存类型，可以理解要手动管理一级二级缓存，例如，AscendC变成框架下，内存的类型有：</p>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 89%" />
</colgroup>
<thead>
<tr>
<th>枚举值</th>
<th>具体含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>GM</td>
<td>Global Memory，对应AI Core的外部存储。</td>
</tr>
<tr>
<td>VECIN</td>
<td>用于矢量计算，搬入数据的存放位置，在数据搬入Vector计算单元时使用此位置</td>
</tr>
<tr>
<td>VECOUT</td>
<td>用于矢量计算，搬出数据的存放位置，在将Vector计算单元结果搬出时使用此位置</td>
</tr>
<tr>
<td>VECCALC</td>
<td>用于矢量计算/矩阵计算，在计算需要临时变量时使用此位置</td>
</tr>
<tr>
<td>A1</td>
<td>用于矩阵计算，存放整块A矩阵，可类比CPU多级缓存中的二级缓存</td>
</tr>
<tr>
<td>B1</td>
<td>用于矩阵计算，存放整块B矩阵，可类比CPU多级缓存中的二级缓存</td>
</tr>
<tr>
<td>A2</td>
<td>用于矩阵计算，存放切分后的小块A矩阵，可类比CPU多级缓存中的一级缓存</td>
</tr>
<tr>
<td>B2</td>
<td>用于矩阵计算，存放切分后的小块B矩阵，可类比CPU多级缓存中的一级缓存</td>
</tr>
<tr>
<td>CO1</td>
<td>用于矩阵计算，存放小块结果C矩阵，可理解为Cube Out</td>
</tr>
<tr>
<td>CO2</td>
<td>用于矩阵计算，存放整块结果C矩阵，可理解为Cube Out</td>
</tr>
</tbody>
</table>
<p>不同的处理单元，不同的处理步骤访问的内存是不同的，需要开发者自行处理。</p>
<p><strong>编程模型</strong></p>
<p>Host编程一般为串行的，如果想启用并行处理需要手动开启多线程，或者SIMD(Single
Instruction, Multiple Data)。</p>
<p>Device编程一般为并行，SPMD(Single-Program
Multiple-Data)。在设备上启动多线程，共同处理一份数据。Device编程代码分为两个部分，Host侧执行的一般代码和在设备上执行的核函数(kernel
function)。</p>
<p>AscendC还需要注意的是流水线编程范式，流水线编程主要是为了加速数据拷贝，Device处理以及数据拷回的流程。因为DMA搬运单元，各个计算单元是并行工作的，使用流水线能够提高设备单元的使用率。</p>
<h2 id="device-的内部结构抽象">Device 的内部结构抽象</h2>
<p><strong>Ascend AI Core 内部抽象结构</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2021051922433521.png"
alt="达芬奇架构" />
<figcaption aria-hidden="true">达芬奇架构</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001706909793.png"
alt="AI Core抽象结构" />
<figcaption aria-hidden="true">AI Core抽象结构</figcaption>
</figure>
<p><strong>CUDA核心内部抽象结构</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/70.png"
alt="CUDA核心结构" />
<figcaption aria-hidden="true">CUDA核心结构</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/70-20231010170118685.png"
alt="CUDA核心结构" />
<figcaption aria-hidden="true">CUDA核心结构</figcaption>
</figure>
<p><strong>AI Core和Stream Multiprocessor的最主要区别是：</strong></p>
<ul>
<li><p><strong>AI
Core中是专用处理单元，包括Vector和Cube，分别用户向量和矩阵运算，能用向量和矩阵运算的操作效率会很高。</strong></p></li>
<li><p><strong>Stream
Multiprocessor基本上都是大量的int32核心，float32核心或者双精度核心，由于数量众多，所以并行能力更强。</strong></p></li>
</ul>
<h2 id="ascendc编程和cuda编程对比">AscendC编程和CUDA编程对比</h2>
<p><strong>AscendC</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;acl/acl.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tikicpulib.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel_operator.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;data_loader.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> AscendC;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __aicore__</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __aicore__ [aicore]</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BUFFER_NUM = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BLOCK_DIM = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*****************************Copy scalar to ubuf*****************************/</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">FlipTilingData</span> &#123;</span><br><span class="line">  <span class="type">uint32_t</span> height;</span><br><span class="line">  <span class="type">uint32_t</span> width;</span><br><span class="line">  <span class="type">uint32_t</span> channel;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> __aicore__ <span class="type">int32_t</span> <span class="title">align32</span><span class="params">(<span class="type">int32_t</span> n)</span> </span>&#123; <span class="keyword">return</span> ((n + <span class="number">31</span>) &amp; ~<span class="number">31</span>); &#125;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> __aicore__ <span class="type">int32_t</span> <span class="title">AlignDiv32</span><span class="params">(<span class="type">int32_t</span> n)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">align32</span>(n) / <span class="number">32</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  __ubuf__ tilingStruct* tilingDataPointer =                                \</span></span><br><span class="line"><span class="meta">      reinterpret_cast<span class="string">&lt;__ubuf__ tilingStruct*&gt;</span>(                             \</span></span><br><span class="line"><span class="meta">          (__ubuf__ uint8_t*)(tilingPointer));</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INIT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer);</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INIT_TILING_DATA(tilingStruct, tilingDataPointer, tilingPointer) \</span></span><br><span class="line"><span class="meta">  __ubuf__ uint8_t* tilingUbPointer = (__ubuf__ uint8_t*)get_imm(0);     \</span></span><br><span class="line"><span class="meta">  copy_gm_to_ubuf(((__ubuf__ uint8_t*)(tilingUbPointer)),                \</span></span><br><span class="line"><span class="meta">                  ((__gm__ uint8_t*)(tilingPointer)), 0, 1,              \</span></span><br><span class="line"><span class="meta">                  AlignDiv32(sizeof(tilingStruct)), 0, 0);               \</span></span><br><span class="line"><span class="meta">  CONVERT_TILING_DATA(tilingStruct, tilingDataPointer, tilingUbPointer); \</span></span><br><span class="line"><span class="meta">  pipe_barrier(PIPE_ALL);</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GET_TILING_DATA(tilingData, tilingPointer) \</span></span><br><span class="line"><span class="meta">  INIT_TILING_DATA(FlipTilingData, tilingData, tilingPointer);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_ACL(x)                                                    \</span></span><br><span class="line"><span class="meta">  do &#123;                                                                  \</span></span><br><span class="line"><span class="meta">    aclError __ret = x;                                                 \</span></span><br><span class="line"><span class="meta">    <span class="keyword">if</span> (__ret != ACL_ERROR_NONE) &#123;                                      \</span></span><br><span class="line"><span class="meta">      std::cerr &lt;&lt; __FILE__ &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; __LINE__ &lt;&lt; <span class="string">&quot; aclError:&quot;</span> &lt;&lt; __ret \</span></span><br><span class="line"><span class="meta">                &lt;&lt; std::endl;                                           \</span></span><br><span class="line"><span class="meta">    &#125;                                                                   \</span></span><br><span class="line"><span class="meta">  &#125; while (0);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*******************************Kernel function*******************************/</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KernelFlip</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="title">KernelFlip</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Init</span><span class="params">(GM_ADDR input, GM_ADDR output, <span class="type">uint32_t</span> _height,</span></span></span><br><span class="line"><span class="params"><span class="function">                              <span class="type">uint32_t</span> _width, <span class="type">uint32_t</span> _channel)</span> </span>&#123;</span><br><span class="line">    <span class="type">uint32_t</span> blockNum = <span class="built_in">GetBlockNum</span>();</span><br><span class="line">    <span class="type">uint32_t</span> blockIdx = <span class="built_in">GetBlockIdx</span>();</span><br><span class="line"></span><br><span class="line">    rowLength = _height / blockNum;</span><br><span class="line">    startRowIdx = blockIdx * rowLength;</span><br><span class="line">    <span class="keyword">if</span> (startRowIdx + rowLength &gt; _height) &#123;</span><br><span class="line">      rowLength = _height - startRowIdx;</span><br><span class="line">    &#125;</span><br><span class="line">    width = _width;</span><br><span class="line">    height = _height;</span><br><span class="line">    channel = _channel;</span><br><span class="line">    rowSize = width * channel;</span><br><span class="line">    <span class="type">uint32_t</span> bufferSize = <span class="built_in">align32</span>(rowSize * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>));</span><br><span class="line"></span><br><span class="line">    inputGM.<span class="built_in">SetGlobalBuffer</span>((__gm__ <span class="type">uint8_t</span>*)input + startRowIdx * rowSize,</span><br><span class="line">                            rowLength * rowSize);</span><br><span class="line">    outputGM.<span class="built_in">SetGlobalBuffer</span>((__gm__ <span class="type">uint8_t</span>*)output + startRowIdx * rowSize,</span><br><span class="line">                             rowLength * rowSize);</span><br><span class="line">    pipe.<span class="built_in">InitBuffer</span>(inQueue, BUFFER_NUM, bufferSize);</span><br><span class="line">    pipe.<span class="built_in">InitBuffer</span>(outQueue, BUFFER_NUM, bufferSize);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Process</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int32_t</span> i = <span class="number">0</span>; i &lt; rowLength; i++) &#123;</span><br><span class="line">      <span class="built_in">CopyIn</span>(i);</span><br><span class="line">      <span class="built_in">Compute</span>(i);</span><br><span class="line">      <span class="built_in">CopyOut</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">CopyIn</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; local = inQueue.<span class="built_in">AllocTensor</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="built_in">DataCopy</span>(local, inputGM[loop * rowSize], rowSize);</span><br><span class="line">    inQueue.<span class="built_in">EnQue</span>(local);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">Compute</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; inputLocal = inQueue.<span class="built_in">DeQue</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; outputLocal = outQueue.<span class="built_in">AllocTensor</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int32_t</span> i = <span class="number">0</span>; i &lt; width; i++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int32_t</span> c = <span class="number">0</span>; c &lt; channel; c++) &#123;</span><br><span class="line">        outputLocal.<span class="built_in">SetValue</span>(</span><br><span class="line">            i * channel + c,</span><br><span class="line">            inputLocal.<span class="built_in">GetValue</span>((width - i - <span class="number">1</span>) * channel + c));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    outQueue.<span class="built_in">EnQue</span>&lt;<span class="type">uint8_t</span>&gt;(outputLocal);</span><br><span class="line">    inQueue.<span class="built_in">FreeTensor</span>(inputLocal);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">__aicore__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title">CopyOut</span><span class="params">(<span class="type">int32_t</span> loop)</span> </span>&#123;</span><br><span class="line">    LocalTensor&lt;<span class="type">uint8_t</span>&gt; local = outQueue.<span class="built_in">DeQue</span>&lt;<span class="type">uint8_t</span>&gt;();</span><br><span class="line">    <span class="built_in">DataCopy</span>(outputGM[loop * rowSize], local, rowSize);</span><br><span class="line">    outQueue.<span class="built_in">FreeTensor</span>(local);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  TPipe pipe;</span><br><span class="line">  TQue&lt;QuePosition::VECIN, BUFFER_NUM&gt; inQueue;</span><br><span class="line">  TQue&lt;QuePosition::VECOUT, BUFFER_NUM&gt; outQueue;</span><br><span class="line">  GlobalTensor&lt;<span class="type">uint8_t</span>&gt; inputGM, outputGM;</span><br><span class="line">  <span class="type">uint32_t</span> startRowIdx, rowLength, rowSize, height, width, channel;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*******************************kernel interface******************************/</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> <span class="function">__global__ __aicore__ <span class="type">void</span> <span class="title">flip</span><span class="params">(GM_ADDR input, GM_ADDR output,</span></span></span><br><span class="line"><span class="params"><span class="function">                                           GM_ADDR tiling)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">GET_TILING_DATA</span>(tilingData, tiling);</span><br><span class="line">  KernelFlip op;</span><br><span class="line">  op.<span class="built_in">Init</span>(input, output, tilingData-&gt;height, tilingData-&gt;width,</span><br><span class="line">          tilingData-&gt;channel);</span><br><span class="line">  op.<span class="built_in">Process</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> __CCE_KT_TEST__</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">flip_do</span><span class="params">(<span class="type">uint32_t</span> blockDim, <span class="type">void</span>* l2ctrl, <span class="type">void</span>* stream, <span class="type">uint8_t</span>* input,</span></span></span><br><span class="line"><span class="params"><span class="function">             <span class="type">uint8_t</span>* output, <span class="type">uint8_t</span>* tiling)</span> </span>&#123;</span><br><span class="line">  flip&lt;&lt;&lt;blockDim, l2ctrl, stream&gt;&gt;&gt;(input, output, tiling);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************************caller************************************/</span></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">main</span><span class="params">(<span class="type">int32_t</span> argc, <span class="type">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">&quot; path/to/datafile&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> blockDim = BLOCK_DIM;</span><br><span class="line">  <span class="type">uint32_t</span> height, width, channel;</span><br><span class="line">  <span class="type">uint8_t</span>* data = <span class="built_in">readFile</span>(argv[<span class="number">1</span>], height, width, channel);</span><br><span class="line">  <span class="type">const</span> <span class="type">char</span>* resultFile = std::<span class="built_in">string</span>(argv[<span class="number">1</span>]).<span class="built_in">append</span>(<span class="string">&quot;.ret&quot;</span>).<span class="built_in">c_str</span>();</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> dataSize = width * height * channel * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>);</span><br><span class="line">  <span class="type">size_t</span> inputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> outputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> tilingSize = <span class="built_in">sizeof</span>(FlipTilingData);</span><br><span class="line"></span><br><span class="line">  <span class="type">uint8_t</span> *inputHost, *outputHost, *tilingHost;</span><br><span class="line">  <span class="type">uint32_t</span> shape[]&#123;height, width, channel&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CCE_KT_TEST__</span></span><br><span class="line">  inputHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(inputByteSize);</span><br><span class="line">  outputHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(outputByteSize);</span><br><span class="line">  tilingHost = (<span class="type">uint8_t</span>*)AscendC::<span class="built_in">GmAlloc</span>(tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(tilingHost, shape, tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(inputHost, data, dataSize);</span><br><span class="line"></span><br><span class="line">  AscendC::<span class="built_in">SetKernelMode</span>(KernelMode::AIV_MODE);</span><br><span class="line">  <span class="built_in">ICPU_RUN_KF</span>(flip, blockDim, inputHost, outputHost, tilingHost);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, outputHost);</span><br><span class="line"></span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)inputHost);</span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)outputHost);</span><br><span class="line">  AscendC::<span class="built_in">GmFree</span>((<span class="type">void</span>*)tilingHost);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclInit</span>(<span class="literal">nullptr</span>));</span><br><span class="line">  aclrtContext context;</span><br><span class="line">  <span class="type">int32_t</span> deviceId = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtSetDevice</span>(deviceId));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtCreateContext</span>(&amp;context, deviceId));</span><br><span class="line">  aclrtStream stream = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtCreateStream</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">  <span class="type">uint8_t</span> *inputDevice, *outputDevice, *tilingDevice;</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;tilingHost), tilingSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;inputHost), inputByteSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMallocHost</span>((<span class="type">void</span>**)(&amp;outputHost), outputByteSize));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;inputDevice, inputByteSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;outputDevice, outputByteSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMalloc</span>((<span class="type">void</span>**)&amp;tilingDevice, tilingSize,</span><br><span class="line">                        ACL_MEM_MALLOC_HUGE_FIRST));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">memcpy</span>(tilingHost, shape, tilingSize);</span><br><span class="line">  <span class="built_in">memcpy</span>(inputHost, data, dataSize);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(inputDevice, inputByteSize, inputHost, inputByteSize,</span><br><span class="line">                        ACL_MEMCPY_HOST_TO_DEVICE));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(tilingDevice, tilingSize, tilingHost, tilingSize,</span><br><span class="line">                        ACL_MEMCPY_HOST_TO_DEVICE));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">flip_do</span>(blockDim, <span class="literal">nullptr</span>, stream, inputDevice, outputDevice, tilingDevice);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtSynchronizeStream</span>(stream));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtMemcpy</span>(outputHost, outputByteSize, outputDevice,</span><br><span class="line">                        outputByteSize, ACL_MEMCPY_DEVICE_TO_HOST));</span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, outputHost);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(inputDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(outputDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFree</span>(tilingDevice));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(inputHost));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(outputHost));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtFreeHost</span>(tilingHost));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtDestroyStream</span>(stream));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtDestroyContext</span>(context));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclrtResetDevice</span>(deviceId));</span><br><span class="line">  <span class="built_in">CHECK_ACL</span>(<span class="built_in">aclFinalize</span>());</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="built_in">free</span>(data);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001706789009.png"
alt="流水线示例" />
<figcaption aria-hidden="true">流水线示例</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/zh-cn_image_0000001658510374.png"
alt="数据切分示例" />
<figcaption aria-hidden="true">数据切分示例</figcaption>
</figure>
<p><strong>CUDA</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;data_loader.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">flip</span><span class="params">(<span class="type">uint8_t</span>* input, <span class="type">uint8_t</span>* output, <span class="type">uint32_t</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="type">uint32_t</span> width, <span class="type">uint32_t</span> channel)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> rowIdx = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">  <span class="type">int</span> rowSize = width * channel;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> row = rowIdx; row &lt; height; row += stride) &#123;</span><br><span class="line">    <span class="type">int</span> startOffset = row * rowSize;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> idx = <span class="number">0</span>; idx &lt; width; idx++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; channel; c++) &#123;</span><br><span class="line">        output[startOffset + idx * channel + c] =</span><br><span class="line">            input[startOffset + (width - idx - <span class="number">1</span>) * channel + c];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int32_t</span> argc, <span class="type">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">&quot; path/to/datafile&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">uint32_t</span> height, width, channel;</span><br><span class="line">  <span class="type">char</span> fileName[<span class="number">256</span>], resultFile[<span class="number">256</span>];</span><br><span class="line">  <span class="built_in">memset</span>(fileName, <span class="number">0</span>, <span class="number">256</span>);</span><br><span class="line">  <span class="built_in">memset</span>(resultFile, <span class="number">0</span>, <span class="number">256</span>);</span><br><span class="line">  <span class="built_in">strcpy</span>(fileName, argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="built_in">strcat</span>(resultFile, fileName);</span><br><span class="line">  <span class="built_in">strcat</span>(resultFile, <span class="string">&quot;.ret&quot;</span>);</span><br><span class="line">  <span class="type">uint8_t</span>* data = <span class="built_in">readFile</span>(fileName, height, width, channel);</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> dataSize = width * height * channel * <span class="built_in">sizeof</span>(<span class="type">uint8_t</span>);</span><br><span class="line">  <span class="type">size_t</span> inputByteSize = dataSize;</span><br><span class="line">  <span class="type">size_t</span> outputByteSize = dataSize;</span><br><span class="line">  <span class="type">uint8_t</span> *input, *output;</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;input, inputByteSize);</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;output, outputByteSize);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">memcpy</span>(input, data, inputByteSize);</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">gridSize</span><span class="params">((height + blockSize.x - <span class="number">1</span>) / blockSize.x)</span></span>;</span><br><span class="line"></span><br><span class="line">  flip&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(input, output, height, width, channel);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">  <span class="built_in">writeFile</span>(resultFile, height, width, channel, output);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(input);</span><br><span class="line">  <span class="built_in">cudaFree</span>(output);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/20/Compile-OpenCV-with-CANN-backend/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/20/Compile-OpenCV-with-CANN-backend/" class="post-title-link" itemprop="url">Compile OpenCV with CANN backend</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-20 15:35:05" itemprop="dateCreated datePublished" datetime="2023-09-20T15:35:05+08:00">2023-09-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:36:26" itemprop="dateModified" datetime="2025-01-23T15:36:26+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="prepare-environment">Prepare environment</h2>
<p>Simply you can use docker container, <a
target="_blank" rel="noopener" href="https://github.com/hipudding/opencv_contrib/blob/npu_support/modules/cannops/Dockerfile">Dockerfile
is here</a>.</p>
<p>Or install manually:</p>
<p><strong>driver</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Download driver here. https://www.hiascend.com/hardware/firmware-drivers</span><br><span class="line">chmod +x A300-3010-npu-driver_6.0.0_linux-x86_64.run</span><br><span class="line">./A300-3010-npu-driver_6.0.0_linux-x86_64.run --install</span><br></pre></td></tr></table></figure>
<p><strong>toolkit</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Download ascend_toolkit here. https://www.hiascend.com/en/software/cann/community</span><br><span class="line">chmod +x Ascend-cann-toolkit_7.0.RC1.alpha002_linux-x86_64.run</span><br><span class="line">./Ascend-cann-toolkit_7.0.RC1.alpha002_linux-x86_64.run --install</span><br></pre></td></tr></table></figure>
<h2 id="clone-repository">Clone repository</h2>
<p>Clone opencv repositories, In order to facilitate code reading, make
sure opencv_contrib is inside opencv dir.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:opencv/opencv.git</span><br><span class="line">cd opencv</span><br><span class="line">git clone git@github.com:opencv/opencv_contrib.git</span><br></pre></td></tr></table></figure>
<h2 id="build">Build</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cmake </span><br><span class="line">-DCMAKE_INSTALL_PREFIX=/home/hua/code/opencv/build/install </span><br><span class="line">-DWITH_DEBUG=1 </span><br><span class="line">-DBUILD_WITH_DEBUG_INFO=1 </span><br><span class="line">-DOPENCV_EXTRA_MODULES_PATH=/home/hua/code/opencv/opencv_contrib/modules </span><br><span class="line">-DWITH_CUDA=0 </span><br><span class="line">-DWITH_CANN=1 </span><br><span class="line">-DPYTHON3_EXECUTABLE=/home/hua/anaconda3/envs/py39/bin/python </span><br><span class="line">-DPYTHON_LIBRARY=/home/hua/anaconda3/envs/py39 </span><br><span class="line">-DPYTHON_INCLUDE_DIR=/home/hua/anaconda3/envs/py39/include/python3.9 </span><br><span class="line">-DBUILD_opencv_wechat_qrcode=OFF </span><br><span class="line">-DBUILD_opencv_xfeatures2d=OFF </span><br><span class="line">-DBUILD_opencv_face=OFF </span><br><span class="line">-DBUILD_opencv_dnn=OFF </span><br><span class="line">-DBUILD_opencv_features2d=OFF </span><br><span class="line">-DWITH_CAROTENE=OFF </span><br><span class="line">-DWITH_IPP=OFF </span><br><span class="line">-DBUILD_DOCS=ON</span><br><span class="line"></span><br><span class="line">make -j</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>Make sure CANN and python-dev is detected:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[cmake] --   CANN:                          YES</span><br><span class="line">[cmake] --     Include path                 /home/hua/Ascend/ascend-toolkit/latest/include /home/hua/Ascend/ascend-toolkit/latest/opp</span><br><span class="line">[cmake] --     Link libraries:              /home/hua/Ascend/ascend-toolkit/latest/acllib/lib64/libascendcl.so /home/hua/Ascend/ascend-toolkit/latest/lib64/libacl_op_compiler.so /home/hua/Ascend/ascend-toolkit/latest/opp/built-in/op_proto/lib/linux/x86_64/libopsproto.so /home/hua/Ascend/ascend-toolkit/latest/compiler/lib64/libgraph.so /home/hua/Ascend/ascend-toolkit/latest/compiler/lib64/libge_compiler.so /home/hua/Ascend/ascend-toolkit/latest/compiler/lib64/libgraph_base.so</span><br><span class="line">[cmake] -- </span><br><span class="line">[cmake] --   Python 3:</span><br><span class="line">[cmake] --     Interpreter:                 /home/hua/anaconda3/envs/py39/bin/python (ver 3.9.17)</span><br><span class="line">[cmake] --     Libraries:                   /home/hua/anaconda3/envs/py39/lib/libpython3.9.so (ver 3.9.17)</span><br><span class="line">[cmake] --     numpy:                       /home/hua/anaconda3/envs/py39/lib/python3.9/site-packages/numpy/core/include (ver 1.25.2)</span><br><span class="line">[cmake] --     install path:                lib/python3.9/site-packages/cv2/python-3.9</span><br></pre></td></tr></table></figure>
<h2 id="run-test">Run Test</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/opencv_test_cannops</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/29/Pytorch-DDP%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/29/Pytorch-DDP%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Pytorch DDP原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-06-29 17:10:02 / 修改时间：17:13:19" itemprop="dateCreated datePublished" datetime="2023-06-29T17:10:02+08:00">2023-06-29</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>DDP=分布式数据并行(DISTRIBUTED DATA
PARALLEL)，和DP一样也是一种数据并行的方法，详细文档可以参考<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/ddp.html">官方手册</a></p>
<h2 id="ddp使用">DDP使用</h2>
<p>以下是使用NPU进行2个节点并行的示例代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch_npu</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    torch.npu.set_device(rank)</span><br><span class="line">    device = torch.device(<span class="string">&#x27;npu&#x27;</span>)</span><br><span class="line">    <span class="comment"># create default process group</span></span><br><span class="line">    dist.init_process_group(<span class="string">&quot;hccl&quot;</span>, rank=rank, world_size=world_size)</span><br><span class="line">    <span class="comment"># create local model</span></span><br><span class="line">    model = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(device)</span><br><span class="line">    <span class="comment"># construct DDP model</span></span><br><span class="line">    ddp_model = DDP(model, device_ids=[rank])</span><br><span class="line">    <span class="comment"># define loss function and optimizer</span></span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    optimizer = optim.SGD(ddp_model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    <span class="comment"># forward pass</span></span><br><span class="line">    outputs = ddp_model(torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(device))</span><br><span class="line">    labels = torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward pass</span></span><br><span class="line">    loss = loss_fn(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update parameters</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    world_size = <span class="number">2</span></span><br><span class="line">    mp.spawn(example,</span><br><span class="line">        args=(world_size,),</span><br><span class="line">        nprocs=world_size,</span><br><span class="line">        join=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Environment variables which need to be</span></span><br><span class="line">    <span class="comment"># set when using c10d&#x27;s default &quot;env&quot;</span></span><br><span class="line">    <span class="comment"># initialization mode.</span></span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_ADDR&quot;</span>] = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_PORT&quot;</span>] = <span class="string">&quot;29500&quot;</span></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="ddp原理">DDP原理</h2>
<h3 id="相关文件">相关文件</h3>
<table>
<colgroup>
<col style="width: 57%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr>
<th>文件</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch/nn/parallel/distributed.py</td>
<td>DistributedDataParallel类实现</td>
</tr>
<tr>
<td>torch/csrc/distributed/c10d/reducer.cpp</td>
<td>DDP reduce类实现文件</td>
</tr>
<tr>
<td>torch/csrc/distributed/c10d/reducer.hpp</td>
<td>DDP reduce接口文件</td>
</tr>
<tr>
<td>torch/csrc/distributed/c10d/init.cpp</td>
<td>python C插件注册文件</td>
</tr>
</tbody>
</table>
<h3 id="进程交互">进程交互</h3>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230629163126602.png"
alt="image-20230629163126602" />
<figcaption aria-hidden="true">image-20230629163126602</figcaption>
</figure>
<ul>
<li>主线程通过torch.multiprocessing，fork出新的进程，满足word_size的要求，然后各个线程独立运行，主进程将自己的host和port放到环境变量中共子进程使用。</li>
<li>各个进程<strong><font color='orange'>初始化process
group</font></strong>，这些进程属于同一个进程组。</li>
<li>每个进程分别创建模型，DDP对象，定义loss函数和优化器。</li>
<li>创建DDP对象时，主进程会<strong><font color="red">同步模型参数</font></strong>，这一步完成后，各个进程的模型一致。</li>
<li>每个模型输入部分数据，这些数据需要调用者自行分割，可以使用DistributedSampler。</li>
<li>然后执行模型的训练过程，每次backward执行完后，调用注册的reducer
hook函数，做<strong><font color='green'>梯度聚合</font></strong>。</li>
</ul>
<h3 id="reducer">Reducer</h3>
<p>普通模型的DDP，除了每次反向执行后的梯度同步之外，进程之间没有其他通信的需求。DDP构造时，会在每个模型参数上配置回调函数(autograd_hook)，当参数的梯度计算完成后，会调用这个回调函数。为了能够一边计算，一边同步梯度数据，DDP将相邻参数分组，每个组叫一个bucket。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/72401724-d296d880-371a-11ea-90ab-737f86543df9.png"
alt="ddp_grad_sync.png" />
<figcaption aria-hidden="true">ddp_grad_sync.png</figcaption>
</figure>
<p>这是参数分组和allreduce的官方示意图，回调函数的执行流程如下：</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230629170221846.png"
alt="image-20230629170221846" />
<figcaption aria-hidden="true">image-20230629170221846</figcaption>
</figure>
<ul>
<li>每个参数梯度计算完成后，均会调用回调函数，该回调函数会将参数标记为ready。</li>
<li>然后检查bucket中的pending是否为0，也就是bucket的参数是否全部完成梯度计算，如果完成，则准备进行一步all
reduce。</li>
<li>all
reduce是一个异步方法，任务执行后不阻塞，继续执行。（如果某个进程的当前bucket没有执行完成，dist
backend会等待，执行完成后异步返回）。</li>
<li>当所有的bucket都发起all reduce后，等待每个bucket的all
reduce结果，所有结果都返回后，完成本次backward。</li>
</ul>
<h2 id="参考">参考</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/ddp.html">DISTRIBUTED
DATA PARALLEL</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187610959">原创 深度 PyTorch
DDP系列第二篇：实现原理与源代码解析</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/21/Pytorch-DP%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/21/Pytorch-DP%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Pytorch DP原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-21 15:50:59" itemprop="dateCreated datePublished" datetime="2023-06-21T15:50:59+08:00">2023-06-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-26 10:15:22" itemprop="dateModified" datetime="2023-06-26T10:15:22+08:00">2023-06-26</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>DP(DataParallel)=数据并行，DP框架会将数据切片，然后模型拷贝，并在不同的backend上并行执行。<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html?highlight=data+parallel#torch.nn.DataParallel">官方手册说明</a></p>
<h2 id="dp使用">DP使用</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = LinearModel()</span><br><span class="line">model = torch.nn.DataParallel(model,device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>从代码中看到，DP仅支持cuda，xpu或者昇腾，并不支持cpu。</p>
<h2 id="dp原理">DP原理</h2>
<h3 id="相关文件">相关文件</h3>
<table>
<colgroup>
<col style="width: 45%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr>
<th>文件</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch/nn/parallel/data_parallel.py</td>
<td>DP类实现</td>
</tr>
<tr>
<td>torch/nn/parallel/_functions.py</td>
<td>DP中使用的自定义autograd Function</td>
</tr>
<tr>
<td>torch/nn/parallel/comm.py</td>
<td>线程间数据传递接口，例如，gather,scatter等</td>
</tr>
<tr>
<td>torch/nn/parallel/replicate.py</td>
<td>模型拷贝相关</td>
</tr>
<tr>
<td>torch/nn/parallel/parallel_apply.py</td>
<td>多线程执行模型</td>
</tr>
<tr>
<td>torch/nn/parallel/scatter_gather.py</td>
<td>scatter和gather的封装</td>
</tr>
<tr>
<td>torch/csrc/cuda/python_comm.cpp</td>
<td>C语言实现的数据传递函数extension注册</td>
</tr>
<tr>
<td>torch/csrc/cuda/comm.cpp</td>
<td>C语言的数据传递实现</td>
</tr>
</tbody>
</table>
<h3 id="基本原理">基本原理</h3>
<p>DP基于单机多卡，所有的卡都参与训练。相对于非数据并行训练来说，DP会将一个batch数据切分为更小的batch，将数据复制到每一张计算的卡上，然后复制模型到所有的卡上，进行多线程执行，Forward计算完成后，会在device[0]上收集到一组预测值。device[0]计算loss，然后执行Backward计算梯度。（<strong>计算Backward没有看到创建线程的过程，单线程执行？？</strong>）</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/v2-5c5b0d8e3d7d6653a9ebd47bac93090c_720w.webp"
alt="DP原理图" />
<figcaption aria-hidden="true">DP原理图</figcaption>
</figure>
<h3 id="dp-wapper">DP wapper</h3>
<p>回顾DP使用方法，参与数据并行的模型进需要使用DataParalle包一层即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model,device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>可以理解，下图就是上述代码的实现，DP就是在原本的模型执行之前增加了数据切分(Scatter)，模型复制(Broadcast)，然后将模型的并行执行结果进行合并(Gather)。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230621152641246.png"
alt="image-20230621152641246" />
<figcaption aria-hidden="true">image-20230621152641246</figcaption>
</figure>
<h3 id="数据传递">数据传递</h3>
<p>DP的Forward函数核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, *inputs: <span class="type">Any</span>, **kwargs: <span class="type">Any</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">	    ...</span><br><span class="line">      <span class="comment"># 训练数据切分，并拷贝到每一个卡上</span></span><br><span class="line">      inputs, module_kwargs = self.scatter(inputs, kwargs, self.device_ids)</span><br><span class="line">      </span><br><span class="line">      ...</span><br><span class="line">      <span class="comment"># 模型数据拷贝到每一个卡上</span></span><br><span class="line">      replicas = self.replicate(self.module, self.device_ids[:<span class="built_in">len</span>(inputs)])</span><br><span class="line">      <span class="comment"># 每张卡启动一个线程执行，每个线程执行模型的forward函数</span></span><br><span class="line">      outputs = self.parallel_apply(replicas, inputs, module_kwargs)</span><br><span class="line">      <span class="comment"># 集合所有线程的预测结果</span></span><br><span class="line">      <span class="keyword">return</span> self.gather(outputs, self.output_device)</span><br></pre></td></tr></table></figure>
<p>由于是单机单进程，所以数据的拷贝过程相对简单，以上述代码的scatter和broadcast为例，数据切分，然后将切分后的数据移动到指定设备上。但是Tensor还是在同一个数组中。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230626101410741.png"
alt="image-20230626101410741" />
<figcaption aria-hidden="true">image-20230626101410741</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230621152441395.png"
alt="image-20230621152441395" />
<figcaption aria-hidden="true">image-20230621152441395</figcaption>
</figure>
<h3 id="自定义自动梯度方法">自定义自动梯度方法</h3>
<p>Pytorch实现了自动梯度计算，除了官方实现的梯度计算之外，还能够自定义梯度计算方法，具体描述参考<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">官方说明</a>。以下是代码中对Function类的使用举例说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Examples::</span><br><span class="line"></span><br><span class="line">    &gt;&gt;&gt; <span class="comment"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span></span><br><span class="line">    &gt;&gt;&gt; <span class="keyword">class</span> <span class="title class_">Exp</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line">    &gt;&gt;&gt;     @<span class="built_in">staticmethod</span></span><br><span class="line">    &gt;&gt;&gt;     <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, i</span>):</span><br><span class="line">    &gt;&gt;&gt;         result = i.exp()</span><br><span class="line">    &gt;&gt;&gt;         ctx.save_for_backward(result)</span><br><span class="line">    &gt;&gt;&gt;         <span class="keyword">return</span> result</span><br><span class="line">    &gt;&gt;&gt;</span><br><span class="line">    &gt;&gt;&gt;     @<span class="built_in">staticmethod</span></span><br><span class="line">    &gt;&gt;&gt;     <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">    &gt;&gt;&gt;         result, = ctx.saved_tensors</span><br><span class="line">    &gt;&gt;&gt;         <span class="keyword">return</span> grad_output * result</span><br><span class="line">    &gt;&gt;&gt;</span><br><span class="line">    &gt;&gt;&gt; <span class="comment"># Use it by calling the apply method:</span></span><br><span class="line">    &gt;&gt;&gt; <span class="comment"># xdoctest: +SKIP</span></span><br><span class="line">    &gt;&gt;&gt; output = Exp.apply(<span class="built_in">input</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>实现自定义自动梯度方法，需要继承Function类，并实现forward和backward方法，并且这两个方法均是静态的。使用自定义自动梯度方法的时候，不应该调用forward，而是调用Exp.apply，这样会返回forward的结果。相当于自己定义了算子的前向和后向的处理方法。当在执行反向传播的时候，会按反顺序调用自定义自动梯度方法的backward方法。</p>
<p>DP实现了三个自定义自动梯度方法，分别是Broadcast，Gather和Scatter，其反向传播方法内分别调用了Reduce_add，Scatter和Gather，即forward和backward中的动作是成对的。</p>
<h3 id="训练流程">训练流程</h3>
<p><strong>正向</strong></p>
<ul>
<li>①：将输入的一个batch切分成三个更小的batch</li>
<li>②：将模型的参数复制三份，分别发送到三个device上</li>
<li>③：启动三个线程，每个线程在不同的device上进行原始模型的前向计算</li>
<li>④：将三个线程的输出结果拼接到一起</li>
</ul>
<p><strong>Loss</strong></p>
<ul>
<li>在device[0]上计算Loss</li>
</ul>
<p><strong>反向</strong></p>
<ul>
<li>④：将Loss切分，分别发送给对应的模型</li>
<li>③：执行三个模型的backward方法，更新梯度（<strong>Backward
Engine多线程处理</strong>）</li>
<li>②：将三个模型上的梯度相加，并储存到Original
Module中（<strong>直接相加？</strong>）</li>
<li>①：将三个模型的梯度信息拼接到一起</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/image-20230621144922423.png"
alt="image-20230621144922423" />
<figcaption aria-hidden="true">image-20230621144922423</figcaption>
</figure>
<h3 id="官方建议">官方建议</h3>
<ol type="1">
<li>不建议使用DP来进行数据并行，因为这不是一个真正的并发执行（python
GIL），并且，由于数据的转移，反而会减慢模型的执行，建议使用DDP。</li>
<li>所有拷贝来的数据都是一次性的，每次迭代后都会销毁重建，所以这些模型上如果有数据修改，将不会持久化。（<strong>Input最后做的gather是为了下一层网络的输入</strong>）</li>
</ol>
<h2 id="参考">参考</h2>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html?highlight=data+parallel#torch.nn.DataParallel">Pytorch
DATAPARALLEL</a></p>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">Extending
torch.autograd</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343951042">PyTorch 源码解读之
DP &amp; DDP：模型并行和分布式训练解析</a></p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187610959">[原创][深度][PyTorch]
DDP系列第二篇：实现原理与源代码解析</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/13/mpi%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/13/mpi%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">mpi入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-13 16:32:00" itemprop="dateCreated datePublished" datetime="2023-06-13T16:32:00+08:00">2023-06-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-21 15:59:43" itemprop="dateModified" datetime="2023-06-21T15:59:43+08:00">2023-06-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>MPI全称叫做<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%A8%8A%E6%81%AF%E5%82%B3%E9%81%9E%E4%BB%8B%E9%9D%A2">消息传递接口</a>（Message
passing interface）</p>
<h2 id="安装mpi4py">安装mpi4py</h2>
<p>Python可以使用<a
target="_blank" rel="noopener" href="https://github.com/mpi4py/mpi4py">mpi4py库</a>，使用pip安装的话需要提前安装好openmpi库：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libopenmpi-dev</span><br></pre></td></tr></table></figure>
<p>由于Anaconda存在的缘故，pip安装编译会报找不到一些so库。解决方法见<a
target="_blank" rel="noopener" href="https://github.com/mpi4py/mpi4py/issues/343">issue</a></p>
<p>不过既然Anaconda存在，可以直接使用conda安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge mpi4py openmpi</span><br></pre></td></tr></table></figure>
<h2 id="名词介绍">名词介绍</h2>
<ul>
<li>world: 一个进程组，聚集通信在这个进程组中进行</li>
<li>rank: 当前进程（自己）的编号，0是master进程</li>
<li>world_size: 进程组中进程的个数</li>
</ul>
<h2 id="点对点通信">点对点通信</h2>
<p><strong>举例</strong></p>
<ul>
<li>send(isend 非阻塞)</li>
<li>recv(irecv 非阻塞)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    comm.send(matrix, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; send matrix &#123;&#125; to other processes&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    matrix_recv = comm.recv()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; recv matrix &#123;&#125; from master&quot;</span>.<span class="built_in">format</span>(rank, matrix_recv))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mpiexec -np 2 python mpi_test.py </span><br><span class="line"></span><br><span class="line">process 1 recv matrix </span><br><span class="line">[[ 0  1 -7  3 -7 -1 -2  2 -3  7]</span><br><span class="line"> [ 2 -6  7  4 -5 -1  0 -4  2  4]</span><br><span class="line"> [ 0  1  3  5  8 -3 -8 -4 -3 -7]</span><br><span class="line"> [ 1 -9 -8  5 -9 -7 -7 -6 -6 -2]</span><br><span class="line"> [-2  3 -3 -4  3  7  0  2  7  8]</span><br><span class="line"> [-3 -6 -4 -5 -5 -1  2 -3 -9  3]</span><br><span class="line"> [-6  6  5  4 -5  8 -2 -2  3  6]</span><br><span class="line"> [ 1  7 -6 -7  0 -2  8  4  7 -6]</span><br><span class="line"> [-2 -7 -5 -4 -9  0 -3  8 -2  7]</span><br><span class="line"> [-3  2  2  6  0 -8 -5  0 -4  5]] from master</span><br><span class="line"> </span><br><span class="line">process 0 send matrix </span><br><span class="line">[[ 0  1 -7  3 -7 -1 -2  2 -3  7]</span><br><span class="line"> [ 2 -6  7  4 -5 -1  0 -4  2  4]</span><br><span class="line"> [ 0  1  3  5  8 -3 -8 -4 -3 -7]</span><br><span class="line"> [ 1 -9 -8  5 -9 -7 -7 -6 -6 -2]</span><br><span class="line"> [-2  3 -3 -4  3  7  0  2  7  8]</span><br><span class="line"> [-3 -6 -4 -5 -5 -1  2 -3 -9  3]</span><br><span class="line"> [-6  6  5  4 -5  8 -2 -2  3  6]</span><br><span class="line"> [ 1  7 -6 -7  0 -2  8  4  7 -6]</span><br><span class="line"> [-2 -7 -5 -4 -9  0 -3  8 -2  7]</span><br><span class="line"> [-3  2  2  6  0 -8 -5  0 -4  5]] to other processes</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="聚集通信">聚集通信</h2>
<p><strong>举例</strong></p>
<ul>
<li><p>bcast</p></li>
<li><p>scatter</p></li>
<li><p>gather(all_gather所有的进程均收到gather结果)</p></li>
<li><p>reduce(all_reduce所有的进程均收到reduce结果)</p></li>
</ul>
<p><strong>bcast</strong>
广播消息到所有进程组内进程，调用者如果是master，则是发送；其他进程是接收</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    matrix = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="built_in">int</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; use empty matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line">matrix = comm.bcast(matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; bcast matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">process 1 use empty matrix [[0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]]</span><br><span class="line"> </span><br><span class="line">process 0 prepare random matrix [[ 5 -8  8  4  1 -7 -8 -6  1  3]</span><br><span class="line"> [-6 -5  6 -1 -8  5  7  2  3  7]</span><br><span class="line"> [-2 -9 -9  7  0  8 -4  2 -8 -1]</span><br><span class="line"> [ 3 -1  3  3 -7  7 -5 -5 -9 -7]</span><br><span class="line"> [ 0 -4 -7 -8 -8 -8  7  2  2  1]</span><br><span class="line"> [-2  0  1  7  6  3  8 -1  1  8]</span><br><span class="line"> [ 6 -4  7 -7  6 -1 -4  6 -6 -9]</span><br><span class="line"> [-3  2  8 -7  8 -8  8  3 -1 -3]</span><br><span class="line"> [ 6  7  2 -2  1 -5  1  2  1 -2]</span><br><span class="line"> [ 2 -3 -2 -3 -7  6  7 -8 -9  4]]</span><br><span class="line"> </span><br><span class="line">process 0 bcast matrix [[ 5 -8  8  4  1 -7 -8 -6  1  3]</span><br><span class="line"> [-6 -5  6 -1 -8  5  7  2  3  7]</span><br><span class="line"> [-2 -9 -9  7  0  8 -4  2 -8 -1]</span><br><span class="line"> [ 3 -1  3  3 -7  7 -5 -5 -9 -7]</span><br><span class="line"> [ 0 -4 -7 -8 -8 -8  7  2  2  1]</span><br><span class="line"> [-2  0  1  7  6  3  8 -1  1  8]</span><br><span class="line"> [ 6 -4  7 -7  6 -1 -4  6 -6 -9]</span><br><span class="line"> [-3  2  8 -7  8 -8  8  3 -1 -3]</span><br><span class="line"> [ 6  7  2 -2  1 -5  1  2  1 -2]</span><br><span class="line"> [ 2 -3 -2 -3 -7  6  7 -8 -9  4]]</span><br><span class="line"> </span><br><span class="line">process 1 bcast matrix [[ 5 -8  8  4  1 -7 -8 -6  1  3]</span><br><span class="line"> [-6 -5  6 -1 -8  5  7  2  3  7]</span><br><span class="line"> [-2 -9 -9  7  0  8 -4  2 -8 -1]</span><br><span class="line"> [ 3 -1  3  3 -7  7 -5 -5 -9 -7]</span><br><span class="line"> [ 0 -4 -7 -8 -8 -8  7  2  2  1]</span><br><span class="line"> [-2  0  1  7  6  3  8 -1  1  8]</span><br><span class="line"> [ 6 -4  7 -7  6 -1 -4  6 -6 -9]</span><br><span class="line"> [-3  2  8 -7  8 -8  8  3 -1 -3]</span><br><span class="line"> [ 6  7  2 -2  1 -5  1  2  1 -2]</span><br><span class="line"> [ 2 -3 -2 -3 -7  6  7 -8 -9  4]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>scatter</strong>会将发送的数据拆分，然后分给进程组中的进程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    line = np.array_split(matrix, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    line = <span class="literal">None</span></span><br><span class="line">one_piece = comm.scatter(line)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; bcast matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, one_piece))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mpiexec --oversubscribe -np 10 python mpi_test.py</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据的分组数量和进程数要对应。 默认mpi最大进程数量和cpu相同，可以添加--oversubscribe解除限制</span></span><br><span class="line"></span><br><span class="line">process 0 prepare random matrix [[-9 -2 -6  4  1  7  5  5 -1 -3]</span><br><span class="line"> [-4  6  3  5 -9 -6  7  3  0 -1]</span><br><span class="line"> [-9 -6  2  5 -1  2 -2 -5 -8 -7]</span><br><span class="line"> [ 3 -4 -1 -9 -2  5  4 -8  0  1]</span><br><span class="line"> [ 3  1 -2  6 -4  0 -2 -3  6  8]</span><br><span class="line"> [ 6  5 -8 -7 -3 -7 -5 -8  5  8]</span><br><span class="line"> [-3 -9  2 -2  0 -7  6 -5  5 -2]</span><br><span class="line"> [-6 -4  8  5 -6  6  2 -3 -2  4]</span><br><span class="line"> [-5  7  3 -2  4 -8  3 -5  7 -5]</span><br><span class="line"> [ 0  4 -9 -1 -5 -2 -1 -5  7  7]]</span><br><span class="line">process 0 bcast matrix [[-9 -2 -6  4  1  7  5  5 -1 -3]]</span><br><span class="line">process 2 bcast matrix [[-9 -6  2  5 -1  2 -2 -5 -8 -7]]</span><br><span class="line">process 4 bcast matrix [[ 3  1 -2  6 -4  0 -2 -3  6  8]]</span><br><span class="line">process 6 bcast matrix [[-3 -9  2 -2  0 -7  6 -5  5 -2]]</span><br><span class="line">process 1 bcast matrix [[-4  6  3  5 -9 -6  7  3  0 -1]]</span><br><span class="line">process 3 bcast matrix [[ 3 -4 -1 -9 -2  5  4 -8  0  1]]</span><br><span class="line">process 5 bcast matrix [[ 6  5 -8 -7 -3 -7 -5 -8  5  8]]</span><br><span class="line">process 8 bcast matrix [[-5  7  3 -2  4 -8  3 -5  7 -5]]</span><br><span class="line">process 9 bcast matrix [[ 0  4 -9 -1 -5 -2 -1 -5  7  7]]</span><br><span class="line">process 7 bcast matrix [[-6 -4  8  5 -6  6  2 -3 -2  4]]</span><br></pre></td></tr></table></figure>
<p><strong>gather</strong>收集所有分发后的数据，比如，我们需要对scatter分发的所有数据+1，然后再收集回来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    line = np.array_split(matrix, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    line = <span class="literal">None</span></span><br><span class="line">line = comm.scatter(line)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> line:</span><br><span class="line">    item += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">recv_result = comm.gather(line)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.vstack(recv_result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; get calculated matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">process 0 prepare random matrix </span><br><span class="line">[[-2  4  0  4 -2 -1  7  3 -1  4]</span><br><span class="line"> [-4 -8 -1 -9  8  7  8 -3 -6  1]</span><br><span class="line"> [ 1 -8  0  6 -6 -8 -7  2 -9 -1]</span><br><span class="line"> [ 7 -7  0 -4 -4  0 -2  4 -7  7]</span><br><span class="line"> [-7  3  0 -8  5 -8 -2  0  7  2]</span><br><span class="line"> [-6 -9 -3 -8  0  0  0 -7 -4 -5]</span><br><span class="line"> [ 2 -6  3 -8  8 -8 -2  0 -2 -9]</span><br><span class="line"> [ 8  8 -8 -8  7  8  4 -3 -9  3]</span><br><span class="line"> [ 8 -4  4  2  8 -2  2  7  0 -6]</span><br><span class="line"> [ 3  1 -3  2 -8 -6 -4 -5  3  0]]</span><br><span class="line"> </span><br><span class="line">process 0 get calculated matrix </span><br><span class="line">[[-1  5  1  5 -1  0  8  4  0  5]</span><br><span class="line"> [-3 -7  0 -8  9  8  9 -2 -5  2]</span><br><span class="line"> [ 2 -7  1  7 -5 -7 -6  3 -8  0]</span><br><span class="line"> [ 8 -6  1 -3 -3  1 -1  5 -6  8]</span><br><span class="line"> [-6  4  1 -7  6 -7 -1  1  8  3]</span><br><span class="line"> [-5 -8 -2 -7  1  1  1 -6 -3 -4]</span><br><span class="line"> [ 3 -5  4 -7  9 -7 -1  1 -1 -8]</span><br><span class="line"> [ 9  9 -7 -7  8  9  5 -2 -8  4]</span><br><span class="line"> [ 9 -3  5  3  9 -1  3  8  1 -5]</span><br><span class="line"> [ 4  2 -2  3 -7 -5 -3 -4  4  1]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>reduce</strong> 将收集到的数据进行规约，例如SUM，MAX等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    matrix = np.random.randint(-<span class="number">9</span>, <span class="number">9</span>, size=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    line = np.array_split(matrix, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; prepare random matrix &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, matrix))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    line = <span class="literal">None</span></span><br><span class="line">line = comm.scatter(line)</span><br><span class="line">sum_result = comm.reduce(line, op=MPI.SUM)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process &#123;&#125; get calculated sum result &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rank, sum_result))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">process 0 prepare random matrix </span><br><span class="line">[[ 2 -9 -9 -8 -9 -8 -7  8 -6  5]</span><br><span class="line"> [ 0  6  6 -2 -6  4 -5 -7  3 -4]</span><br><span class="line"> [-9  2  6 -5 -1 -6  5 -2  3  4]</span><br><span class="line"> [ 7  4 -6 -5  1  4 -5  5 -1  1]</span><br><span class="line"> [ 1  2 -8 -7  5  2 -6 -4  5 -8]</span><br><span class="line"> [-8  0  7  1  0 -8  3 -8 -2  3]</span><br><span class="line"> [ 6  5  2 -7 -6 -3 -7  4  2 -1]</span><br><span class="line"> [ 2 -8  8 -1 -3 -9  0  4 -9 -9]</span><br><span class="line"> [-4 -7 -1 -8  4  7  1  6  4  5]</span><br><span class="line"> [-2  6  3  3  7  7 -4  1 -7 -3]]</span><br><span class="line"> </span><br><span class="line">process 0 get calculated sum result [[ -5   1   8 -39  -8 -10 -25   7  -8  -7]]</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/12/DeepSpeed-%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/12/DeepSpeed-%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">DeepSpeed 安装使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-12 16:57:33" itemprop="dateCreated datePublished" datetime="2023-06-12T16:57:33+08:00">2023-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-19 09:24:00" itemprop="dateModified" datetime="2023-06-19T09:24:00+08:00">2023-06-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="deepspeedubuntucpu">DeepSpeed+Ubuntu+CPU</h2>
<p>目前CPU支持很有限，仅支持部分推理。环境配置可参考<a
target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed/commit/1f72082fc0ea159d0de46886d8e713dda7df9ce2#diff-69a0fa9b9a28351868fc14c345088d9af3cf3560538482a386dc616ebc023fe3">CI配置</a></p>
<p><strong>安装intel_extension_for_pytorch</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install intel_extension_for_pytorch</span><br><span class="line">python -m pip install oneccl_bind_pt==2.0 -f https://developer.intel.com/ipex-whl-stable-cpu</span><br></pre></td></tr></table></figure>
<p><strong>安装oneCCL</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/oneapi-src/oneCCL</span><br><span class="line">cd oneCCL</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">source ./_install/env/setvars.sh</span><br></pre></td></tr></table></figure>
<p><strong>安装Transformers(用于跑用例)</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huggingface/transformers</span><br><span class="line">cd transformers</span><br><span class="line">git rev-parse --short HEAD</span><br><span class="line">pip install .</span><br></pre></td></tr></table></figure>
<p><strong>安装DeepSpeed</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install DeepSpeed</span><br></pre></td></tr></table></figure>
<p><strong>ds_report</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed C++/CUDA extension op report</span><br><span class="line">--------------------------------------------------</span><br><span class="line">NOTE: Ops not installed will be just-in-time (JIT) compiled at</span><br><span class="line">      runtime if needed. Op compatibility means that your system</span><br><span class="line">      meet the required dependencies to JIT install the op.</span><br><span class="line">--------------------------------------------------</span><br><span class="line">JIT compiled ops requires ninja</span><br><span class="line">ninja .................. [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">op name ................ installed .. compatible</span><br><span class="line">--------------------------------------------------</span><br><span class="line">deepspeed_not_implemented  [NO] ....... [OKAY]</span><br><span class="line">deepspeed_ccl_comm ....... [NO] ....... [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed general environment info:</span><br><span class="line">torch install path ............... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/torch&#x27;]</span><br><span class="line">torch version .................... 2.0.1+cu117</span><br><span class="line">deepspeed install path ........... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/deepspeed&#x27;]</span><br><span class="line">deepspeed info ................... 0.9.4+e5fe5f65, e5fe5f65, master</span><br><span class="line">deepspeed wheel compiled w. ...... torch 0.0</span><br></pre></td></tr></table></figure>
<h2 id="deepspeedubuntugpu">DeepSpeed+Ubuntu+GPU</h2>
<p><strong>操作系统</strong></p>
<p>华为云上gpu的镜像是16.04的，很多依赖的软件版本过低，建议升级到20.04，DeepSpeedExample有些需要高版本的glibc，可以直接升级到22.04。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt-get dist-upgrade</span><br><span class="line">sudo do-release-upgrade</span><br></pre></td></tr></table></figure>
<p><strong>GPU驱动和cuda</strong></p>
<ul>
<li><p><a
target="_blank" rel="noopener" href="https://www.nvidia.cn/Download/index.aspx?lang=cn">驱动下载页面</a>选择相应的型号，选择cuda
11.7</p></li>
<li><p><a
target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=runfile_local">CUDA
toolkit下载页面</a>选择系统版本，然后下载runfile（使用apt总是会升级驱动和cuda到最新版本，所以直接下载二进制安装）。</p></li>
</ul>
<p><strong>pip源</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<p><strong>安装<a
target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">pytorch</a></strong></p>
<p>DeepSpeed
0.9.2（pip版本）目前依赖的是pytorch1.13.1，安装对应的版本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117</span><br></pre></td></tr></table></figure>
<p><strong>triton==1.0.0</strong></p>
<p>DeepSpeed依赖triton版本是1.0.0，pip仓库无法直接安装，git上下载源码编译安装。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编译依赖</span></span><br><span class="line">sudo apt-get install llvm-11 llvm-11-*</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">源码安装</span></span><br><span class="line">wget https://github.com/openai/triton/archive/refs/tags/v1.0.zip</span><br><span class="line">unzip v1.0.zip</span><br><span class="line">cd triton/python</span><br><span class="line">pip install cmake</span><br><span class="line">pip install .</span><br></pre></td></tr></table></figure>
<p><strong>安装DeepSpeed</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">预编译算子(编译不通过：https://github.com/microsoft/DeepSpeed/issues/425)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">workground: Setting NVCC_PREPEND_FLAGS=<span class="string">&quot;--forward-unknown-opts&quot;</span></span></span><br><span class="line">DS_BUILD_OPS=1 pip install deepspeed --global-option=&quot;build_ext&quot; --global-option=&quot;-j8&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JIT_load</span></span><br><span class="line">pip install deepspeed</span><br></pre></td></tr></table></figure>
<p><strong>ds_report</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed C++/CUDA extension op report</span><br><span class="line">--------------------------------------------------</span><br><span class="line">NOTE: Ops not installed will be just-in-time (JIT) compiled at</span><br><span class="line">      runtime if needed. Op compatibility means that your system</span><br><span class="line">      meet the required dependencies to JIT install the op.</span><br><span class="line">--------------------------------------------------</span><br><span class="line">JIT compiled ops requires ninja</span><br><span class="line">ninja .................. [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">op name ................ installed .. compatible</span><br><span class="line">--------------------------------------------------</span><br><span class="line">async_io ............... [NO] ....... [OKAY]</span><br><span class="line">cpu_adagrad ............ [NO] ....... [OKAY]</span><br><span class="line">cpu_adam ............... [NO] ....... [OKAY]</span><br><span class="line">fused_adam ............. [NO] ....... [OKAY]</span><br><span class="line">fused_lamb ............. [NO] ....... [OKAY]</span><br><span class="line">quantizer .............. [NO] ....... [OKAY]</span><br><span class="line">random_ltd ............. [NO] ....... [OKAY]</span><br><span class="line">sparse_attn ............ [NO] ....... [OKAY]</span><br><span class="line">spatial_inference ...... [NO] ....... [OKAY]</span><br><span class="line">transformer ............ [NO] ....... [OKAY]</span><br><span class="line">stochastic_transformer . [NO] ....... [OKAY]</span><br><span class="line">transformer_inference .. [NO] ....... [OKAY]</span><br><span class="line">utils .................. [NO] ....... [OKAY]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">DeepSpeed general environment info:</span><br><span class="line">torch install path ............... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/torch&#x27;]</span><br><span class="line">torch version .................... 1.13.1+cu117</span><br><span class="line">deepspeed install path ........... [&#x27;/home/hua/anaconda3/lib/python3.10/site-packages/deepspeed&#x27;]</span><br><span class="line">deepspeed info ................... 0.9.2, unknown, unknown</span><br><span class="line">torch cuda version ............... 11.7</span><br><span class="line">torch hip version ................ None</span><br><span class="line">nvcc version ..................... 11.7</span><br><span class="line">deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7</span><br></pre></td></tr></table></figure>
<p><strong>运行例子</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/microsoft/DeepSpeedExamples.git</span><br><span class="line">cd DeepSpeedExamples/training/cifar</span><br><span class="line">./run_ds.py</span><br></pre></td></tr></table></figure>
<p>python报错，修改方法：</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">diff --git a/training/cifar/cifar10_deepspeed.py b/training/cifar/cifar10_deepspeed.py</span></span><br><span class="line"><span class="comment">index 33ea569..d1117c3 100755</span></span><br><span class="line"><span class="comment">--- a/training/cifar/cifar10_deepspeed.py</span></span><br><span class="line"><span class="comment">+++ b/training/cifar/cifar10_deepspeed.py</span></span><br><span class="line"><span class="meta">@@ -159,7 +159,7 @@</span> def imshow(img):</span><br><span class="line"></span><br><span class="line"> # get some random training images</span><br><span class="line"> dataiter = iter(trainloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # show images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="meta">@@ -309,7 +309,7 @@</span> print(&#x27;Finished Training&#x27;)</span><br><span class="line"> # Okay, first step. Let us display an image from the test set to get familiar.</span><br><span class="line"></span><br><span class="line"> dataiter = iter(testloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # print images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment">diff --git a/training/cifar/cifar10_tutorial.py b/training/cifar/cifar10_tutorial.py</span></span><br><span class="line"><span class="comment">index 2154e36..114e8c5 100644</span></span><br><span class="line"><span class="comment">--- a/training/cifar/cifar10_tutorial.py</span></span><br><span class="line"><span class="comment">+++ b/training/cifar/cifar10_tutorial.py</span></span><br><span class="line"><span class="meta">@@ -110,7 +110,7 @@</span> def imshow(img):</span><br><span class="line"></span><br><span class="line"> # get some random training images</span><br><span class="line"> dataiter = iter(trainloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # show images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="meta">@@ -219,7 +219,7 @@</span> torch.save(net.state_dict(), PATH)</span><br><span class="line"> # Okay, first step. Let us display an image from the test set to get familiar.</span><br><span class="line"></span><br><span class="line"> dataiter = iter(testloader)</span><br><span class="line"><span class="deletion">-images, labels = dataiter.next()</span></span><br><span class="line"><span class="addition">+images, labels = next(dataiter)</span></span><br><span class="line"></span><br><span class="line"> # print images</span><br><span class="line"> imshow(torchvision.utils.make_grid(images))</span><br></pre></td></tr></table></figure>
<h2 id="简单模型改写为deepspeed">简单模型改写为DeepSpeed</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">with_ds = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> with_ds:</span><br><span class="line">    <span class="keyword">import</span> deepspeed</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x_data = torch.tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]]).to(device)</span><br><span class="line">y_data = torch.tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]]).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LinearModel()</span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> with_ds:</span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ds_config = &#123;</span><br><span class="line">        <span class="string">&quot;train_micro_batch_size_per_gpu&quot;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">&quot;optimizer&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;SGD&quot;</span>,</span><br><span class="line">            <span class="string">&quot;params&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;lr&quot;</span>: <span class="number">1e-2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    model, _, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=ds_config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> with_ds:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.backward(loss)</span><br><span class="line">        model.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.tensor([[<span class="number">4.0</span>]]).to(device)</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>DeepSpeed基本执行流程</strong></p>
<p><em>这个简单的模型实际上还是完全调用的pytorch的函数，ds包的wapper其实啥都没干，完全透传</em></p>
<ol type="1">
<li>首先获取Accelerator（加速器），判断能不能import
intel_extension_for_deepspeed，如果能就用XPU，如果不能就用CUDA（Accelerator将设备管理，内存管理，Tensor等等进行了抽象，不同的后端设备继承实现），高版本这部分逻辑有改变，可以通过环境变量控制。<strong>切入点1，这里需要判断是否能够使用npu</strong></li>
<li>选择并行计算后端，如果Torch.distributed已经初始化，则使用直接使用，检查是否在Aure或者aws机器上，针对这些机器做环境变量配置，否则尝试寻找mpi，然后根据Accelerator的comm
backend类型初始化TorchBackend.<strong>切入点2，这里需要针对华为云机器做专门的环境变量配置，以及支持昇腾并行后端</strong></li>
<li>解析ds的配置文件</li>
<li>创建ds引擎
<ol type="1">
<li>检查环境变量，配置dist相关配置，包括rank，world size 等等</li>
<li>用dist分发模型参数，所有进程同步模型参数</li>
<li>根据配置创建optimizer，例如，上例中，就会生成torch自带的SGD优化器，也可以指定ds提供的Adam，lamb等优化器</li>
<li>配置checkpoint</li>
<li>编译Utils，就是flateen_unflateen.cpp</li>
</ol></li>
<li>执行forward，计算损失
<strong>切入点3，以下基本调用的都是pytorch的optimizer，这些需要pytorch有昇腾支持，另外，DS提供的优化器也需要昇腾支持</strong></li>
<li>反向传播
<ol type="1">
<li>梯度累加</li>
<li>根据是否使用zero优化，自动精度，混合精度等调用optimizer其他wapper的backward，并传入合适的参数</li>
<li>多进程计算梯度并收集结果</li>
</ol></li>
<li>更新参数
<ol type="1">
<li>如果到了梯度累加的预制，根据不同的配置，最终调用optimizer.step更新参数，并清空梯度信息</li>
</ol></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">hipudding</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/hipudding" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
