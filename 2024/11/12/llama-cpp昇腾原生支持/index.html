<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="16x16" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="mask-icon" href="https://dogecoin.com/assets/images/doge.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>



<link rel="canonical" href="http://example.com/2024/11/12/llama-cpp%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/11/12/llama-cpp%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/","path":"2024/11/12/llama-cpp昇腾原生支持/","title":"llama.cpp昇腾原生支持"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>llama.cpp昇腾原生支持 | hipudding's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">hipudding's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF"><span class="nav-number">1.</span> <span class="nav-text">1. 项目背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 项目概述</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF"><span class="nav-number">2.</span> <span class="nav-text">2. 设计思路</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#llama.cpp%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 llama.cpp项目架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%98%87%E8%85%BE%E5%90%8E%E7%AB%AF%E6%8E%A5%E5%85%A5%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 昇腾后端接入方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">3. 实现原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E6%97%B6"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 运行时</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E5%AD%90"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 算子</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor%E8%BD%AC%E6%8D%A2"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 Tensor转换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#aclnn%E7%AE%97%E5%AD%90"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.1 aclnn算子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ascendc%E7%AE%97%E5%AD%90"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.2.2 AscendC算子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 内存管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8F%E5%8C%96%E6%A0%BC%E5%BC%8F"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 量化格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC%E5%92%8C%E6%B3%A8%E9%87%8A"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 代码风格和注释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E5%92%8C%E9%AA%8C%E8%AF%81"><span class="nav-number">4.</span> <span class="nav-text">4. 测试和验证</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#runtime%E6%B5%8B%E8%AF%95"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 Runtime测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E5%AD%90%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 算子单元测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 性能测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%B2%BE%E5%BA%A6%E9%AA%8C%E8%AF%81"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 模型精度验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%94%AF%E6%8C%81%E9%AA%8C%E8%AF%81"><span class="nav-number">4.5.</span> <span class="nav-text">4.5 模型支持验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BE%E5%8C%BAci"><span class="nav-number">4.6.</span> <span class="nav-text">4.6 社区CI</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ollama%E6%94%AF%E6%8C%81"><span class="nav-number">5.</span> <span class="nav-text">5. Ollama支持</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BE%E5%8C%BA%E8%B7%9F%E8%BF%9B"><span class="nav-number">6.</span> <span class="nav-text">6. 社区跟进</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%A1%A3%E5%92%8C%E8%AF%B4%E6%98%8E"><span class="nav-number">7.</span> <span class="nav-text">7. 文档和说明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BE%E5%8C%BAdoc"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 社区doc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%98%87%E8%85%BE%E5%BC%80%E6%BA%90%E6%89%8B%E5%86%8C"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 昇腾开源手册</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E5%BC%95%E7%94%A8"><span class="nav-number">8.</span> <span class="nav-text">8. 项目引用</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hipudding"
      src="https://dogecoin.com/assets/images/doge.svg">
  <p class="site-author-name" itemprop="name">hipudding</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hipudding" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hipudding" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:huafengchun@gmail.com" title="E-Mail → mailto:huafengchun@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/12/llama-cpp%E6%98%87%E8%85%BE%E5%8E%9F%E7%94%9F%E6%94%AF%E6%8C%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="llama.cpp昇腾原生支持 | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          llama.cpp昇腾原生支持
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-12 15:33:29" itemprop="dateCreated datePublished" datetime="2024-11-12T15:33:29+08:00">2024-11-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-23 15:34:11" itemprop="dateModified" datetime="2025-01-23T15:34:11+08:00">2025-01-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="项目背景">1. 项目背景</h2>
<p>llama.cpp
是一个开源项目，旨在将大模型高效地部署在低资源环境中，例如个人电脑或移动设备。这个项目由
Georgi Gerganov 创建，目标是通过优化和精简，使得 LLaMA 模型能够在不依赖
GPU 的情况下高效运行。llama.cpp 支持多平台和多后端，且兼容大部分
Transformer 模型和部分 CLIP
模型，便于在各种环境中部署。其模块化设计包括模型分片、KV
缓存、推理引擎和输出处理，适合边缘计算、隐私保护和低成本推理场景，帮助用户在普通设备上实现大模型推理。</p>
<h3 id="目标">1.1 目标</h3>
<p>开发基于昇腾的llama.cpp后端，实现昇腾runtime和核心算子。后端使用CANN和昇腾算子库的能力来加速大模型的推理。使得常见的模型能够在llama.cpp中使用昇腾推理，加速推理效率。</p>
<h3 id="项目概述">1.2 项目概述</h3>
<p><strong>昇腾后端和Runtime接入</strong></p>
<p>在 llama.cpp 中，为Ascend加速器提供接口适配层，使 llama.cpp
的模型推理请求能通过接口层传递至 Ascend Runtime。</p>
<p>涉及：</p>
<ul>
<li>设备接入，支持多卡接入；</li>
<li>内存管理和Tensor管理；</li>
<li>Stream，Event管理；</li>
</ul>
<p><strong>昇腾算子</strong></p>
<p>为了支持大部分的模型推理，需要实现43个算子。这些算子可以利用aclnn的算子能力构建，如果aclnn的算子不足以满足llama.cpp的算子，</p>
<p>则：</p>
<ul>
<li>优先使用aclnn算子组合的方式实现功能；</li>
<li>使用AscendC编写自定义算子。</li>
</ul>
<p>对算子的需求，可用性大于性能，为了减少开发工作量并快速完成支持，不考虑acl
op算子。能使用aclnn组合的算子，优先使用算子组合实现。</p>
<p><strong>精度和性能</strong></p>
<ul>
<li><p>实现的算子需要通过llama.cpp的精度对比测试，以及内存越界检查，确保实现的算子实现正确。</p></li>
<li><p>910B算子性能需要超过Intel CPU水平（以Intel(R) Xeon(R) Gold 6348
CPU @ 2.60GHz为例）。</p></li>
<li><p>910B模型推理（llama3
8B）性能延迟不高于100ms，吞吐率不低于300token/s。</p></li>
</ul>
<p><strong>多芯片支持</strong></p>
<ul>
<li>首先支持910B系列芯片，包括主要的模型端到端推理，q4_0，q8_0量化格式；</li>
<li>然后支持310P（910A）系列芯片，除了q4_0外(310P不支持4bit量化)，其他功能应当与910B芯片能力持平；</li>
<li>最后尝试支持310B系列芯片，310B的支持程度以aclnn和AscendC库的支持情况而定。</li>
</ul>
<p><strong>文档和用户指南</strong></p>
<ul>
<li><p>用户指南，介绍文档结构和使用说明，帮助用户理解如何在 llama.cpp
中配置和使用 Ascend 后端；</p></li>
<li><p>安装配置步骤，详细说明 Ascend
后端的安装流程，包括环境依赖、编译步骤及配置方法，以确保用户可以顺利完成安装；</p></li>
<li><p>常见问题和解决方法，总结用户在使用 Ascend
后端时可能遇到的问题，并提供解决方案，如内存溢出、兼容性问题和性能调优建议等。</p></li>
</ul>
<h2 id="设计思路">2. 设计思路</h2>
<h3 id="llama.cpp项目架构">2.1 llama.cpp项目架构</h3>
<figure>
<img
src="https://cdn-0.plantuml.com/plantuml/png/TPJFRjim3CRlVWgYzxn03qE3D1s29LZMRBiCnS1PT2fKfZo9pluKU_T9egkq1f9B1FBJxoCfmZTHCCZOkoPGAyX7Ht2rtU9k2Jjlo5q1HkZp2PuRIBzlMuz6SmyQkFFX5mO3Uunn2dqQaSN-HR6Ufr2v0OV1MH7BnuVcN_FQyiDNM67xI9ayEYgsJwVlRObDpYhOi53eoLWKWdkAevCNstioOqkG_zWW2wnyFuoYPSmCDznH84xoDHyjgwETWjNoa1npFMz8chgbaqt27J8UgIUMkSF7KT8Ls0VVKeofvsBXDVfSPzUZW4edi19pQuFdI77ETGvxN4GA9me5gSSNv7A_IIsPmLj-Ium9-NEaA4hKHrqitftdV0rVajyvK-UHKni--QUKhUgv83LwiYRPtA9WKpD5frqdqNjY2YY9CrKzfno8JOJwEhNcHX55FrszJaaP0yVph9g6lH04UtmKy9rkRXb2hKwN6zcKyEU073iVUWgwrB64DzowCQjdQziG6yWMCwCwGxtT3ycDukkjWNNNsTOIjtykGUeSbB8AFiR5tg7a-WehaNxOLCfbX4vfwfW631Hphw2BfitTMKOtxTn5aCwuwDU_XKA-abThsUPjXivhJgTPd-kCkqcd_5dv3m00"
alt="llama.cpp架构图" />
<figcaption aria-hidden="true">llama.cpp架构图</figcaption>
</figure>
<p>llama.cpp的核心功能主要涉及以上几个部分：</p>
<p><strong>模型管理</strong></p>
<p>llama.cpp不仅支持llama，而且支持多种大语言模型和一些clip模型。llama.cpp使用模型管理模块来搭建模型结构，包括算子，量化等并且加载gguf模型的信息和模型权重。由于llama.cpp支持模型拆分的功能，以便于支持多卡推理和GPU/CPU混合推理，所以模型结构会进行合适的拆分，并且管理子图之间的数据拷贝。</p>
<p><strong>kv-cache</strong></p>
<p>kv-cache有助于加速attention的计算速度，将历史的kv信息做缓存。kv-cache会直接当做算子融合到模型中，kv-cache模块本身负责cache的管理，包括cache写入，更新和替换。</p>
<p><strong>server和api接口</strong></p>
<p>llama.cpp提供了一个简单的服务端，提供api接口。server支持并发推理。</p>
<p><strong>推理引擎</strong></p>
<p>llama.cpp对推理引擎进行了抽象，以便于支持不同的后端。推理引擎负责管理设备的内存，流，事件，多卡以及GPU/CPU数据拷贝。并且计算由模型管理模块构建的模型图。</p>
<h3 id="昇腾后端接入方法">2.2 昇腾后端接入方法</h3>
<p>llama.cpp提供了一系列抽象接口来接入后端加速器：</p>
<ol type="1">
<li><strong><code>ggml_backend_cann_device_interface</code></strong>：用于描述设备接口的模块，定义了设备的基本功能。</li>
<li><strong><code>ggml_backend_cann_interface</code></strong>：用于管理后端通用接口的模块，包含常见的张量异步处理和图计算功能。</li>
<li><strong><code>ggml_backend_cann_buffer_type_host</code></strong>：负责分配主机缓冲区，确保与后端设备内存的接口兼容。</li>
<li><strong><code>ggml_cann_compute_forward</code></strong>：主计算模块，负责分派和执行各个算子操作。</li>
</ol>
<p>在 <code>ggml_cann_compute_forward</code> 中，所有的算子都作为 case
分支进行注册，表示算子名称对应具体操作，例如
<code>GGML_OP_ADD</code>、<code>GGML_OP_MUL</code> 等等。</p>
<p>昇腾接入需要实现llama.cpp的runtime接口，并且实现推理所必须的算子。</p>
<h2 id="实现原理">3. 实现原理</h2>
<h3 id="运行时">3.1 运行时</h3>
<p>runtime提供了多个抽象接口，第一阶段主要目标是基本功能支持，所以仅需要支持必要的接口。其中split
tensor功能和图推理功能暂时不实现。llama.cpp的后端接入主要是通过注册三组接口实现，分别是设备访问接口，资源管理接口，内存管理接口。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_device_i ggml_backend_cann_device_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name                = */</span> ggml_backend_cann_device_get_name,</span><br><span class="line">    <span class="comment">/* .get_description         = */</span> ggml_backend_cann_device_get_description,</span><br><span class="line">    <span class="comment">/* .get_memory              = */</span> ggml_backend_cann_device_get_memory,</span><br><span class="line">    <span class="comment">/* .get_type                = */</span> ggml_backend_cann_device_get_type,</span><br><span class="line">    <span class="comment">/* .get_props               = */</span> ggml_backend_cann_device_get_props,</span><br><span class="line">    <span class="comment">/* .init_backend            = */</span> ggml_backend_cann_device_init,</span><br><span class="line">    <span class="comment">/* .get_buffer_type         = */</span> ggml_backend_cann_device_get_buffer_type,</span><br><span class="line">    <span class="comment">/* .get_host_buffer_type    = */</span> ggml_backend_cann_device_get_host_buffer_type,</span><br><span class="line">    <span class="comment">/* .buffer_from_host_ptr    = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .supports_op             = */</span> ggml_backend_cann_supports_op,</span><br><span class="line">    <span class="comment">/* .supports_buft           = */</span> ggml_backend_cann_supports_buft,</span><br><span class="line">    <span class="comment">/* .offload_op              = */</span> ggml_backend_cann_offload_op,</span><br><span class="line">    <span class="comment">/* .event_new               = */</span> ggml_backend_cann_device_event_new,</span><br><span class="line">    <span class="comment">/* .event_free              = */</span> ggml_backend_cann_device_event_free,</span><br><span class="line">    <span class="comment">/* .event_synchronize       = */</span> ggml_backend_cann_device_event_synchronize,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>此接口 <code>ggml_backend_cann_device_interface</code> 为 CANN 后端在
llama.cpp 中提供了一个通用的设备访问与操作抽象层，便于整合并统一管理
CANN 设备资源。通过实现接口中的各个函数，用户可以控制 CANN
设备的初始化、资源分配、操作支持检测等关键功能，从而确保 llama.cpp
中的模型计算能够顺利利用 CANN 的加速能力。</p>
<p>以下是接口中各函数的功能描述：</p>
<ul>
<li><p><strong>ggml_backend_cann_device_get_name</strong>：返回设备的名称，用于识别不同的设备类型。例如可以返回
"CANN 设备" 或者具体的设备型号。</p></li>
<li><p><strong>ggml_backend_cann_device_get_description</strong>：返回设备的详细描述信息，通常包含设备的硬件特性以及版本信息等，帮助用户理解设备特性。</p></li>
<li><p><strong>ggml_backend_cann_device_get_memory</strong>：获取设备的内存信息，包括总内存大小和当前可用内存，以便
llama.cpp 优化内存分配策略。</p></li>
<li><p><strong>ggml_backend_cann_device_get_type</strong>：返回设备类型，用于区分不同种类的设备（如
CPU、GPU、NPU 等），便于进行不同类型设备的适配。</p></li>
<li><p><strong>ggml_backend_cann_device_get_props</strong>：获取设备的属性信息，包括计算能力、内存带宽等。这些属性信息可用于优化计算分配和选择适合的算子。</p></li>
<li><p><strong>ggml_backend_cann_device_init</strong>：初始化后端设备，确保设备的资源和状态准备就绪。这一步通常在加载模型或开始计算之前调用。</p></li>
<li><p><strong>ggml_backend_cann_device_get_buffer_type</strong>：返回设备内存缓冲区的类型信息，帮助
llama.cpp 决定如何在设备端管理数据缓冲。</p></li>
<li><p><strong>ggml_backend_cann_device_get_host_buffer_type</strong>：返回主机端缓冲区类型，用于在主机和设备之间进行高效的数据交换。</p></li>
<li><p><strong>buffer_from_host_ptr</strong>：该接口可用于将主机端内存直接映射或转换为设备端缓冲区，若未来需求可扩展。</p></li>
<li><p><strong>ggml_backend_cann_supports_op</strong>： 检查 CANN
设备是否支持指定的操作（op），确保模型中的特定操作能够得到设备的加速支持。</p></li>
<li><p><strong>ggml_backend_cann_supports_buft</strong>：检查设备是否支持指定的缓冲区类型，确保数据在缓冲区类型上的一致性和兼容性。</p></li>
<li><p><strong>ggml_backend_cann_offload_op</strong>：将计算操作卸载到设备端执行，提升操作效率和加速模型推理过程。</p></li>
<li><p><strong>ggml_backend_cann_device_event_new</strong>：创建新的事件对象，用于异步操作的状态跟踪，如操作完成的通知。</p></li>
<li><p><strong>ggml_backend_cann_device_event_free</strong>：释放事件对象，清理事件资源，确保内存不被泄漏。</p></li>
<li><p><strong>ggml_backend_cann_device_event_synchronize</strong>：同步事件，确保指定的异步操作完成。这通常用于确保操作的执行顺序。</p></li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_i ggml_backend_cann_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name                = */</span> ggml_backend_cann_name,</span><br><span class="line">    <span class="comment">/* .free                    = */</span> ggml_backend_cann_free,</span><br><span class="line">    <span class="comment">/* .set_tensor_async        = */</span> ggml_backend_cann_set_tensor_async,</span><br><span class="line">    <span class="comment">/* .get_tensor_async        = */</span> ggml_backend_cann_get_tensor_async,</span><br><span class="line">    <span class="comment">/* .cpy_tensor_async        = */</span> ggml_backend_cann_cpy_tensor_async,</span><br><span class="line">    <span class="comment">/* .synchronize             = */</span> ggml_backend_cann_synchronize,</span><br><span class="line">    <span class="comment">/* .graph_plan_create       = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_free         = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_update       = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_plan_compute      = */</span> <span class="literal">NULL</span>,</span><br><span class="line">    <span class="comment">/* .graph_compute           = */</span> ggml_backend_cann_graph_compute,</span><br><span class="line">    <span class="comment">/* .event_record            = */</span> ggml_backend_cann_event_record,</span><br><span class="line">    <span class="comment">/* .event_wait              = */</span> ggml_backend_cann_event_wait,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>ggml_backend_cann_interface</code> 接口提供了 CANN 后端在
llama.cpp 中的资源管理、异步数据传输、计算图执行等功能接口，实现了与
CANN 后端的深度集成。通过该接口，llama.cpp
可以高效地管理张量的异步操作、事件记录、同步及图计算，确保计算任务能够顺畅运行在
CANN 设备上。</p>
<p>以下是接口中各函数的功能描述：</p>
<ul>
<li><p><strong>ggml_backend_cann_name</strong>：
返回后端名称，通常用于标识该后端为 CANN 后端。</p></li>
<li><p><strong>ggml_backend_cann_free</strong>：释放后端资源，确保内存和其他资源在后端不再使用时被正确回收。</p></li>
<li><p><strong>ggml_backend_cann_set_tensor_async</strong>：异步设置张量数据到设备端，为后续计算提供数据准备。异步设置可提高数据传输的效率。</p></li>
<li><p><strong>ggml_backend_cann_get_tensor_async</strong>
：异步获取张量数据，方便在计算完成后从设备端提取数据，避免阻塞主线程。</p></li>
<li><p><strong>ggml_backend_cann_cpy_tensor_async</strong>：异步复制张量数据，支持设备端和主机端之间的数据交互或设备内部的数据拷贝，以便于多任务并行处理。</p></li>
<li><p><strong>ggml_backend_cann_synchronize</strong>
：同步操作，确保所有异步任务完成，通常用于确保张量操作和事件顺序执行。</p></li>
<li><p><strong>graph_plan_create</strong>：该接口目前未实现。将来可用于创建计算图执行计划，优化计算图的操作顺序和资源分配。</p></li>
<li><p><strong>graph_plan_free</strong>：该接口目前未实现。可以释放计算图计划的资源，确保内存使用的高效管理。</p></li>
<li><p><strong>graph_plan_update</strong>：该接口目前未实现。可用于在图执行过程中动态更新计算计划，以适应运行时的资源情况。</p></li>
<li><p><strong>graph_plan_compute</strong>：该接口目前未实现。未来可能用于执行图计划中的所有操作，便于更复杂的任务调度。</p></li>
<li><p><strong>ggml_backend_cann_graph_compute</strong>：执行计算图中的所有节点操作，是核心计算接口之一。该函数负责协调图中的计算任务，使之并行或顺序执行。</p></li>
<li><p><strong>ggml_backend_cann_event_record</strong>：记录事件，用于标记特定操作的时间点，便于在异步计算中追踪进度和执行状态。</p></li>
<li><p><strong>ggml_backend_cann_event_wait</strong>：等待特定事件完成，通常用于确保在后续操作开始前当前任务已完成，以保持计算图的执行正确性。</p></li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> ggml_backend_buffer_type_i ggml_backend_cann_buffer_type_interface = &#123;</span><br><span class="line">    <span class="comment">/* .get_name         = */</span> ggml_backend_cann_buffer_type_name,</span><br><span class="line">    <span class="comment">/* .alloc_buffer     = */</span> ggml_backend_cann_buffer_type_alloc_buffer,</span><br><span class="line">    <span class="comment">/* .get_alignment    = */</span> ggml_backend_cann_buffer_type_get_alignment,</span><br><span class="line">    <span class="comment">/* .get_max_size     = */</span> <span class="literal">NULL</span>,  <span class="comment">// defaults to SIZE_MAX</span></span><br><span class="line">    <span class="comment">/* .get_alloc_size   = */</span> ggml_backend_cann_buffer_type_get_alloc_size,</span><br><span class="line">    <span class="comment">/* .is_host          = */</span> ggml_backend_cann_buffer_type_is_host,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>ggml_backend_cann_buffer_type_interface</code> 结构体定义了
CANN
后端缓冲区类型的接口，它提供了一组操作缓冲区属性和行为的函数接口。这个接口使得
CANN 后端的缓冲区能够在 llama.cpp
中被正确管理和使用，确保内存分配、对齐、大小等操作的一致性和高效性。</p>
<p>以下是 <code>ggml_backend_cann_buffer_type_interface</code>
结构体中各字段的功能描述：</p>
<ul>
<li><strong>ggml_backend_cann_buffer_type_name</strong>：返回缓冲区类型的名称。该函数用于标识当前缓冲区类型，主要用于调试和日志记录。</li>
<li><strong>ggml_backend_cann_buffer_type_alloc_buffer</strong>：用于分配缓冲区的内存。通过该函数，llama.cpp
可以请求 CANN 后端分配指定大小的内存块，用于存储数据和张量。</li>
<li><strong>ggml_backend_cann_buffer_type_get_alignment</strong>：返回缓冲区的对齐方式。内存对齐对于性能至关重要，因为不适当的对齐可能导致
CPU 或 GPU
在访问数据时的效率降低。该函数可以确保数据在内存中的对齐符合硬件的要求。</li>
<li><strong>get_max_size</strong> ：该字段指示缓冲区的最大尺寸，若设置为
<code>NULL</code>，则默认最大值为
<code>SIZE_MAX</code>，即没有固定的尺寸限制。此函数适用于不希望为缓冲区大小设定上限的场景。</li>
<li><strong>ggml_backend_cann_buffer_type_get_alloc_size</strong>
：获取缓冲区实际分配的内存大小。该函数确保返回正确的分配大小，便于用户跟踪内存使用情况。</li>
<li><strong>ggml_backend_cann_buffer_type_is_host</strong>：判断缓冲区是否为主机缓冲区。该函数用于区分主机内存和设备内存，以便进行适当的内存管理和数据传输。</li>
</ul>
<p><strong>Host buffer</strong></p>
<p>Host buffer是一种特殊的buffer
type，用于在CPU上申请内存，用于一些中间数据的临时存储，为后端设备提供了以快速访问的内存区域。</p>
<p>Pin
memory，又称“锁页内存”或“固定内存”，是指将主机内存中的一部分内存固定在物理内存上，以便快速传输至计算设备（如GPU或NPU）。通常情况下，操作系统会将不经常使用的内存页移至虚拟内存中，这可能导致数据传输时出现额外的内存访问延迟。而使用Pin
memory则可以避免这种情况，因为锁页内存不会被系统交换出物理内存，从而大大加速数据传输过程。</p>
<p>Host buffer使用Pin
memory实现，用户加速Host和Device之间的内存拷贝速度。Host
buffer与buffer_type的结构相同，以接口注册的方式提供Host
buffer的能力。</p>
<p><strong>Split Tensor</strong></p>
<p>Split
Tensor用于在做复杂计算的时候充分利用多卡能力，llama.cpp中，对矩阵乘法，使用到了Split
Tensor，计算时会相乘的矩阵其中一个进行拆分，使用多卡进行并行计算，计算完成后做结果的合并。</p>
<p>Split
Tensor实现复杂，并且无法利用已有的aclnn算子，在本次设计中不考虑，待后续性能提升中考虑实现。</p>
<h3 id="算子">3.2 算子</h3>
<p>llama.cpp主要的推理是单算子推理功能，图推理功能在本次设计中暂不考虑实现。昇腾的单算子支持aclop以及aclnn两种调用方式。经过简单的demo进行性能对比，aclop编译执行的方式执行效率较低，主要算子均通过aclnn实现，aclnn不支持的算子使用aclnn基本算子组合的方式实现，后续需要使用AscendC将组合算子进行融合以提高性能。</p>
<h4 id="tensor转换">3.2.1 Tensor转换</h4>
<p>llama.cpp和昇腾算子对Tensor的定义有一定的差异，为了能够使用昇腾算子，需要在调用的时候对Tensor结构做转换。</p>
<p><strong>结构差异</strong></p>
<p>两者的Tensor基本上都是数据和dims，nelements，nstride，dtype的属性集合，但是有一些差异：</p>
<ul>
<li>llama.cpp的ne和nb的顺序是从内到外，也就是与传统意义的维度顺序相反，序号小的是最内的维度。</li>
<li>llama.cpp的stride的单位是字节，而aclnn的stride单位是元素。</li>
</ul>
<p><strong>广义broadcast</strong></p>
<p>当两个计算的tensor维度不同时，会尝试做broadcast，aclnn接口支持的是传统broadcast方式，而llama.cpp支持的是广义的broadcast：</p>
<ul>
<li>aclnn接口的broadcast仅会在Tensor的某个维度不同，但是其中一个Tensor的维度为1的时候发生；</li>
<li>llama.cpp的broadcast会在Tensor的某个维度不同，但一个Tensor的维度大小是另外一个的整数倍的时候发生。</li>
</ul>
<p>为了减少显示broadcast带来的性能和内存的开销，需要进行维度的调整，以便于利用算子的broadcast特性：</p>
<p>例如，Tensor A(9,5,2,7)， Tensor
B(9,10,2,7)，这两个Tensor对aclnn接口来说不可自动broadcast，但是对llama.cpp来说允许自动broadcast。当数据内容连续时，可以通过添加一个维度来兼容aclnn的broadcast规则。
通过将
A(9,5,2,7)转换成A'(9,5,1,2,7)，B(9,10,2,7)转换成B'(9,5,2,2,7)。仅通过调整dims信息，即可利用aclnn算子的自动broadcast能力。</p>
<h4 id="aclnn算子">3.2.1 aclnn算子</h4>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 43%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr>
<th>算子名称</th>
<th>描述</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>Elementwise Add</td>
<td>对两个张量进行逐元素加法，并将结果存储在目标张量中。</td>
<td><span class="math inline">\(dst(i)=src0(i)+src1(i)\)</span></td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td>对输入张量应用 Leaky ReLU 激活函数，并将结果存储在目标张量中。</td>
<td><span class="math display">\[\text{dst(i)}
=\begin{cases}\text{src(i)} &amp; \text{src(i)} \geq \text{0}
\\\text{negative-slope} \times \text{src(i)}&amp; \text{src(i)} &lt;
\text{0}\end{cases}\]</span></td>
</tr>
<tr>
<td>Arange</td>
<td>创建一个从 <code>start</code> 开始，到 <code>stop</code>
结束，每次增长 <code>step</code> 的 Tensor。</td>
<td><span class="math inline">\(\text {out }_{i+1}=\text {out }_i+\text
{step}\)</span></td>
</tr>
<tr>
<td>Clamp</td>
<td>将 input 张量的每个元素夹紧到区间 [min, max]
中，并将结果返回到新的张量中。</td>
<td><span class="math display">\[  \text{dst}(i)
=  \begin{cases}  \text{min} &amp; \text{src(i)} &lt; \text{min}
\\  \text{src(i)} &amp; \text{min} \leq \text{src(i)} \leq \text{min}
\\  \text{max} &amp; \text{src(i)} &gt;
\text{max}  \end{cases}  \]</span></td>
</tr>
<tr>
<td>Scale</td>
<td>使用 <code>scale</code> 缩放一个 Tensor
的所有元素，将结果返回到新的张量中。</td>
<td><span class="math inline">\(dst(i) = src(i) \times
scale\)</span></td>
</tr>
<tr>
<td>Argsort</td>
<td>将输入 Tensor 中的元素根据某个维度进行升序 / 降序排序，返回对应的
index 值。</td>
<td>-</td>
</tr>
<tr>
<td>Layer Norm</td>
<td>对指定层进行均值为 0、标准差为 1
的归一化计算，并将结果写入到新的张量中。</td>
<td><span class="math inline">\(out = \frac{x - E[x]}{\sqrt{Var[x] +
\epsilon}} \times w + b\)</span></td>
</tr>
<tr>
<td>Group Norm</td>
<td>计算输入的组归一化结果返回到新的张量中。</td>
<td>$ out &amp;= + \$</td>
</tr>
<tr>
<td>Acc</td>
<td>将 src 张量的数据累加到 dst 中。</td>
<td><span class="math inline">\(dst(i) = src(i) + dst(i)\)</span></td>
</tr>
<tr>
<td>Sum Rows</td>
<td>返回给定维度中输入张量每行的和。</td>
<td>-</td>
</tr>
<tr>
<td>Upsample Nearest2d</td>
<td>对由多个输入通道组成的输入信号应用最近邻插值算法进行上采样。</td>
<td>-</td>
</tr>
<tr>
<td>Pad</td>
<td>将 Tensor 填充到与目标 Tensor 相同的尺寸。</td>
<td>-</td>
</tr>
<tr>
<td>avg pool2d</td>
<td>对输入 Tensor 进行窗口为 kH×kW、步长为 sH×sW
的二维平均池化操作。</td>
<td><span class="math display">\[\text{out}\left(N_{i}, C_{i}, h,
w\right) = \frac{1}{k H \cdot k W} \sum_{m=0}^{k H-1} \sum_{n=0}^{k W-1}
\text{input}\left(N_{i}, C_{i}, \text{stride}[0] \times h + m,
\text{stride}[1] \times w + n\right)\]</span></td>
</tr>
<tr>
<td>max pooling</td>
<td>对于 dim=3 或 4 维的输入张量，进行最大池化操作。</td>
<td><span class="math inline">\(\text{out}\left(N_{i}, C_{i}, h,
w\right) = \max_{m=0}^{k H-1} \max_{n=0}^{k W-1}
\text{input}\left(N_{i}, C_{i}, \text{stride}[0] \times h + m,
\text{stride}[1] \times w + n\right)\)</span></td>
</tr>
<tr>
<td>rms norm</td>
<td>计算给定 Tensor 的均方根归一化函数，并将结果写入到输出 Tensor
中。</td>
<td><span
class="math inline">\(\text{RmsNorm}\left(x_i\right)=\frac{x_i}{\text{Rms}(\mathbf{x})}
g_i,&lt;br/&gt;   *\quad \text { where }
\text{Rms}(\mathbf{x})=\sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2+e p
s}\)</span></td>
</tr>
<tr>
<td>diag mask</td>
<td>将 Tensor 进行三角形掩码运算，将下三角部分保留，上三角部分置
1。</td>
<td>-</td>
</tr>
<tr>
<td>img2col</td>
<td>用于将二维 Tensor
数据转换成矩阵形式，以便于高效地进行卷积运算。</td>
<td>-</td>
</tr>
<tr>
<td>timestep_embedding</td>
<td>用于生成时间步嵌入。</td>
<td><span class="math inline">\(\text{dst}(t) =
[\sin(\frac{t}{10000^{2i/d}}),
\cos(\frac{t}{10000^{2i/d}})]\)</span></td>
</tr>
<tr>
<td>softmax</td>
<td>将输入的张量转化为概率分布，其值范围在 [0, 1] 之间，总和为 1。</td>
<td>$ (x_i) = $</td>
</tr>
<tr>
<td>matmul</td>
<td>计算两个 Tensor 的矩阵乘法，结果返回到新的 Tensor 中。</td>
<td>$ C_{ij} = <em>{k=1}^{n} A</em>{ik} B_{kj}$</td>
</tr>
<tr>
<td>Rope</td>
<td>算子是一种位置编码方法，通过旋转操作为输入序列引入位置信息，增强模型对位置关系的感知能力。</td>
<td><span class="math inline">\(\text{ROPE}(q, k) = \left[
q_{\text{even}} \cos(\theta) - q_{\text{odd}} \sin(\theta), ;
q_{\text{odd}} \cos(\theta) + q_{\text{even}} \sin(\theta)
\right]\)</span></td>
</tr>
<tr>
<td>repeat</td>
<td>对输入张量的元素沿特定维度重复，扩展原始数据的维度或增加相同数据的次数。</td>
<td><span class="math inline">\(\text{repeat}(x) = [x, x, \dots, x]
\quad (\text{repeated along specified dimension})\)</span></td>
</tr>
<tr>
<td>concat</td>
<td>将两个或多个张量在指定维度上拼接。</td>
<td><span class="math inline">\(\text{concat}(x_1, x_2, \dots, x_n) =
[x_1, x_2, \dots, x_n] \quad (\text{along specified
dimension})\)</span></td>
</tr>
<tr>
<td>Cast</td>
<td>将张量的数据类型从一种类型转换为另一种类型。</td>
<td>-</td>
</tr>
<tr>
<td>permute</td>
<td>重新排列张量的维度顺序。</td>
<td>-</td>
</tr>
<tr>
<td>exp</td>
<td>对 Tensor 的每个元素执行 exp 指数运算。</td>
<td><span class="math inline">\(\text{dst}_i =
e^{\text{src}_i}\)</span></td>
</tr>
<tr>
<td>Elementwise Mul</td>
<td>对两个张量对应元素进行乘法运算。</td>
<td><span class="math inline">\(z = x \times y\)</span></td>
</tr>
<tr>
<td>Cos</td>
<td>对张量的每个元素计算余弦值。</td>
<td><span class="math inline">\(y = \cos(x)\)</span></td>
</tr>
<tr>
<td>Sin</td>
<td>对张量的每个元素计算正弦值。</td>
<td><span class="math inline">\(y = \sin(x)\)</span></td>
</tr>
<tr>
<td>fill scalar</td>
<td>将张量的所有元素填充为指定的标量值。</td>
<td><span class="math inline">\(x[:] = \text{scalar}\)</span></td>
</tr>
<tr>
<td>pow tensor</td>
<td>将一个张量的每个元素提升到对应的指数幂。</td>
<td><span class="math inline">\(y = x^{\text{power}}\)</span></td>
</tr>
<tr>
<td>Alibi</td>
<td>一种相对位置嵌入策略，在注意力分数中加入线性偏置，帮助捕获相对位置信息。</td>
<td>$(i, j) = -m </td>
</tr>
<tr>
<td>repeat interleave</td>
<td>对张量的每个元素按指定次数重复，以在张量中插入更多的副本。</td>
<td><span class="math inline">\(\text{dst}(x, \text{repeats}) = [x_1,
x_1, \dots, x_1, x_2, x_2, \dots, x_2, \dots]\)</span></td>
</tr>
<tr>
<td>roll</td>
<td>将张量元素沿指定维度循环移动，即滚动。</td>
<td><span class="math inline">\(\text{roll}(x, \text{shift}) =
x_{\text{shifted along axis}}\)</span></td>
</tr>
<tr>
<td>index fill tensor</td>
<td>在张量的特定索引位置填充指定值。</td>
<td><span class="math inline">\(dst[\text{index}] =
\text{src}\)</span></td>
</tr>
</tbody>
</table>
<h4 id="ascendc算子">3.2.2 AscendC算子</h4>
<p>以下算子没有aclnn接口可调用，也无法使用基础算子组合，需要通过AscendC编程语言实现。为了简化算子的调用流程，采用kernel
call的方式进行调用。AscendC算子独立编译，以<code>.a</code>的方式链接到llama.cpp中。</p>
<p><strong>dup</strong></p>
<p>dup和copy语义相同，均为Tensor之间的拷贝，需要支持：</p>
<ul>
<li>量化Tensor和非量化Tensor之间的拷贝，拷贝过程中涉及到量化和反量化的计算过程。需要支持Q4_0和Q8_0两种量化格式。非量化格式需要支持fp32和fp16两种格式。</li>
<li>连续Tensor和非连续Tensor之间的拷贝（量化格式Tensor不涉及非连续场景）。</li>
</ul>
<p><strong>get rows</strong></p>
<p>从Tensor中按照index获取每行内容。</p>
<p>需要支持多种数据格式，包括fp32，fp16，Q4_0和Q8_0。获取后的数据均为fp32格式。</p>
<p>AscendC算子通过kernel launch的方式调用，调用时需要判断AI
core的数量，来配置合适的数量以提升执行效率。</p>
<p>为了兼容多种芯片，CMake时需要检测或根据提供的芯片类型进行编译和链接。</p>
<h3 id="内存管理">3.3 内存管理</h3>
<p>aclnn执行时，有些需要申请临时的NPU上内存做临时数据存储，频繁的内存分配和释放效率很低，需要内存池来提高内存分配性能。</p>
<p>在llama.cpp中，需要实现2中内存池：</p>
<p><strong>legacy pool</strong></p>
<p>使用N（256）个buffer做内存缓存，所有的内存释放必须放回内存池（防止异步执行访问到已释放内存），内存申请首先选择内存池中大小最合适的缓存，<strong>内存池为空则去申请内存</strong>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/47dd1352ecaef88457cf9dc7adba3f6e-image-20240806193006017-6f2692.png"
alt="legacy pool流程图" />
<figcaption aria-hidden="true">legacy pool流程图</figcaption>
</figure>
<p>会占用额外的内存，并且存在内存块查找的开销，并且，如果free的内存块超过N（256），则会出现assert失败问题。</p>
<p><strong>vmm pool</strong></p>
<p>使用虚拟内存，业务代码看到的是一段连续的内存，方便使用。实际上申请的物理内存是非连续的，当内存不足时申请一段物理内存映射到虚拟内存中。避免内存碎片和占用额外内存。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/14e4c332447ad345329843f1478b39a3-image-20240806193622691-6c7fee.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p>在虚拟内存中，申请的数据紧密排列，申请和销毁的顺序是相反的。比如，buffer1早于buffer2申请，那么buffer2必须要早于buffer1释放。在内存管理中，仅维护一个free指针，指示下一个buffer申请的起始地址。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/70ebc785a616889aa3601a77a8d74fe0-image-20240806194704808-3b08a3.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p><strong>异步计算的内存延迟释放</strong></p>
<p>由于所有的算子计算都是异步的，但是内存的申请和释放并不是异步的，所以，需要保证在异步计算完成之前，申请的内存是有效的。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/73704ff1cfbe476e085afc7fbfdf0c6a-image-20240806195347967-5ab4b4.png"
alt="vmm pool示意图" />
<figcaption aria-hidden="true">vmm pool示意图</figcaption>
</figure>
<p>如图所示，当算子提交完成后，buffer3就会释放，free指针指向buffer3的起始地址。接着，下个算子开始执行，会从free指针开始申请内存，此时buffer3和buffer4是重叠的，但是由于stream中的算子计算有序，所以buffer3内的数据在完成计算之前，是不会被buffer4修改的。</p>
<h3 id="量化格式">3.4 量化格式</h3>
<p>以4bit量化为例：</p>
<p><strong>量化分组格式</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> QK4_0 32 <span class="comment">// 每组32个f32数据</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">    ggml_half d; <span class="comment">// 公共系数</span></span><br><span class="line">    <span class="type">uint8_t</span> qs[QK4_0 / <span class="number">2</span>]; <span class="comment">// 4bit存储的数据</span></span><br><span class="line">&#125; block_q4_0;</span><br></pre></td></tr></table></figure>
<p><strong>量化算法描述</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quantize_row_q4_0_reference</span><span class="params">(<span class="type">const</span> <span class="type">float</span> * restrict x, block_q4_0 * restrict y, <span class="type">int64_t</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="type">static</span> <span class="type">const</span> <span class="type">int</span> qk = QK4_0;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">assert</span>(k % qk == <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> nb = k / qk;</span><br><span class="line">		</span><br><span class="line">  	<span class="comment">// 1. 找到绝对值最大的数的值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nb; i++) &#123;</span><br><span class="line">        <span class="type">float</span> amax = <span class="number">0.0f</span>; <span class="comment">// absolute max</span></span><br><span class="line">        <span class="type">float</span> max  = <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; qk; j++) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> v = x[i*qk + j];</span><br><span class="line">            <span class="keyword">if</span> (amax &lt; <span class="built_in">fabsf</span>(v)) &#123;</span><br><span class="line">                amax = <span class="built_in">fabsf</span>(v);</span><br><span class="line">                max  = v;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 2. 公共系数是第一步的值除以 -8</span></span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> d  = max / <span class="number">-8</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> id = d ? <span class="number">1.0f</span>/d : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        y[i].d = <span class="built_in">GGML_FP32_TO_FP16</span>(d);</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 3. 对组内的所有数据，除以公共系数，然后按以下顺序存储</span></span><br><span class="line">        <span class="comment">// 量化前： 1,2,3,4,5,6,7,8 ...... 30,31</span></span><br><span class="line">      	<span class="comment">// 量化后： 1,17,2,18,3,19 ...... 16,32</span></span><br><span class="line">        <span class="comment">// 也就是数据按顺序先填充量化后组的低4位，然后再填充高4位。</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; qk/<span class="number">2</span>; ++j) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x0 = x[i*qk + <span class="number">0</span>    + j]*id;</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x1 = x[i*qk + qk/<span class="number">2</span> + j]*id;</span><br><span class="line">						<span class="comment">// 量化的值+8.5，向上去整并转无符号数。</span></span><br><span class="line">            <span class="type">const</span> <span class="type">uint8_t</span> xi0 = <span class="built_in">MIN</span>(<span class="number">15</span>, (<span class="type">int8_t</span>)(x0 + <span class="number">8.5f</span>));</span><br><span class="line">            <span class="type">const</span> <span class="type">uint8_t</span> xi1 = <span class="built_in">MIN</span>(<span class="number">15</span>, (<span class="type">int8_t</span>)(x1 + <span class="number">8.5f</span>));</span><br><span class="line"></span><br><span class="line">            y[i].qs[j]  = xi0;</span><br><span class="line">            y[i].qs[j] |= xi1 &lt;&lt; <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>NPU善于做向量和矩阵计算，按字节的计算，以及位计算性能不佳。
所以需要调整数据存储格式。</p>
<p>在set
tensor过程中，使用cpu做tensor的内存调整，让后续的计算能够充分利用NPU能力。</p>
<figure>
<img
src="https://raw.githubusercontent.com/hipudding/blog_img/main/img/2024/08/08/3580ce93f92d2075f1ee71f434f9d54a-image-20240806191922718-496986.png"
alt="量化内存格式调整示意图" />
<figcaption aria-hidden="true">量化内存格式调整示意图</figcaption>
</figure>
<p>如上图所示，输入的Tensor是按组存放的，每组存放了该组的公共系数，以及32个数据的量化后的值，int4类型量化值是先填充高4位，再填充低4位。为了昇腾的计算效率，在做这类伪量化算法时，将原始Tensor拆解成2个Tensor，一个按顺序记录所有的值，另外一个记录每一组的公共系数，并且值和公共系数按32:1的方式对应。然后昇腾算子按照weight和group
scale的方式输入进算子，能够提高量化后Tensor的执行效率。</p>
<p>所有的内存布局修改的时机是set tensor和get
tensor过程中，对于整个程序来说，对内存布局的修改是不感知的，拷贝到NPU上时进行布局修改，从NPU下载时再进行布局复原，这样，及时设计CPU，NPU混合运算，也不会影响数据的正确性。</p>
<p>对于更加复杂的量化方式，例如q5_0，需要设计到位运算，此类量化由于性能问题尚未支持。</p>
<h3 id="代码风格和注释">3.5 代码风格和注释</h3>
<p>llama.cpp社区对代码风格没有详细的要求，社区仅要求”清除所有尾随空格，使用
4 个空格缩进，括号在同一行<code>void * ptr</code>，int &amp;
a`”。并且，对注释也没有明确的要求。为了能够保持一致的风格，以及方便社区开发者了解昇腾后端的业务逻辑，需要在编写代码时遵循一致的编码规范和详尽的注释。</p>
<ul>
<li>除了社区要求的内容之外，其他代码规范需要遵循google编码规范；</li>
<li>注释需要包含函数和变量的介绍，参数和返回值说明，算子相关代码需要注释算法的数学公式。其他的复杂逻辑按需求添加注释；</li>
<li>注释需要符合doxygen风格，以便于生成方便阅读的手册。</li>
</ul>
<h2 id="测试和验证">4. 测试和验证</h2>
<p>本设计文档主要是昇腾的后端支持，llama.cpp已经做了后端抽象，所有测试用例可以复用社区的内容。针对社区用例没有看护到的部分，添加必要的用例来看护。</p>
<h3 id="runtime测试">4.1 Runtime测试</h3>
<p>Runtime测试主要是验证设备注册，内存分配，stream和event管理相关功能。</p>
<p><strong>设备注册和卸载（单卡，多卡）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证昇腾设备可以正常注册到llama.cpp中，支持单卡和多卡注册。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>调用设备注册接口，注册单卡以及多卡；</li>
<li>查看设备信息是否正常获取。</li>
</ol>
<p><strong>预期结果</strong></p>
<p>无报错信息，并根据<code>ASCEND_VISIBLE_DEVICES</code>的设置情况，能够正常获取到对应的设备信息。</p>
<p><strong>buffer和Tensor创建</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证昇腾后端可以正常的创建buffer以及llama.cpp的Tensor结构；</li>
</ul>
<p><strong>测试步骤</strong></p>
<ul>
<li>构造若干个Tensor结构，并给这些Tensor分配内存buffer；</li>
</ul>
<p><strong>预期结果</strong></p>
<ul>
<li>内存完成分配，无错误信息。</li>
</ul>
<p><strong>Tensor的上传和下载（同步，异步）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证数据可以正确的上传和下载。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机数据；</li>
<li>将数据拷贝到创建好的Tensor中；</li>
<li>将Tensor中的数据下载；</li>
<li>与原始随机数据进行对比。</li>
<li>分别使用同步拷贝和异步拷贝，重复以上过程。</li>
</ol>
<p><strong>预期结果</strong></p>
<ul>
<li>数据比对与原始数据相同。</li>
</ul>
<p><strong>Tensor卡间拷贝（包括event同步）</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>验证卡间拷贝以及事件同步的正确性。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机数据；</li>
<li>将数据拷贝到卡1的Tensor中；</li>
<li>开启卡1和卡2的卡间拷贝开关；</li>
<li>在卡1的stream提交卡1Tensor向卡2Tensor拷贝的任务；</li>
<li>在拷贝流中插入卡2的event事件；</li>
<li>在卡2的流中等待event事件；</li>
<li>从卡2中下载Tensor数据；</li>
<li>与原始数据做比对。</li>
</ol>
<p><strong>预期结果</strong></p>
<ol type="1">
<li>数据比对与原始数据相同；</li>
<li>卡2event同步正确，在卡1stream中构造耗时操作，确保event能够等待拷贝动作结束。</li>
</ol>
<p><strong>量化拷贝验证</strong></p>
<p><strong>测试目的</strong></p>
<ul>
<li>量化Tensor拷贝需要调整内存布局，验证量化Tensor的拷贝结果正确。</li>
</ul>
<p><strong>测试步骤</strong></p>
<ol type="1">
<li>构造随机的量化Tensor；</li>
<li>将量化Tensor上传到设备上；</li>
<li>使用aclrtmemcpy直接拷贝数据；</li>
<li>从设备上将Tensor下载下来；</li>
<li>与原始数据作对比。</li>
</ol>
<p><strong>预期结果</strong></p>
<ol type="1">
<li>步骤3的memcpy的结果与原始数据不同，因为上传过程做了内存布局调整；</li>
<li>步骤5数据对比与原始数据相同。</li>
</ol>
<h3 id="算子单元测试">4.2 算子单元测试</h3>
<p>单元测试复用社区的单元测试用例(test-backend-ops)，包含1500多个用例。其覆盖的场景有：</p>
<ul>
<li>算子多shape多dtype验证，保证该算子所有的输出输出的shape和dtype类型都能够覆盖；</li>
<li>算子的精度验证，用例会构造随机数据，分别在设备上和CPU上运行，最后对比精度，两个Tensor的归一化方差需要小于
1e-6。</li>
<li>计算结果越界检查，由于推理过程中，Tensor是紧密排列，所以每个tensor的计算结果不能越界，否则会损坏其他tensor的数据，用例会在每个输入和输出tensor前后分别放置一个随机tensor，通过对比随机tensor的计算前后的结果，来检查是否存在越界行为。</li>
</ul>
<p>单元测试用例会判断后端的算子支持情况，理论上，所有支持的算子（包括shape和dtype）都需要通过该测试用例集的验证。</p>
<h3 id="性能测试">4.3 性能测试</h3>
<p>算子的性能测试用例与单元测试用例相同，区别是性能测试用例不会验证精度，也不会创建随机tensor用作越界检查。性能测试会构造一个特殊的图，包含最多8192个计算节点，然后交给后端进行推理，并计算平均每次的执行时间，以及数据吞吐率。</p>
<ul>
<li>910B对于简单算子（包括直接调用aclnn接口的，或者做了简单的参数调整的）性能要超过Intel主流CPU的性能。</li>
<li>对于复杂算子（包括构造多个临时tensor，以及需要多个算子组合的）暂不做算子的性能要求。</li>
<li>非910B芯片，不做性能要求。</li>
</ul>
<p>910B模型推理（llama3
8B）整体性能，token延迟需要小于100ms（人类的阅读速度大致是10个token/s，延迟小于100ms，可以满足人类的阅读需求），吞吐需要超过300token/s（0.6
* A100 vllm llama3 8B的推理性能）。</p>
<p>以下为 Qwen 2.5 全系列模型在昇腾 910B 上的推理性能表现汇总数据，包括
Qwen2.5 0.5B、1.5B、3B 的 Q8_0 和Q4_0
量化的推理性能数据作为对比参考：</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 27%" />
<col style="width: 16%" />
<col style="width: 15%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><strong>Model</strong></th>
<th style="text-align: left;"><strong>Tokens</strong> <strong>/</strong>
<strong>Second</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Util</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Mem</strong></th>
<th style="text-align: left;"><strong>NPU Card（64G/Card）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B FP16</td>
<td style="text-align: left;">42 tokens/second</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B FP16</td>
<td style="text-align: left;">35 tokens/second</td>
<td style="text-align: left;">Util 11~13%</td>
<td style="text-align: left;">Mem 10%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B FP16</td>
<td style="text-align: left;">29 tokens/second</td>
<td style="text-align: left;">Util 15~16%</td>
<td style="text-align: left;">Mem 15%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 7B FP16</td>
<td style="text-align: left;">32 tokens/second</td>
<td style="text-align: left;">Util 16~21%</td>
<td style="text-align: left;">Mem 16%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 14B FP16</td>
<td style="text-align: left;">19 tokens/second</td>
<td style="text-align: left;">Util 19~22%</td>
<td style="text-align: left;">Mem 28%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 32B FP16</td>
<td style="text-align: left;">10.5 tokens/second</td>
<td style="text-align: left;">Util 10~45%</td>
<td style="text-align: left;">Mem 54%</td>
<td style="text-align: left;">双卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 72B FP16</td>
<td style="text-align: left;">6 tokens/second</td>
<td style="text-align: left;">Util 10~60%</td>
<td style="text-align: left;">Mem 78%</td>
<td style="text-align: left;">三卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B Q8_0</td>
<td style="text-align: left;">6.5 tokens/second</td>
<td style="text-align: left;">Util 2~5%</td>
<td style="text-align: left;">Mem 6%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 0.5B Q4_0</td>
<td style="text-align: left;">6 tokens/second</td>
<td style="text-align: left;">Util 4~5%</td>
<td style="text-align: left;">Mem 6%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B Q8_0</td>
<td style="text-align: left;">3.5 tokens/second</td>
<td style="text-align: left;">Util 4~11%</td>
<td style="text-align: left;">Mem 8%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 1.5B Q4_0</td>
<td style="text-align: left;">17~18 tokens/second</td>
<td style="text-align: left;">Util 9~12%</td>
<td style="text-align: left;">Mem 7%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B Q8_0</td>
<td style="text-align: left;">3.2 tokens/second</td>
<td style="text-align: left;">Util 10~15%</td>
<td style="text-align: left;">Mem 10%</td>
<td style="text-align: left;">单卡</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5 3B Q4_0</td>
<td style="text-align: left;">14.5 tokens/second</td>
<td style="text-align: left;">Util 8~15%</td>
<td style="text-align: left;">Mem 8%</td>
<td style="text-align: left;">单卡</td>
</tr>
</tbody>
</table>
<p>对其中的 Qwen 2.5 0.5B FP16 模型进行并发测试的性能表现如下：</p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 31%" />
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><strong>Concurrency</strong></th>
<th style="text-align: left;"><strong>Tokens</strong> <strong>/</strong>
<strong>Second</strong></th>
<th style="text-align: left;"><strong>Throughput</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Util</strong></th>
<th style="text-align: left;"><strong>NPU</strong>
<strong>Mem</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">39 tokens/second</td>
<td style="text-align: left;">39</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">38 tokens/second</td>
<td style="text-align: left;">76</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">37.66 tokens/second</td>
<td style="text-align: left;">113</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">34.25 tokens/second</td>
<td style="text-align: left;">137</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">31 tokens/second</td>
<td style="text-align: left;">155</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">28.16 tokens/second</td>
<td style="text-align: left;">169</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">27.57 tokens/second</td>
<td style="text-align: left;">193</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">26.87 tokens/second</td>
<td style="text-align: left;">215</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">26 tokens/second</td>
<td style="text-align: left;">234</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">10</td>
<td style="text-align: left;">26.9 tokens/second</td>
<td style="text-align: left;">269</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 7%</td>
</tr>
<tr>
<td style="text-align: left;">20</td>
<td style="text-align: left;">20.3 tokens/second</td>
<td style="text-align: left;">406</td>
<td style="text-align: left;">Util 6~7%</td>
<td style="text-align: left;">Mem 8%</td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">10.34 tokens/second</td>
<td style="text-align: left;">517</td>
<td style="text-align: left;">Util 3~5%</td>
<td style="text-align: left;">Mem 8%</td>
</tr>
<tr>
<td style="text-align: left;">100</td>
<td style="text-align: left;">4.17 tokens/second</td>
<td style="text-align: left;">417</td>
<td style="text-align: left;">Util 2~5%</td>
<td style="text-align: left;">Mem 9%</td>
</tr>
</tbody>
</table>
<h3 id="模型精度验证">4.4 模型精度验证</h3>
<p>除了算子的精度验证以外，对模型需要做整体的精度验证，以避免在数据加载拷贝，kv_cache操作等过程中出现错误。</p>
<p><strong>eval-callback</strong></p>
<p>llama.cpp社区提供了一个精度对比工具：eval-callback，这个工具会执行一次推理过程，并将推理过程中所有涉及的算子的计算结果进行打印。通过对比相同seed情况下的NPU和CPU的推理结果，判断整个推理过程是否存在异常。</p>
<p>需要注意的是，tensor的内容在会存在微小的差异，这不属于精度异常。</p>
<p><strong>CPU推理对比</strong></p>
<p>使用llama3模型，使用相同的seed，分别在NPU和CPU上进行相同的推理内容，理论上前数百token应该完全一致。由于存在精度的微小差异，推理累计的过程中，在长回复的后段，可能会出现细微差异。</p>
<h3 id="模型支持验证">4.5 模型支持验证</h3>
<p>目前，llama.cpp支持以下模型以及多种量化格式，我们仅关注fp16，Q8_0和Q4_0三种dtype。</p>
<p>模型支持的原则是不存在不支持的算子，检查方式是查看切图的情况，如果出现了大量子图（超过100），说明存在算子不支持，已经fallback到CPU进行推理，此类模型虽然能够完成推理，但是推理性能较低。</p>
<table>
<thead>
<tr>
<th style="text-align: center;">模型</th>
<th style="text-align: center;">FP16</th>
<th style="text-align: center;">Q8_0</th>
<th style="text-align: center;">Q4_0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AquilaChat2-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Baichuan-7b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Baichuan2-7B-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bitnet_b1_58-large</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bloom-560m</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">bloomz-alpaca-560m</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">c4ai-command-r-35B-v01</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">chatglm3-6B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">chinese-alpaca-2-1.3b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">CodeShell-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td
style="text-align: center;">deepseek-ai_deepseek-coder-1.3B-base</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">deepseek-ai_DeepSeek-V2-Lite</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">deepseek-coder-6.7B-instruct</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-V2-Lite-64x1.5B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">falcon-7b-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">flan-t5-large</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">gemma-2-9b-it</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">glm-4-9B</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">gpt2</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Gpt2-163M</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">granite-3B-code-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">GritLM-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">internlm2_5-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">koala-7B-HF</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7b-chat-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-Smaug-8B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-Chinese-7b-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama3-8B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Llama3-8b-chinese</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">mamba-130m-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">mpt-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">OLMo-1B-hf</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">OpenELM-3B-Instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Orion-14b-base</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">phi1</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">phi2</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">Phi-3-mini-4k-instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">plamo-13b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">pythia-70M</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-1.5B-Instruct</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Refact-1_6B-fim</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">SmolLM-135M</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">stablelm-zephyr</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">stablelm-2-zephyr-1_6b</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: center;">starcoderbase-1b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">starcoder2-3b</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">vigogne-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">xverse-7b-chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
<tr>
<td style="text-align: center;">Yi-6b-Chat</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
<td style="text-align: center;">√</td>
</tr>
</tbody>
</table>
<h3 id="社区ci">4.6 社区CI</h3>
<p>目前由于资源限制，暂时无法向社区提供开发机和CI机器，但是需要保证编译通过，防止社区的重构导致的昇腾后端被破坏的问题。编译不需要昇腾硬件，可以使用社区的CI机器。</p>
<ul>
<li>提供昇腾构建的容器镜像，避免配置复杂的环境。</li>
<li>提供github workflow的job，添加昇腾的CI验证，并作为门禁。</li>
</ul>
<h2 id="ollama支持">5. Ollama支持</h2>
<p>Ollama
是一个旨在提升本地大型语言模型（LLM）运行效率和灵活性的开源平台，快速在本地部署启动大模型的应用。Ollama
的设计初衷是通过优化硬件加速和支持更高效的推理计算，帮助开发者和研究人员更方便地在本地部署和运行
LLM，从而不依赖云计算资源或其他昂贵的基础设施。，Ollama使用llama.cpp作为推理引擎。一条命令可以完成安装和模型拉起。</p>
<p><strong>安装</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>
<p><strong>运行</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3</span><br></pre></td></tr></table></figure>
<p>除此之外，Ollama还有有一个模型的仓库，保存有海量的gguf模型，其兼容openAI
API，有着众多的前端应用。</p>
<p>为了能够充分的利用llama.cpp的昇腾后端能力，简化昇腾使用门槛，同时需要完成Ollama的昇腾适配。简单来说，OIlama需要适配一下几个关键部分：</p>
<p><strong>构建</strong></p>
<p>Ollama会构建llama.cpp工程，并将二进制打包到ollama的二进制文件中，在构建ollama的过程中，需要完成llama.cpp的昇腾版本的构建。</p>
<p><strong>NPU检测</strong></p>
<p>Ollama在运行时会检测NPU硬件，显存容量等，来判断模型是否能够运行，以及合理的模型拆分方式，所以需要在ollama中实现必要的昇腾硬件检测接口。</p>
<p><strong>拉起</strong></p>
<p>Ollama运行模型时，会拉起对应后端的llama.cpp服务器，这里需要根据硬件检测的结果来拉起NPU版本的llama.cpp服务器。</p>
<p>这里仅做Ollama兼容昇腾后端的简单洞察，不做详细设计，社区方案已完成，PR提交中。</p>
<h2 id="社区跟进">6. 社区跟进</h2>
<p>llama.cpp是一个非常活跃的社区，平均每天有十几个提交的合入，包括大量的重构和大粒度特性的合入。昇腾后端需要紧跟社区的发展路线，根据社区的重构和特性进行适配。</p>
<p>同时，在社区也存在对昇腾后端的需求，以及问题反馈，需要及时完成解决。</p>
<p>社区没有要求SLA，原则上，简单问题修复和重构适配应当在5个工作日内完成，特性需求根据实际情况灵活处理。</p>
<h2 id="文档和说明">7. 文档和说明</h2>
<p>为了帮助llama.cpp的昇腾用户，需要编写详尽的文档，包括环境搭建，构建，运行，模型和数据类型支持情况以及贡献指导等。</p>
<h3 id="社区doc">7.1 社区doc</h3>
<ul>
<li>在社区README添加Ascend的支持描述，并且提供跳转链接。</li>
<li>提供环境搭建步骤，包括操作系统版本，昇腾驱动和CANN的版本要求和安装方法。</li>
<li>提供Dockerfile，包含llama.cpp所需的环境配置，能够避免复杂的环境部署。</li>
<li>提供构建，运行的命令。</li>
<li>提供模型和数据类型支持情况。</li>
<li>提供issue和PR提交规范。</li>
</ul>
<h3 id="昇腾开源手册">7.2 昇腾开源手册</h3>
<p>为了方便中文用户，以及昇腾社区入口的用户，还需要在昇腾开源文档中提供中文版的step
by step构建和推理手册。</p>
<table>
<colgroup>
<col style="width: 85%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th>PR</th>
<th>代码量</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/6035">[CANN]
Add Ascend NPU backend #6035</a></td>
<td>+10,756 −8</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/8867">[CANN]
Add doc and docker image #8867</a></td>
<td>+329 −0</td>
</tr>
</tbody>
</table>
<p>其他参与review的PR和issue见<a
target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/issues?q=+label%3A%22Ascend+NPU%22+">链接</a>。</p>
<p><a
target="_blank" rel="noopener" href="https://ascend.github.io/docs/sources/llama_cpp/index.html">llama.cpp昇腾开源使用手册</a>。</p>
<h2 id="项目引用">8. 项目引用</h2>
<p><strong>ollama</strong></p>
<p><a
target="_blank" rel="noopener" href="https://github.com/ollama/ollama">Ollama</a>是一款专注于在本地运行大型语言模型的工具，旨在简化模型的部署和使用，提供高性能且无需云端依赖的AI推理体验，使用llama.cpp作为推理引擎，以git
submodule的方式引用llama.cpp代码。目前已与2012同事一同完成设计并提交<a
target="_blank" rel="noopener" href="https://github.com/ollama/ollama/pull/5872">PR</a>。</p>
<p><strong>llama edge</strong></p>
<p><a target="_blank" rel="noopener" href="https://github.com/LlamaEdge/LlamaEdge">Llama
Edge</a>是一个为边缘设备优化的轻量级大语言模型框架，旨在支持本地化、高效的推理，以满足低延迟和有限资源的计算需求，使用llama.cpp作为其推理后端。llama
edge官方发表了一篇知乎的<a
target="_blank" rel="noopener" href="https://www.zhihu.com/question/624955377/answer/13849002583?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_psn=1833491376626614272&amp;utm_source=wechat_session&amp;utm_id=0">回复</a>以及一篇<a
target="_blank" rel="noopener" href="https://www.secondstate.io/articles/llm-agents-on-ascend/">官方文档</a>。</p>
<p><strong>llamabox&amp;gpu stack</strong></p>
<p><a
target="_blank" rel="noopener" href="https://github.com/gpustack/llama-box">Llamabox</a>是一个便捷的平台，提供开箱即用的大语言模型部署方案，使用户能够轻松运行和管理AI模型；而<a
target="_blank" rel="noopener" href="https://github.com/gpustack/gpustack">gpustack</a>是一项云服务，专为高性能计算和AI模型训练优化，提供灵活的GPU资源共享和管理功能，其使用了llama.cpp作为其推理后端之一。有一篇使用gpustack使用昇腾推理的实践<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/KZ-kuNUx03BPo9vlK1OPxg">文章</a>。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/12/25/%E6%8E%A8%E7%90%86%E5%85%A8%E5%9C%BA%E6%99%AF%E6%B4%9E%E5%AF%9F/" rel="prev" title="推理全场景洞察">
                  <i class="fa fa-angle-left"></i> 推理全场景洞察
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/01/23/Ray%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8-%E7%BC%96%E8%AF%91%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BB%BB%E5%8A%A1%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/" rel="next" title="Ray技术入门-编译部署和任务生命周期">
                  Ray技术入门-编译部署和任务生命周期 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">hipudding</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/hipudding" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
