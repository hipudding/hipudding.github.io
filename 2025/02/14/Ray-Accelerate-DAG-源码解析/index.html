<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="icon" type="image/png" sizes="16x16" href="https://dogecoin.com/assets/images/doge.svg">
  <link rel="mask-icon" href="https://dogecoin.com/assets/images/doge.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>



<link rel="canonical" href="http://example.com/2025/02/14/Ray-Accelerate-DAG-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2025/02/14/Ray-Accelerate-DAG-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","path":"2025/02/14/Ray-Accelerate-DAG-源码解析/","title":"Ray Accelerate DAG 源码解析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Ray Accelerate DAG 源码解析 | hipudding's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">hipudding's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E9%97%B4%E9%80%9A%E4%BF%A1"><span class="nav-number">1.</span> <span class="nav-text">节点间通信</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E9%80%9A%E9%81%93"><span class="nav-number">2.</span> <span class="nav-text">数据传输通道</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cachedchannel"><span class="nav-number">2.1.</span> <span class="nav-text">CachedChannel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#intraprocesschannel"><span class="nav-number">2.2.</span> <span class="nav-text">IntraProcessChannel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#channel"><span class="nav-number">2.3.</span> <span class="nav-text">Channel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bufferedsharedmemorychannel"><span class="nav-number">2.4.</span> <span class="nav-text">BufferedSharedMemoryChannel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compositechannel"><span class="nav-number">2.5.</span> <span class="nav-text">CompositeChannel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torchtensorncclchannel"><span class="nav-number">2.6.</span> <span class="nav-text">_TorchTensorNcclChannel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torchtensorncclchannel-1"><span class="nav-number">2.7.</span> <span class="nav-text">TorchTensorNcclChannel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#channeloutputtype"><span class="nav-number">2.8.</span> <span class="nav-text">ChannelOutputType</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#readerwriterinterface"><span class="nav-number">2.9.</span> <span class="nav-text">Reader&#x2F;WriterInterface</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92"><span class="nav-number">3.</span> <span class="nav-text">生成执行计划</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dagoperationgraphnode"><span class="nav-number">3.1.</span> <span class="nav-text">DAGOperationGraphNode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#add_edge-_build_dag_node_operation_graph"><span class="nav-number">3.2.</span> <span class="nav-text">_add_edge &amp;&amp;
_build_dag_node_operation_graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#generate_actor_to_execution_schedule"><span class="nav-number">3.3.</span> <span class="nav-text">_generate_actor_to_execution_schedule</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#generate_overlapped_execution_schedule"><span class="nav-number">3.4.</span> <span class="nav-text">_generate_overlapped_execution_schedule</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E8%AF%91%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">4.</span> <span class="nav-text">编译计算图</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#executabletask"><span class="nav-number">4.1.</span> <span class="nav-text">ExecutableTask</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compileddag"><span class="nav-number">4.2.</span> <span class="nav-text">CompiledDAG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%85%B3%E6%B3%A8%E7%9A%84%E7%82%B9"><span class="nav-number">5.</span> <span class="nav-text">其他关注的点</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hipudding"
      src="https://dogecoin.com/assets/images/doge.svg">
  <p class="site-author-name" itemprop="name">hipudding</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hipudding" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hipudding" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:huafengchun@gmail.com" title="E-Mail → mailto:huafengchun@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/14/Ray-Accelerate-DAG-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://dogecoin.com/assets/images/doge.svg">
      <meta itemprop="name" content="hipudding">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hipudding's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Ray Accelerate DAG 源码解析 | hipudding's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Ray Accelerate DAG 源码解析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-02-14 14:54:32 / 修改时间：14:55:00" itemprop="dateCreated datePublished" datetime="2025-02-14T14:54:32+08:00">2025-02-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="节点间通信">节点间通信</h2>
<p>Communucator是不同actor之间数据传输的工具，目前支持CPU和GPU两种Communucator。</p>
<figure>
<img
src="https://cdn-0.plantuml.com/plantuml/png/bLBDJiCm3BxtANBinDO7gAPf4q9mG0p1E5TUuh48DudoCnW_xqwI0Mt7ED13fThVpsVNS-V1-j1I0MldBN3FE85px5otRL2IWzUMlHKiFcTIII-1v2kM5jHJpQJo8rQpdPRYW7X4tm2Fb6OBIX2wClMlfVDJ2_igjwf5Ub_b_goZ9SKo0sfs4ZjYsR1ZEAHDfcMJfV5IAw1fnFHadxKbqRYKUI3WK8bo1nImPj62RwiLAgVjc1b4s_mndqM-AzqMJ6IjWY7CRcCc_Dxy2EMFOvqn0oATrzdBkag4W_7YkprBblr8dNKJVn52sv-Mcue70TN5YqSR2_SPCUk6_2y7Y2oAmB7Cx7NO72ye9pbHrwPcTvcwDCEr2OmXzBxyVIm-BiWPRNsZeCKCYD3YeoY-pl3sONY9NQkvuPmkhGucLbDv09o-JoR7x1E8JgIOnt53Ivy0"
alt="PlantUML diagram" />
<figcaption aria-hidden="true">PlantUML diagram</figcaption>
</figure>
<p>Communicator主要记录了分布式计算的相关信息，例如，<code>world_size</code>，<code>rank</code>等，并实现了<code>all_reduce</code>接口。CPU通过共享内存进行通信（<strong>跨节点怎么通信？</strong>），而GPU使用nccl进行卡间带外通信。</p>
<p>Communucator核心功能提供了<code>send</code>，<code>recv</code>，<code>allreduce</code>等通信操作。CPU通信中由于使用的是共享内存，所以不需要实现<code>send</code>和<code>recv</code>，<code>all_reduce</code>使用了<code>CPUCommBarrier</code>来等待所有actor完成计算并执行op，这里使用了barrier远程对象，该对象会被多个actor调用传递数据，并且计算完成后，每个actor都能拿到规约后的数据。GPU使用nccl库完成上述操作。</p>
<h2 id="数据传输通道">数据传输通道</h2>
<p>Channel是多个actor之间的数据传输通道，每个channel有一个writer，以及多个reader。</p>
<figure>
<img
src="https://cdn-0.plantuml.com/plantuml/png/XPB1JiCm44Jl-GeVWw3zW0ga5YuWGOj4RrLbBSw6M7BiQBq6HS2_upH1BS99S_0X-sOzC_BICn27scGPl7ecK2oK0U_5NGNMedcmZ5I2Gl6PYVXTIgcjPYdp3z9nj12RXhMpO7O4xJpg7SrNtNaFjZ2ulnA4K23vXNZKddTdyCXpsvni7Nni0ZU1jss3-ulU25eV22NXMroBX8K4BtjJ-HJIs_b53eBy7Kp0XLZPDbwlQtI1JooKnW7lnOrOEuk3hjkIdqCF6tjU6USnNlgTPKE_e2eiXdIntECaLaxmJ4wXzwbvwjzjACiku4i5yNp2sb6RrBgwSJv6JWstZbIrZQKvMYjbKeYSPFwzcUklsUnlqZIJo3n6Ja_7UnXH71kPnXAvns4vGcTBj4MepGy0"
alt="PlantUML diagram" />
<figcaption aria-hidden="true">PlantUML diagram</figcaption>
</figure>
<p><code>write</code>表示将数据写入到channel中，<code>read</code>表示从channel中读取数据，这是同步操作的，如果写入时还有数据未读，或者读取时没有数据可读，这两个方法会block等待，可以通过timeout控制block时间。</p>
<h3 id="cachedchannel">CachedChannel</h3>
<p>CachedChannel表示一次写入需要被读取多次的Channel，它可以接受一个内部的ChannelInterface，它作为一个warpper来缓存数据供reader多次读取。如果没有内部ChannelInterface，它会将write的数据写入到ChannelContext（单例）中，然后供reader多次读取。</p>
<p>ChannelContext的<code>serialization_context</code>使用了一个字典存储写入的数据，当读取次数达到设定后，会将数据从字典中清除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write</span>(<span class="params">self, value: <span class="type">Any</span>, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> better organize the imports</span></span><br><span class="line">    <span class="keyword">from</span> ray.experimental.channel <span class="keyword">import</span> ChannelContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._inner_channel <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self._inner_channel.write(value, timeout) <span class="comment"># 如果有内部channel，直接写到内部channel中。</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Otherwise no need to check timeout as the operation is non-blocking.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Because both the reader and writer are in the same worker process,</span></span><br><span class="line">    <span class="comment"># we can directly store the data in the context instead of storing</span></span><br><span class="line">    <span class="comment"># it in the channel object. This removes the serialization overhead of `value`.</span></span><br><span class="line">    ctx = ChannelContext.get_current().serialization_context</span><br><span class="line">    ctx.set_data(self._channel_id, value, self._num_reads)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read</span>(<span class="params">self, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> better organize the imports</span></span><br><span class="line">    <span class="keyword">from</span> ray.experimental.channel <span class="keyword">import</span> ChannelContext</span><br><span class="line"></span><br><span class="line">    ctx = ChannelContext.get_current().serialization_context</span><br><span class="line">    <span class="keyword">if</span> ctx.has_data(self._channel_id):</span><br><span class="line">        <span class="comment"># No need to check timeout as the operation is non-blocking.</span></span><br><span class="line">        <span class="keyword">return</span> ctx.get_data(self._channel_id)  <span class="comment"># 如果缓存里有数据，从缓存读。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (</span><br><span class="line">        self._inner_channel <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    ), <span class="string">&quot;Cannot read from the serialization context while inner channel is None.&quot;</span></span><br><span class="line">    value = self._inner_channel.read(timeout)         <span class="comment"># 如果缓存里没有，就从内部channel中读，然后写到缓存供后续reader读取。</span></span><br><span class="line">    ctx.set_data(self._channel_id, value, self._num_reads)</span><br><span class="line">    <span class="keyword">return</span> ctx.get_data(self._channel_id)</span><br></pre></td></tr></table></figure>
<h3 id="intraprocesschannel">IntraProcessChannel</h3>
<p>如果两个task在同一个worker进程中执行（一个worker可以同时执行多个task么？顺序执行么？），那么直接使用<code>serialization_context</code>来进行数据的传递(不需要加锁么？不是多线程，先后执行的么？)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write</span>(<span class="params">self, value: <span class="type">Any</span>, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># No need to check timeout as the operation is non-blocking.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Because both the reader and writer are in the same worker process,</span></span><br><span class="line">    <span class="comment"># we can directly store the data in the context instead of storing</span></span><br><span class="line">    <span class="comment"># it in the channel object. This removes the serialization overhead of `value`.</span></span><br><span class="line">    ctx = ChannelContext.get_current().serialization_context</span><br><span class="line">    ctx.set_data(self._channel_id, value, self._num_readers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read</span>(<span class="params">self, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>, deserialize: <span class="built_in">bool</span> = <span class="literal">True</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">    <span class="keyword">assert</span> deserialize, <span class="string">&quot;Data passed from the actor to itself is never serialized&quot;</span></span><br><span class="line">    <span class="comment"># No need to check timeout as the operation is non-blocking.</span></span><br><span class="line">    ctx = ChannelContext.get_current().serialization_context</span><br><span class="line">    <span class="keyword">return</span> ctx.get_data(self._channel_id)</span><br></pre></td></tr></table></figure>
<h3 id="channel">Channel</h3>
<p>Channel可以提供节点内或者节点之间的数据传输。Channel是对ray.ObjectRef的一个封装，使用plasma共享存储进行数据的传输。</p>
<ul>
<li>在channel创建时，write角色的actor会使用global
worker执行put_object写入一个全0的buffer；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">worker = ray._private.worker.global_worker</span><br><span class="line">   worker.check_connected()</span><br><span class="line"></span><br><span class="line">   value = <span class="string">b&quot;0&quot;</span> * buffer_size_bytes</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span>:</span><br><span class="line">       object_ref = worker.put_object(</span><br><span class="line">           value, owner_address=<span class="literal">None</span>, _is_experimental_channel=<span class="literal">True</span></span><br><span class="line">       )</span><br><span class="line">   <span class="keyword">except</span> ray.exceptions.ObjectStoreFullError:</span><br><span class="line">       logger.info(</span><br><span class="line">           <span class="string">&quot;Put failed since the value was either too large or the &quot;</span></span><br><span class="line">           <span class="string">&quot;store was full of pinned objects.&quot;</span></span><br><span class="line">       )</span><br><span class="line">       <span class="keyword">raise</span></span><br><span class="line">   <span class="keyword">return</span> object_ref</span><br></pre></td></tr></table></figure>
<ul>
<li>然后对所有reader中与writer不是同一节点的节点上创建一个对象的引用，每个节点只需要创建一个即可，这样当前节点上的所有reader都可以获取到这个数据（为什么不直接把writer_ref传过去，而是每个reader
node上创建一个引用，并且这个引用需要拷贝？）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find 1 reader in a remote node to create a reference that&#x27;s</span></span><br><span class="line"><span class="comment"># shared by all readers. When a new value is written to a reference,</span></span><br><span class="line"><span class="comment"># it is sent to this reference.</span></span><br><span class="line">reader = readers[<span class="number">0</span>]</span><br><span class="line">fn = reader.__ray_call__</span><br><span class="line">self._node_id_to_reader_ref_info[node_id] = ReaderRefInfo(</span><br><span class="line">    reader_ref=ray.get(</span><br><span class="line">        fn.remote(_create_channel_ref, buffer_size_bytes)</span><br><span class="line">    ),</span><br><span class="line">    ref_owner_actor_id=reader._actor_id,</span><br><span class="line">    num_reader_actors=<span class="built_in">len</span>(readers),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>向channel写数据时，先把数据序列化，然后直接写入channel，这部分调用的是core_woker中的c++代码。写入的数据还有可能是resize_buffer的消息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(value, SerializedObject):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">        serialized_value = self._worker.get_serialization_context().serialize(</span><br><span class="line">            value</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">        sio = io.StringIO()</span><br><span class="line">        ray.util.inspect_serializability(value, print_file=sio)</span><br><span class="line">        msg = (</span><br><span class="line">            <span class="string">&quot;Could not serialize the put value &quot;</span></span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">repr</span>(value)&#125;</span>:\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;sio.getvalue()&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">raise</span> TypeError(msg) <span class="keyword">from</span> e</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    serialized_value = value</span><br><span class="line"></span><br><span class="line">start_time = time.monotonic()</span><br><span class="line">self._resize_channel_if_needed(serialized_value, timeout_ms)</span><br><span class="line"><span class="keyword">if</span> timeout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    timeout_ms -= <span class="built_in">int</span>((time.monotonic() - start_time) * <span class="number">1000</span>)</span><br><span class="line">    timeout_ms = <span class="built_in">max</span>(timeout_ms, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">self._worker.core_worker.experimental_channel_put_serialized(</span><br><span class="line">    serialized_value,</span><br><span class="line">    self._writer_ref,</span><br><span class="line">    self._num_local_readers,</span><br><span class="line">    timeout_ms,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>读取信息时，如果是resize_buffer消息，则会重新注册reader，然后再读取信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ret = self._worker.get_objects(</span><br><span class="line">    [self._local_reader_ref], timeout=timeout, return_exceptions=<span class="literal">True</span></span><br><span class="line">)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(ret, _ResizeChannel):</span><br><span class="line">    self._node_id_to_reader_ref_info = ret._node_id_to_reader_ref_info</span><br><span class="line">    self._local_reader_ref = self._get_local_reader_ref(</span><br><span class="line">        self._node_id_to_reader_ref_info</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># We need to register the new reader_ref.</span></span><br><span class="line">    self._reader_registered = <span class="literal">False</span></span><br><span class="line">    self.ensure_registered_as_reader()</span><br><span class="line">    <span class="keyword">if</span> timeout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        timeout -= time.monotonic() - start_time</span><br><span class="line">        timeout = <span class="built_in">max</span>(timeout, <span class="number">0</span>)</span><br><span class="line">    ret = self._worker.get_objects(</span><br><span class="line">        [self._local_reader_ref], timeout=timeout, return_exceptions=<span class="literal">True</span></span><br><span class="line">    )[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="bufferedsharedmemorychannel">BufferedSharedMemoryChannel</h3>
<p>这是对上面Channel的一个封装，BufferedSharedMemoryChannel允许创建多个buffer，循环写入和读取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write</span>(<span class="params">self, value: <span class="type">Any</span>, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Write a value to a channel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If the next buffer is available, it returns immediately. If the next</span></span><br><span class="line"><span class="string">    buffer is not read by downstream consumers, it blocks until a buffer is</span></span><br><span class="line"><span class="string">    available to write. If a buffer is not available within timeout, it raises</span></span><br><span class="line"><span class="string">    RayChannelTimeoutError.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># A single channel is not supposed to read and write at the same time.</span></span><br><span class="line">    <span class="keyword">assert</span> self._next_read_index == <span class="number">0</span></span><br><span class="line">    self._buffers[self._next_write_index].write(value, timeout)</span><br><span class="line">    self._next_write_index += <span class="number">1</span></span><br><span class="line">    self._next_write_index %= self._num_shm_buffers</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read</span>(<span class="params">self, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Read a value from a channel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If the next buffer is available, it returns immediately. If the next</span></span><br><span class="line"><span class="string">    buffer is not written by an upstream producer, it blocks until a buffer is</span></span><br><span class="line"><span class="string">    available to read. If a buffer is not available within timeout, it raises</span></span><br><span class="line"><span class="string">    RayChannelTimeoutError.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># A single channel is not supposed to read and write at the same time.</span></span><br><span class="line">    <span class="keyword">assert</span> self._next_write_index == <span class="number">0</span></span><br><span class="line">    output = self._buffers[self._next_read_index].read(timeout)</span><br><span class="line">    self._next_read_index += <span class="number">1</span></span><br><span class="line">    self._next_read_index %= self._num_shm_buffers</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="compositechannel">CompositeChannel</h3>
<p>CompositeChannel允许一个writer将数据写入多个channel，当reader和writer是同一个actor时，创建一个IntraProcessChannel，如果不是，则创建一个BufferedSharedMemoryChannel。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(</span><br><span class="line">    remote_reader_and_node_list,</span><br><span class="line">    local_reader_and_node_list,</span><br><span class="line">) = utils.split_readers_by_locality(self._writer, self._reader_and_node_list)</span><br><span class="line"><span class="comment"># There are some local readers which are the same worker process as the writer.</span></span><br><span class="line"><span class="comment"># Create a local channel for the writer and the local readers.</span></span><br><span class="line">num_local_readers = <span class="built_in">len</span>(local_reader_and_node_list)</span><br><span class="line"><span class="keyword">if</span> num_local_readers &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Use num_readers = 1 when creating the local channel,</span></span><br><span class="line">    <span class="comment"># because we have channel cache to support reading</span></span><br><span class="line">    <span class="comment"># from the same channel multiple times.</span></span><br><span class="line">    local_channel = IntraProcessChannel(num_readers=<span class="number">1</span>)</span><br><span class="line">    self._channels.add(local_channel)</span><br><span class="line">    actor_id = self._get_actor_id(self._writer)</span><br><span class="line">    self._channel_dict[actor_id] = local_channel</span><br><span class="line"><span class="comment"># There are some remote readers which are not the same Ray actor as the writer.</span></span><br><span class="line"><span class="comment"># Create a shared memory channel for the writer and the remote readers.</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(remote_reader_and_node_list) != <span class="number">0</span>:</span><br><span class="line">    remote_channel = BufferedSharedMemoryChannel(</span><br><span class="line">        self._writer, remote_reader_and_node_list, num_shm_buffers</span><br><span class="line">    )</span><br><span class="line">    self._channels.add(remote_channel)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> reader, _ <span class="keyword">in</span> remote_reader_and_node_list:</span><br><span class="line">        actor_id = self._get_actor_id(reader)</span><br><span class="line">        self._channel_dict[actor_id] = remote_channel</span><br></pre></td></tr></table></figure>
<p>写入数据时，循环将数据写入所有的channel，读取时，仅需要读取当前reader
actorid对应的channel。</p>
<p>(一个channel对象好像被write和reader一起持有？)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write</span>(<span class="params">self, value: <span class="type">Any</span>, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    self.ensure_registered_as_writer()</span><br><span class="line">    <span class="keyword">for</span> channel <span class="keyword">in</span> self._channels:</span><br><span class="line">        channel.write(value, timeout)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read</span>(<span class="params">self, timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">    self.ensure_registered_as_reader()</span><br><span class="line">    <span class="keyword">return</span> self._channel_dict[self._resolve_actor_id()].read(timeout)</span><br></pre></td></tr></table></figure>
<h3 id="torchtensorncclchannel">_TorchTensorNcclChannel</h3>
<p>用户传输torch.tensor类型的数据，不能包含其他CPU传输的数据。使用该Channel之前，首先需要调用<code>_init_communicator</code>初始化nccl_group，然后根据writer和reader设置nccl_group的writer和reader对应的rank。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">self._writer_rank = self._nccl_group.get_rank(self._writer)</span><br><span class="line">self._reader_ranks = [</span><br><span class="line">    self._nccl_group.get_rank(reader)</span><br><span class="line">    <span class="keyword">for</span> reader, _ <span class="keyword">in</span> self._reader_and_node_list</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (</span><br><span class="line">    self._writer_rank <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">and</span> self._writer_rank == self._nccl_group.get_self_rank()</span><br><span class="line">):</span><br><span class="line">    self._writer_registered = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (</span><br><span class="line">    self._reader_ranks</span><br><span class="line">    <span class="keyword">and</span> self._nccl_group.get_self_rank() <span class="keyword">in</span> self._reader_ranks</span><br><span class="line">):</span><br><span class="line">    self._reader_registered = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>如果没有指定元数据的Channel（shape和dtype），那么还会初始化一个meta_channel来通过CPU传输元数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self._meta_channel <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> self._writer_registered:</span><br><span class="line">    <span class="comment"># We are the writer. Therefore, we also need to allocate a metadata</span></span><br><span class="line">    <span class="comment"># channel that will be used to send the shape and dtype of the</span></span><br><span class="line">    <span class="comment"># tensor to the receiver(s).</span></span><br><span class="line">    metadata_type = SharedMemoryType()</span><br><span class="line">    self._meta_channel = metadata_type.create_channel(</span><br><span class="line">        self._writer,</span><br><span class="line">        self._reader_and_node_list,</span><br><span class="line">        <span class="literal">None</span>,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>在发送数据时，需要先处理元数据，遍历所有tensor，获取他们的shape和dtype，写入到metadata数组中。如果设置了static_tensor，还会做元数据校验，如果出现了元数据不匹配会报错。</p>
<p>接受数据时，会先读取meta_channel，先获取到元数据信息，然后读取tensor数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    tensors: <span class="type">List</span>[<span class="string">&quot;torch.Tensor&quot;</span>],</span></span><br><span class="line"><span class="params">    timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> tensor <span class="keyword">in</span> tensors:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(</span><br><span class="line">            tensor, torch.Tensor</span><br><span class="line">        ), <span class="string">f&quot;<span class="subst">&#123;tensor&#125;</span> must be instance of torch.Tensor&quot;</span></span><br><span class="line"></span><br><span class="line">    metadata = self._get_send_tensors_metadata(tensors) <span class="comment"># 先发送元数据</span></span><br><span class="line">    <span class="keyword">if</span> metadata <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self._meta_channel.write(metadata)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> tensor <span class="keyword">in</span> tensors:</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> If there are multiple readers, can replace with a</span></span><br><span class="line">        <span class="comment"># broadcast.</span></span><br><span class="line">        <span class="keyword">for</span> rank <span class="keyword">in</span> self._reader_ranks:</span><br><span class="line">            self._nccl_group.send(tensor, rank) <span class="comment"># 发送tensor数据</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    timeout: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Union</span>[<span class="string">&quot;torch.Tensor&quot;</span>, <span class="type">List</span>[<span class="string">&quot;torch.Tensor&quot;</span>]]:</span><br><span class="line">    meta_list: <span class="type">List</span>[_TorchTensorMetadata] = self._get_recv_tensors_metadata(timeout) <span class="comment"># 先接受所有元数据</span></span><br><span class="line"></span><br><span class="line">    bufs: <span class="type">List</span>[<span class="string">&quot;torch.Tensor&quot;</span>] = []</span><br><span class="line">    <span class="keyword">for</span> meta <span class="keyword">in</span> meta_list:</span><br><span class="line">        buf = self._nccl_group.recv(</span><br><span class="line">            meta.shape, meta.dtype, self._writer_rank, _torch_zeros_allocator   <span class="comment"># 在接收每个tensor</span></span><br><span class="line">        )</span><br><span class="line">        bufs.append(buf)</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Sync CUDA stream after receiving all tensors, instead of after</span></span><br><span class="line">    <span class="comment"># each tensor.</span></span><br><span class="line">    <span class="keyword">return</span> bufs</span><br></pre></td></tr></table></figure>
<h3 id="torchtensorncclchannel-1">TorchTensorNcclChannel</h3>
<p>该Channel可以发送包含torch.tensor的混合数据，其包含cpu_data_channel，gpu_data_channel:(_TorchTensorNcclChannel)，发送前会先对value进行序列化，序列化会将torch.tensor对象记录下来，在value中记录一个pleaceholder，然后将其他数据完成序列化。Tensor数据不会进行序列化，直接通过nccl发送。如果typ标记为_direct_return，那么说明发送的数据仅有torch.tensor，可以不调用value的序列化以提高执行效率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># Serialize the data. All tensors that match our current device</span></span><br><span class="line">    <span class="comment"># will be extracted into the serialization context and replaced</span></span><br><span class="line">    <span class="comment"># with a placeholder.</span></span><br><span class="line">    cpu_data = self._worker.get_serialization_context().serialize(value)</span><br><span class="line"><span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">    sio = io.StringIO()</span><br><span class="line">    ray.util.inspect_serializability(value, print_file=sio)</span><br><span class="line">    msg = (</span><br><span class="line">        <span class="string">&quot;Could not serialize the put value &quot;</span></span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">repr</span>(value)&#125;</span>:\n&quot;</span></span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;sio.getvalue()&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">raise</span> TypeError(msg) <span class="keyword">from</span> e</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># Pop the tensors that were found during serialization of `value`.</span></span><br><span class="line">    gpu_tensors, _ = self.serialization_ctx.reset_out_of_band_tensors([])</span><br><span class="line">    <span class="comment"># Reset the serialization method to now serialize torch.Tensors</span></span><br><span class="line">    <span class="comment"># normally.</span></span><br><span class="line">    self.serialization_ctx.set_use_external_transport(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># First send the extracted tensors through a GPU-specific channel.</span></span><br><span class="line">self._gpu_data_channel.write(gpu_tensors)</span><br><span class="line"><span class="comment"># Next send the non-tensor data through a CPU-specific channel. The</span></span><br><span class="line"><span class="comment"># data contains placeholders for the extracted tensors.</span></span><br><span class="line">self._cpu_data_channel.write(cpu_data)</span><br></pre></td></tr></table></figure>
<p>Channel读取时，先读取tensor数据，然后读取cpu数据，这个过程会将placeholder替换回Tensor，然后返回数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> self.serialization_ctx.reset_out_of_band_tensors(tensors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Next, read and deserialize the non-tensor data. The registered custom</span></span><br><span class="line"><span class="comment"># deserializer will replace the found tensor placeholders with</span></span><br><span class="line"><span class="comment"># `tensors`.</span></span><br><span class="line">data = self._cpu_data_channel.read(</span><br><span class="line">    timeout=timeout,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Check that all placeholders had a corresponding tensor.</span></span><br><span class="line">(</span><br><span class="line">    _,</span><br><span class="line">    deserialized_tensor_placeholders,</span><br><span class="line">) = self.serialization_ctx.reset_out_of_band_tensors([])</span><br><span class="line"><span class="keyword">assert</span> deserialized_tensor_placeholders == <span class="built_in">set</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(tensors)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
<h3 id="channeloutputtype">ChannelOutputType</h3>
<p>Channel的创建，一般是根据DAGNode的输出类型来创建，此时一般创建一个ChannelOutputType的结构，然后调用其<code>create_channel</code>方法来创建channel。</p>
<p>该类型表示当前Channel的输出类型，目前有两个子类<code>SharedMemoryType</code>和<code>TorchTensorType</code>。其中<code>SharedMemoryType</code>比较简单，仅提供一个<code>create_channel</code>方法，该方法会创建一个CompositeChannel类型的channel。</p>
<p><code>TorchTensorType</code>还需要注册自定义序列化和反序列化函数，如果使用nccl通信，torch.tensor类型的对象会直接通过nccl发送，并且在原数据的序列化中记录一个placeholder。如果没有nccl，那么会转成numpy对象，然后进行序列化通过CPU发送，这将带来额外的开销。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register_custom_serializer</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line"><span class="built_in">super</span>().register_custom_serializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">serialize</span>(<span class="params">t</span>):</span><br><span class="line">    ctx = ChannelContext.get_current()</span><br><span class="line">    <span class="keyword">return</span> ctx.serialization_context.serialize_tensor(t)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deserialize</span>(<span class="params">b</span>):</span><br><span class="line">    ctx = ChannelContext.get_current()</span><br><span class="line">    <span class="keyword">return</span> ctx.serialization_context.deserialize_tensor(b)</span><br><span class="line"></span><br><span class="line">ray.util.serialization.register_serializer(</span><br><span class="line">    torch.Tensor,</span><br><span class="line">    serializer=serialize,</span><br><span class="line">    deserializer=deserialize,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="readerwriterinterface">Reader/WriterInterface</h3>
<p>Reader/WriterInterface是从actor的角度整合相关Channel，也就是某个actor可能会读取多个输入，并且将输出写入到多个channel中。这两个Interface分别有两个子类，同步和异步的Interface。同步Interface较简单，从channel
list中读取所有的数据即可。异步的Interface会使用异步(asyncio)来等待数据。</p>
<h2 id="生成执行计划">生成执行计划</h2>
<p>DAGOperationGraph是更细粒度的任务图，构建图能够理清任务之间的依赖关系，便于生成每个actor的schedule。</p>
<h3 id="dagoperationgraphnode">DAGOperationGraphNode</h3>
<p>DAGNodeOperationNode一共有三种类型，分别是<code>READ</code>,<code>COMPUTE</code>和<code>WRITE</code>。DAG计算图经过编译后，会生成DAGOperationGraph，并根据这个图，生成每个actor执行的schedule，编译图执行时，会循环执行schedule中的任务，直到channel关闭。</p>
<p>DAGOperationGraphNode主要实现的是一个优先级比较函数，用于任务优先级队列的排序。（exec_task_id和task_id的区别？这里的compare函数是不是完备的）</p>
<p>其内部实现了compare函数，比较逻辑是：如果exec_task_id不同，则比较operation的exec_task_id，否则比较task_id</p>
<ol type="1">
<li>如果任务节点来自同一个actor，使用compare比较；</li>
<li>如果参与比较的两个任务一个是需要nccl参与，另外一个不需要，那么不需要参与的优先级高；</li>
<li>如果都依赖nccl，那么也用compare比较。</li>
</ol>
<h3 id="add_edge-_build_dag_node_operation_graph">_add_edge &amp;&amp;
_build_dag_node_operation_graph</h3>
<p>该函数建立所有GraphNode之间的依赖关系，为拓扑排序做准备。在<code>_build_dag_node_operation_graph</code>建图过程中，会使用该函数添加所有节点之间的边。每条边要记录两端节点的<code>in_edges</code>和<code>out_edges</code>。</p>
<p>每个DAG计算图中的节点会在编译时创建三个DAGOperationGraphNode，分别是<code>READ</code>,<code>COMPUTE</code>和<code>WRITE</code>，遍历所有的Node，按一下规则进行连接：</p>
<ol type="1">
<li>相同task中，从READ节点连接到COMPUTE节点，然后从COMPUTE节点连接到WRITE节点。</li>
<li>同一个actor中的不同bind_index的COMPUTE节点，按bind的顺序添加边。（看起来是为了保证actor执行顺序的）。</li>
<li>遍历所有的task的下游节点，task的WRITE连接到下游的READ节点。</li>
</ol>
<p>注意，如果是class_method_output类型的ClassMethodNode，需要连接其上游节点和其下游节点，因为class_method_output类型的Node，仅用于保存数据，不涉及节点依赖关系。</p>
<h3
id="generate_actor_to_execution_schedule">_generate_actor_to_execution_schedule</h3>
<p>根据OperationGraph构建actor schedule。</p>
<ol type="1">
<li>先找出入度为0的节点，这些节点可以直接执行；</li>
<li>然后找出这些节点的后续节点；</li>
<li>在检查next_node的后续节点，如果入度已经为0（或者所有read_connective_Idxs全部ready），则可以执行。</li>
</ol>
<p>直到所有的节点都访问过。这样就获得了每个actor的node的执行顺序。如果node是一个计算节点，并且需要使用nccl，说明其后是一个collective_node，当前节点is_ready后，将当前节点信息记录到collective_node的read_connective_Idxs中，方便该节点检查是否ready。</p>
<h3
id="generate_overlapped_execution_schedule">_generate_overlapped_execution_schedule</h3>
<p>该方法对已经构建的每个actor的schedule进行顺序调整，让其支持计算和通信并行。目前支持nccl
read操作。方法是，根据这个READ操作向前找，直到找到一个COMPUTE操作，然后将该READ操作插入到COMPUTE操作之前，由于异步读取的缘故，读取过程和COMPUTE过程可以并行执行。</p>
<h2 id="编译计算图">编译计算图</h2>
<p>简单的理解，计算图的编译就是将DAG涉及的节点之间的数据传递关系创建好通道（channel），然后将所有的Task运行起来，每个Task分为三个操作，分别是读取，计算和发送，循环执行。</p>
<p>这样避免了临时的资源创建的调度开销，并且，还支持NCCL这种带外通信，避免了Host拷贝和传输的开销。</p>
<h3 id="executabletask">ExecutableTask</h3>
<p>ExecutableTask近似于代表一个DAGNode，但是多了一些元数据信息。例如输出输出的Channel等，Task会将所有的输入封装成<code>ReaderInterface</code>，所有的输出封装成<code>WriterInterface</code>。</p>
<p>该Task对外提供<code>exec_operation</code>方法，此方法根据传入的<code>op_type</code>来决定执行读，计算还是发送。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exec_operation</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    class_handle,</span></span><br><span class="line"><span class="params">    op_type: _DAGNodeOperationType,</span></span><br><span class="line"><span class="params">    overlap_gpu_communication: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">if</span> op_type == _DAGNodeOperationType.READ:</span><br><span class="line">        <span class="keyword">with</span> _device_context_manager():</span><br><span class="line">            <span class="keyword">with</span> self._recv_stream:</span><br><span class="line">                <span class="keyword">return</span> self._read(overlap_gpu_communication)</span><br><span class="line">    <span class="keyword">elif</span> op_type == _DAGNodeOperationType.COMPUTE:</span><br><span class="line">        <span class="keyword">return</span> self._compute(overlap_gpu_communication, class_handle)</span><br><span class="line">    <span class="keyword">elif</span> op_type == _DAGNodeOperationType.WRITE:</span><br><span class="line">        <span class="keyword">with</span> _device_context_manager():</span><br><span class="line">            <span class="keyword">with</span> self._send_stream:</span><br><span class="line">                <span class="keyword">return</span> self._write()</span><br></pre></td></tr></table></figure>
<p>其中为了支持gpu的计算通信并行，还涉及到<code>AwaitableBackgroundReader</code>和<code>AwaitableBackgroundWriter</code>，以及<code>GPUFuture</code>。也就是说Read和Compute操作可以返回（或操作）一个future对象。</p>
<h3 id="compileddag">CompiledDAG</h3>
<p>编译流程比较长，重点的两个函数是<code>preprocess</code>和<code>get_or_compile</code>。</p>
<p><strong>preprocess</strong></p>
<ol type="1">
<li>找到InputNode和OutputNode，检查所有的Task是不是都是Actor，task暂时不支持；</li>
<li>配置带外通信相关信息，并且收集collective_ops；</li>
<li>处理上游节点，包括配置type_hint，记录上游actor等；</li>
<li>根据带外通信信息，初始化communicator；</li>
</ol>
<p><strong>get_or_compile</strong></p>
<ol type="1">
<li>从input_task广度优先遍历DAG，创建所有的Channel；</li>
<li>做死锁检测，目前尚未实现逻辑；</li>
<li>针对每一个task，构建所有arg和其消费者的map；</li>
<li>如果有多个消费者，创建Cached Channel供多次读取；</li>
<li>针对每个Task创建Executable_task；</li>
<li>创建每个task的执行计划<code>build_execution_schedule</code>:
<ol type="1">
<li>创建DAGOperationGraph节点，每个Task对应读，计算，写；</li>
<li>根据依赖关系，创建DAGOperationGraph；</li>
<li>根据图的拓扑排序，生成每个actor的执行计划；</li>
<li>如果配置了gpu并行计算（overlapped_execution），将actor的执行计划做调整；</li>
</ol></li>
<li>然后将执行计划交给<code>exec_task_func（do_exec_tasks）</code>执行，这个函数是一个死循环，会循环执行执行计划中的Operation；</li>
<li>开启Monitor线程，定期检测所有Task的运行情况，如果有Exception，终止执行。</li>
</ol>
<h2 id="其他关注的点">其他关注的点</h2>
<ol type="1">
<li>全篇Accelerators的通信全部交xxx_nccl，但是只有Nvidia的ccl库才叫nccl，应该修改来避免歧义；同理也有gpu_xxx，应该改为device_xxx等；</li>
<li>TorchTensorType的transform允许传入一个Comminicator对象，但是会校验名称是否在AUTO，NCCL或者CPU中，这里存在bug，应当加入传入的Communicator的名称。同时，AUTO其实没有作用，默认使用CPU通信，这个AUTO实际上可以通过自动检测硬件的方式来初始化对应的Communicator。</li>
<li>自定义Communicator目前只实现了CPUCommunicator，为了方便调试，需要补充文档和样例，来指导使用自定义Communicator。</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/02/10/Ray-DAG%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/" rel="prev" title="Ray DAG 源码解读">
                  <i class="fa fa-angle-left"></i> Ray DAG 源码解读
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/02/21/HCCL-%E9%80%9A%E4%BF%A1%E9%AA%8C%E8%AF%81demo/" rel="next" title="HCCL 通信验证demo">
                  HCCL 通信验证demo <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">hipudding</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/hipudding" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
